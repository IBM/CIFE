{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec55af33",
   "metadata": {},
   "source": [
    "# Testing the Reasoning Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea49c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.compute_clients import create_clients\n",
    "import pandas as pd\n",
    "import os\n",
    "from utils.compute_clients import LLMClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e7f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v10_v2.jsonl\", lines=True)\n",
    "user_prompts = df[\"combined_instruction\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba896ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.getenv(\"RITS_API_KEY\")\n",
    "model_id = \"microsoft/Phi-4-reasoning\"\n",
    "base_url = \"https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com/phi-4-reasoning/v1\"\n",
    "client = LLMClient(\n",
    "    api_key=api_key,\n",
    "    model_id=model_id,\n",
    "    client_type=\"rits\",\n",
    "    base_url=base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.getenv(\"RITS_API_KEY\")\n",
    "model_id = \"Qwen/Qwen3-8B\"\n",
    "base_url = \"https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com/qwen3-8b/v1\"\n",
    "client = LLMClient(\n",
    "    api_key=api_key,\n",
    "    model_id=model_id,\n",
    "    client_type=\"rits\",\n",
    "    base_url=base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "568a666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.getenv(\"RITS_API_KEY\")\n",
    "model_id = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "base_url = \"https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com/granite-3-3-8b-instruct/v1\"\n",
    "client = LLMClient(\n",
    "    api_key=api_key,\n",
    "    model_id=model_id,\n",
    "    client_type=\"rits\",\n",
    "    base_url=base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7645321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(user_prompts,client=client,max_new_tokens=8000,temperature=0.1, system_prompt=None):\n",
    "    responses = client.get_model_response_batch(\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompts=user_prompts,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature\n",
    "\n",
    "    )\n",
    "    return responses \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d3666bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 1489/1489 [12:53<00:00,  1.92it/s] \n"
     ]
    }
   ],
   "source": [
    "prompts = user_prompts\n",
    "responses = generate_response(prompts, client=client, temperature=0.1)\n",
    "# for i, response in enumerate(responses):\n",
    "#     print(f\"Prompt {i+1}: {prompts[i]}\")\n",
    "#     print(f\"Response {i+1}: {response}\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5542d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want a parser which can parse the responses and extract the <response> tag content\n",
    "import re\n",
    "def parse_response(response):\n",
    "    # Use regex to find the content within <response> tags\n",
    "    match = re.search(r'<response>(.*?)</response>', response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return \"No response found\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cf0842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_responses = [parse_response(response) for response in responses]\n",
    "df['response'] = filter_responses\n",
    "# for i, response in enumerate(responses):\n",
    "#     print(f\"Parsed Response {i+1}: {response}\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c1e3078",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/outputs/V10_experiments/granite-3.3-8b-instruct_reasoning.jsonl\", orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f856da",
   "metadata": {},
   "source": [
    "# o3-mini model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d00e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v10_v2.jsonl\", lines=True)\n",
    "user_prompts = df[\"combined_instruction\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f763b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.compute_clients import create_clients\n",
    "import os\n",
    "api_key = os.getenv(\"IBM_OPENAI_API_KEY\")\n",
    "model_id = \"Azure/o3-mini\"\n",
    "client = create_clients(mode=\"GPT-azure\", model_id=model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a2c06b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(user_prompts,client=client,max_new_tokens=8000,temperature=0.1, system_prompt=None):\n",
    "    responses = client.get_model_response_batch(\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompts=user_prompts,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature\n",
    "\n",
    "    )\n",
    "    return responses \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285bd1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 1489/1489 [05:47<00:00,  4.29it/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = user_prompts\n",
    "responses = generate_response(prompts, client=client, temperature=1)\n",
    "# for i, response in enumerate(responses):\n",
    "#     print(f\"Prompt {i+1}: {prompts[i]}\")\n",
    "#     print(f\"Response {i+1}: {response}\")\n",
    "df['response'] = responses\n",
    "df.to_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/outputs/V10_experiments/o3-mini_results.jsonl\", orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520cc65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e405944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_metrics(metrics_dir):\n",
    "    \"\"\"\n",
    "    Load SSR metrics from JSONL files in a directory.\n",
    "    Expects files named <model>_results_correctness.jsonl\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    for fname in os.listdir(metrics_dir):\n",
    "        if fname.endswith(\"_results_correctness.jsonl\"):\n",
    "            model = fname.replace(\"_results_correctness.jsonl\", \"\")\n",
    "            with open(os.path.join(metrics_dir, fname), \"r\") as f:\n",
    "                data = json.loads(f.read())\n",
    "            # Extract SSR by category\n",
    "            ssr_cat = data[\"SSR_by_Category\"]\n",
    "            # Extract SSR by instruction part and normalize keys\n",
    "            ip = data[\"SSR_by_Instruction_Part\"]\n",
    "            ssr_ip = {\n",
    "                \"Extracted\": ip[\"Extracted from instruction\"],\n",
    "                \"Newly\": ip[\"Newly Generated\"],\n",
    "                \"Original\": ip[\"Original source: 'Extracted from instruction'\"],\n",
    "                \"Combined\": ip[\"Combined/Refined\"]\n",
    "            }\n",
    "            metrics[model] = {\n",
    "                \"SSR_by_Category\": ssr_cat,\n",
    "                \"SSR_by_Instruction_Part\": ssr_ip\n",
    "            }\n",
    "    return metrics\n",
    "\n",
    "def plot_line(df, title, ylabel=\"SSR\"):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for col in df.columns:\n",
    "        plt.plot(df.index, df[col], marker='o', label=col)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"best\", fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_bar(df, title, ylabel=\"SSR\"):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    df.plot.bar()\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"best\", fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Path to folder containing your JSONL files\n",
    "    metrics_dir = \"./metrics_outputs/filtered_final_data\"\n",
    "    metrics = load_metrics(metrics_dir)\n",
    "\n",
    "    # Build DataFrames\n",
    "    df_cat = pd.DataFrame({m: v[\"SSR_by_Category\"] for m, v in metrics.items()})\n",
    "    df_ip  = pd.DataFrame({m: v[\"SSR_by_Instruction_Part\"] for m, v in metrics.items()})\n",
    "\n",
    "    # Define model groups\n",
    "    open_source      = [m for m in metrics if m not in (\"phi-4\",\"Qwen3-8B\")]\n",
    "    closed_source    = [\"phi-4\", \"Qwen3-8B\"]\n",
    "    reasoning_models = [\"phi-4\", \"Qwen3-8B\"]\n",
    "    non_reasoning    = [m for m in metrics if m not in reasoning_models]\n",
    "\n",
    "    # 1. SSR by Category Across Models\n",
    "    plot_line(df_cat, \"SSR by Category Across Models\")\n",
    "\n",
    "    # 2. SSR by Instruction Part Across Models\n",
    "    plot_line(df_ip, \"SSR by Instruction Part Across Models\")\n",
    "\n",
    "    # 3. Avg SSR by Category: Open vs Closed\n",
    "    avg_open_cat   = df_cat[open_source].mean(axis=1)\n",
    "    avg_closed_cat = df_cat[closed_source].mean(axis=1)\n",
    "    df_avg_cat     = pd.DataFrame({\"Open-Source\": avg_open_cat, \"Closed-Source\": avg_closed_cat})\n",
    "    plot_line(df_avg_cat, \"Avg SSR by Category: Open vs Closed\")\n",
    "\n",
    "    # 4. Avg SSR by Category: Reasoning vs Non-Reasoning\n",
    "    avg_reason_cat = df_cat[reasoning_models].mean(axis=1)\n",
    "    avg_non_cat    = df_cat[non_reasoning].mean(axis=1)\n",
    "    df_reason_cat  = pd.DataFrame({\"Reasoning\": avg_reason_cat, \"Non-Reasoning\": avg_non_cat})\n",
    "    plot_line(df_reason_cat, \"Avg SSR by Category: Reasoning vs Non-Reasoning\")\n",
    "\n",
    "    # 5. Avg SSR by Instruction Part: Open vs Closed\n",
    "    avg_open_ip   = df_ip[open_source].mean(axis=1)\n",
    "    avg_closed_ip = df_ip[closed_source].mean(axis=1)\n",
    "    df_avg_ip     = pd.DataFrame({\"Open-Source\": avg_open_ip, \"Closed-Source\": avg_closed_ip})\n",
    "    plot_line(df_avg_ip, \"Avg SSR by Instruction Part: Open vs Closed\")\n",
    "\n",
    "    # 6. Avg SSR by Instruction Part: Reasoning vs Non-Reasoning\n",
    "    avg_reason_ip = df_ip[reasoning_models].mean(axis=1)\n",
    "    avg_non_ip    = df_ip[non_reasoning].mean(axis=1)\n",
    "    df_reason_ip  = pd.DataFrame({\"Reasoning\": avg_reason_ip, \"Non-Reasoning\": avg_non_ip})\n",
    "    plot_line(df_reason_ip, \"Avg SSR by Instruction Part: Reasoning vs Non-Reasoning\")\n",
    "\n",
    "    # 7. Bar chart: Avg SSR by Category (Open vs Closed)\n",
    "    plot_bar(df_avg_cat, \"Bar: Avg SSR by Category\", \"SSR\")\n",
    "\n",
    "    # 8. Bar chart: Avg SSR by Instruction Part (Open vs Closed)\n",
    "    plot_bar(df_avg_ip, \"Bar: Avg SSR by Instruction Part\", \"SSR\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88a878b",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f835af",
   "metadata": {},
   "source": [
    "## CSR Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_josn(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/metrics_outputs/filtered_final_data/metrics_summary_v4.jsonl\", lines=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9f186",
   "metadata": {},
   "source": [
    "# Filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd4ac951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ranked_path = \"LLMjudge_outputs/filtered_final_data/ssr_ranked_dataset.jsonl\"\n",
    "inputpath = \"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v10_v2.jsonl\"\n",
    "output_path = \"benchmark_experiments/benchmark_dataset\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e7b1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import argparse\n",
    "import pandas as pd\n",
    "def get_tail_ids(ranked_jsonl: str, drop_top: int) -> set:\n",
    "    \"\"\"\n",
    "    Load the ranked JSONL, drop the first `drop_top` rows,\n",
    "    and return the set of remaining `id` values.\n",
    "    \"\"\"\n",
    "    ranked = pd.read_json(ranked_jsonl, lines=True)\n",
    "    tail = ranked.iloc[drop_top:]\n",
    "    return set(tail[\"id\"].tolist())\n",
    "\n",
    "def filter_and_save_all(\n",
    "    original_folder: str,\n",
    "    tail_ids: set,\n",
    "    filtered_folder: str\n",
    "):\n",
    "        fname = \"benchmark_data.jsonl\"\n",
    "        # in_path = os.path.join(original_folder, fname)\n",
    "        df = pd.read_json(original_folder, lines=True)\n",
    "        filtered = df[df[\"id\"].isin(tail_ids)]\n",
    "\n",
    "        out_path = os.path.join(filtered_folder, \"benchmark_data.jsonl\")\n",
    "        filtered.to_json(out_path, orient=\"records\", lines=True)\n",
    "        print(f\"Filtered {fname}: {len(df)} → {len(filtered)} rows → {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0dfb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered benchmark_data.jsonl: 1489 → 1000 rows → benchmark_dataset/benchmark_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ranked_path = \"LLMjudge_outputs/filtered_final_data/ssr_ranked_dataset.jsonl\"\n",
    "    # inputpath = \"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v10_v2.jsonl\"\n",
    "    input_folder = \"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v10_v2.jsonl\"\n",
    "    output_folder = \"benchmark_dataset\"\n",
    "    ranked_filename = \"ssr_ranked_dataset.jsonl\"\n",
    "    drop_top = 489 \n",
    "\n",
    "\n",
    "    # ranked_path = rank_and_save(\n",
    "    #     input_folder=input_folder,\n",
    "    #     ranked_folder=output_folder,\n",
    "    #     ranked_filename=ranked_filename\n",
    "    # )\n",
    "\n",
    "    ranked_path = \"LLMjudge_outputs/filtered_final_data/ssr_ranked_dataset.jsonl\"\n",
    "\n",
    "    tail_ids = get_tail_ids(ranked_path, drop_top)\n",
    "    filter_and_save_all(\n",
    "        original_folder=input_folder,\n",
    "        tail_ids=tail_ids,\n",
    "        filtered_folder=output_folder\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f728a82e-f1a8-4e5d-a392-555386c86f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 23 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1000 non-null   int64  \n",
      " 1   dataset                          1000 non-null   object \n",
      " 2   instruction                      1000 non-null   object \n",
      " 3   code                             1000 non-null   object \n",
      " 4   test                             362 non-null    object \n",
      " 5   relevant_categories              1000 non-null   object \n",
      " 6   simplified_instruction           1000 non-null   object \n",
      " 7   extracted_constraints            1000 non-null   object \n",
      " 8   final_comprehensive_constraints  1000 non-null   object \n",
      " 9   filtered_relevant_constraints    1000 non-null   object \n",
      " 10  quality_scores                   1000 non-null   object \n",
      " 11  relevance_score                  1000 non-null   float64\n",
      " 12  objectivity_score                1000 non-null   float64\n",
      " 13  atomicity_score                  1000 non-null   float64\n",
      " 14  unified_quality_score            1000 non-null   float64\n",
      " 15  combined_instruction             1000 non-null   object \n",
      " 16  constraint_wise_presence         1000 non-null   object \n",
      " 17  constraint_presence_response     1000 non-null   object \n",
      " 18  final_constraints                1000 non-null   object \n",
      " 19  instruction_difficulty_labels    1000 non-null   object \n",
      " 20  difficulty_response              1000 non-null   object \n",
      " 21  constraint_difficulty_labels     1000 non-null   object \n",
      " 22  constraint_difficulty_response   1000 non-null   object \n",
      "dtypes: float64(4), int64(1), object(18)\n",
      "memory usage: 179.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"benchmark_dataset/benchmark_data.jsonl\",lines=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17e0df40-4bec-4cd4-a91b-95abd2b44b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your DataFrame (assuming it's already loaded as df)\n",
    "\n",
    "# Reset index and drop the old index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update the 'id' column with the new index values\n",
    "df['id'] = df.index\n",
    "\n",
    "# Optional: Save to CSV or JSONL\n",
    "# df.to_csv(\"benchmark_data_with_reset_id.csv\", index=False)\n",
    "# or\n",
    "# df.to_json(\"benchmark_data_with_reset_id.jsonl\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e63c466-86a9-40e9-a233-2ebb025e435a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>instruction</th>\n",
       "      <th>code</th>\n",
       "      <th>test</th>\n",
       "      <th>relevant_categories</th>\n",
       "      <th>simplified_instruction</th>\n",
       "      <th>extracted_constraints</th>\n",
       "      <th>final_comprehensive_constraints</th>\n",
       "      <th>filtered_relevant_constraints</th>\n",
       "      <th>...</th>\n",
       "      <th>atomicity_score</th>\n",
       "      <th>unified_quality_score</th>\n",
       "      <th>combined_instruction</th>\n",
       "      <th>constraint_wise_presence</th>\n",
       "      <th>constraint_presence_response</th>\n",
       "      <th>final_constraints</th>\n",
       "      <th>instruction_difficulty_labels</th>\n",
       "      <th>difficulty_response</th>\n",
       "      <th>constraint_difficulty_labels</th>\n",
       "      <th>constraint_difficulty_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>Write a Python program that simulates a simple...</td>\n",
       "      <td>```python\\nimport threading\\nimport time\\nimpo...</td>\n",
       "      <td>None</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Write a Python program that simulates a simple...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Write a Python program that simulates a simple...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>{\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>medium</td>\n",
       "      <td>```json\\n{\\n  \"Reason\": \"The task involves usi...</td>\n",
       "      <td>[easy, hard, easy, easy, hard, easy, easy, har...</td>\n",
       "      <td>```json\\n{\\n  \"Classification\": [\\n    {\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>Design a Python class-based system to manage t...</td>\n",
       "      <td>```python\\n# -*- coding: utf-8 -*-\\nimport imp...</td>\n",
       "      <td>None</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Design a Python class-based system to manage t...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>...</td>\n",
       "      <td>4.82</td>\n",
       "      <td>4.88</td>\n",
       "      <td>Design a Python class-based system to manage t...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>{\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>hard</td>\n",
       "      <td>```json\\n{\\n  \"Reason\": \"The task involves des...</td>\n",
       "      <td>[easy, easy, hard, hard, easy, hard, hard, eas...</td>\n",
       "      <td>```json\\n{\\n  \"Classification\": [\\n    {\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>Design a simple MVC (Model-View-Controller) ap...</td>\n",
       "      <td>```python\\n# Import necessary packages\\nimport...</td>\n",
       "      <td>None</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Design a simple MVC (Model-View-Controller) ap...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>...</td>\n",
       "      <td>4.83</td>\n",
       "      <td>4.83</td>\n",
       "      <td>Design a simple MVC (Model-View-Controller) ap...</td>\n",
       "      <td>[True, True, True, True, True, True]</td>\n",
       "      <td>{\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>medium</td>\n",
       "      <td>```json\\n{\\n  \"Reason\": \"The task involves des...</td>\n",
       "      <td>[easy, easy, hard, hard, hard, hard]</td>\n",
       "      <td>```json\\n{\\n  \"Classification\": [\\n    {\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>Create a Python script that provides a command...</td>\n",
       "      <td>```python\\nimport argparse\\nimport fnmatch\\nim...</td>\n",
       "      <td>None</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Create a Python script that provides a command...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Create a Python script that provides a command...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>{\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>medium</td>\n",
       "      <td>```json\\n{\\n  \"Reason\": \"The task involves mul...</td>\n",
       "      <td>[easy, hard, hard, hard, easy]</td>\n",
       "      <td>```json\\n{\\n  \"Classification\": [\\n    {\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>Design a Python class `PluginManager` that man...</td>\n",
       "      <td>```python\\n# Import necessary packages\\nfrom t...</td>\n",
       "      <td>None</td>\n",
       "      <td>['Code Structure and Modularity', 'Error Handl...</td>\n",
       "      <td>Design a Python class `PluginManager` that man...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Design a Python class `PluginManager` that man...</td>\n",
       "      <td>[True, True, True, True, True, True, True, True]</td>\n",
       "      <td>{\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>medium</td>\n",
       "      <td>```json\\n{\\n  \"Reason\": \"The task involves cre...</td>\n",
       "      <td>[easy, easy, easy, easy, easy, easy, easy, easy]</td>\n",
       "      <td>```json\\n{\\n  \"Classification\": [\\n    {\\n    ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                      dataset  \\\n",
       "995  995  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "996  996  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "997  997  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "998  998  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "999  999  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "\n",
       "                                           instruction  \\\n",
       "995  Write a Python program that simulates a simple...   \n",
       "996  Design a Python class-based system to manage t...   \n",
       "997  Design a simple MVC (Model-View-Controller) ap...   \n",
       "998  Create a Python script that provides a command...   \n",
       "999  Design a Python class `PluginManager` that man...   \n",
       "\n",
       "                                                  code  test  \\\n",
       "995  ```python\\nimport threading\\nimport time\\nimpo...  None   \n",
       "996  ```python\\n# -*- coding: utf-8 -*-\\nimport imp...  None   \n",
       "997  ```python\\n# Import necessary packages\\nimport...  None   \n",
       "998  ```python\\nimport argparse\\nimport fnmatch\\nim...  None   \n",
       "999  ```python\\n# Import necessary packages\\nfrom t...  None   \n",
       "\n",
       "                                   relevant_categories  \\\n",
       "995  ['Code Structure and Modularity', 'Input and O...   \n",
       "996  ['Code Structure and Modularity', 'Input and O...   \n",
       "997  ['Code Structure and Modularity', 'Input and O...   \n",
       "998  ['Code Structure and Modularity', 'Input and O...   \n",
       "999  ['Code Structure and Modularity', 'Error Handl...   \n",
       "\n",
       "                                simplified_instruction  \\\n",
       "995  Write a Python program that simulates a simple...   \n",
       "996  Design a Python class-based system to manage t...   \n",
       "997  Design a simple MVC (Model-View-Controller) ap...   \n",
       "998  Create a Python script that provides a command...   \n",
       "999  Design a Python class `PluginManager` that man...   \n",
       "\n",
       "                                 extracted_constraints  \\\n",
       "995  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "996  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "997  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "998  [{'type': 'File and Data Management', 'constra...   \n",
       "999  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "\n",
       "                       final_comprehensive_constraints  \\\n",
       "995  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "996  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "997  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "998  [{'type': 'File and Data Management', 'constra...   \n",
       "999  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "\n",
       "                         filtered_relevant_constraints  ... atomicity_score  \\\n",
       "995  [{'type': 'Code Structure and Modularity', 'co...  ...            5.00   \n",
       "996  [{'type': 'Code Structure and Modularity', 'co...  ...            4.82   \n",
       "997  [{'type': 'Code Structure and Modularity', 'co...  ...            4.83   \n",
       "998  [{'type': 'File and Data Management', 'constra...  ...            5.00   \n",
       "999  [{'type': 'Code Structure and Modularity', 'co...  ...            5.00   \n",
       "\n",
       "     unified_quality_score                               combined_instruction  \\\n",
       "995                   5.00  Write a Python program that simulates a simple...   \n",
       "996                   4.88  Design a Python class-based system to manage t...   \n",
       "997                   4.83  Design a simple MVC (Model-View-Controller) ap...   \n",
       "998                   5.00  Create a Python script that provides a command...   \n",
       "999                   5.00  Design a Python class `PluginManager` that man...   \n",
       "\n",
       "                              constraint_wise_presence  \\\n",
       "995  [True, True, True, True, True, True, True, Tru...   \n",
       "996  [True, True, True, True, True, True, True, Tru...   \n",
       "997               [True, True, True, True, True, True]   \n",
       "998                     [True, True, True, True, True]   \n",
       "999   [True, True, True, True, True, True, True, True]   \n",
       "\n",
       "                          constraint_presence_response  \\\n",
       "995  {\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...   \n",
       "996  {\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...   \n",
       "997  {\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...   \n",
       "998  {\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...   \n",
       "999  {\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...   \n",
       "\n",
       "                                     final_constraints  \\\n",
       "995  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "996  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "997  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "998  [{'type': 'File and Data Management', 'constra...   \n",
       "999  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "\n",
       "    instruction_difficulty_labels  \\\n",
       "995                        medium   \n",
       "996                          hard   \n",
       "997                        medium   \n",
       "998                        medium   \n",
       "999                        medium   \n",
       "\n",
       "                                   difficulty_response  \\\n",
       "995  ```json\\n{\\n  \"Reason\": \"The task involves usi...   \n",
       "996  ```json\\n{\\n  \"Reason\": \"The task involves des...   \n",
       "997  ```json\\n{\\n  \"Reason\": \"The task involves des...   \n",
       "998  ```json\\n{\\n  \"Reason\": \"The task involves mul...   \n",
       "999  ```json\\n{\\n  \"Reason\": \"The task involves cre...   \n",
       "\n",
       "                          constraint_difficulty_labels  \\\n",
       "995  [easy, hard, easy, easy, hard, easy, easy, har...   \n",
       "996  [easy, easy, hard, hard, easy, hard, hard, eas...   \n",
       "997               [easy, easy, hard, hard, hard, hard]   \n",
       "998                     [easy, hard, hard, hard, easy]   \n",
       "999   [easy, easy, easy, easy, easy, easy, easy, easy]   \n",
       "\n",
       "                        constraint_difficulty_response  \n",
       "995  ```json\\n{\\n  \"Classification\": [\\n    {\\n    ...  \n",
       "996  ```json\\n{\\n  \"Classification\": [\\n    {\\n    ...  \n",
       "997  ```json\\n{\\n  \"Classification\": [\\n    {\\n    ...  \n",
       "998  ```json\\n{\\n  \"Classification\": [\\n    {\\n    ...  \n",
       "999  ```json\\n{\\n  \"Classification\": [\\n    {\\n    ...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8ee5c5a-c566-48da-942a-d5641dab967d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'dataset', 'instruction', 'code', 'test', 'relevant_categories',\n",
       "       'simplified_instruction', 'extracted_constraints',\n",
       "       'final_comprehensive_constraints', 'filtered_relevant_constraints',\n",
       "       'quality_scores', 'relevance_score', 'objectivity_score',\n",
       "       'atomicity_score', 'unified_quality_score', 'combined_instruction',\n",
       "       'constraint_wise_presence', 'constraint_presence_response',\n",
       "       'final_constraints', 'instruction_difficulty_labels',\n",
       "       'difficulty_response', 'constraint_difficulty_labels',\n",
       "       'constraint_difficulty_response'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64418619-0f20-49cd-93e4-8bdf534e5787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "Multilingual-Multimodal-NLP/McEval-Instruct    373\n",
       "ajibawa-2023/Python-Code-23k-ShareGPT          265\n",
       "bigcode/bigcodebench                           183\n",
       "xlangai/DS-1000                                179\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"dataset\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9218f928-3735-4418-be1b-211f45d258e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['instruction', 'code', 'test', 'relevant_categories',\n",
    "       'simplified_instruction', 'extracted_constraints',\n",
    "       'final_comprehensive_constraints', 'filtered_relevant_constraints',\n",
    "       'quality_scores', 'relevance_score', 'objectivity_score',\n",
    "       'atomicity_score', 'unified_quality_score',\n",
    "       'constraint_wise_presence', 'constraint_presence_response',\n",
    "       'difficulty_response', 'constraint_difficulty_labels',\n",
    "       'constraint_difficulty_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8456d1e-01e6-4dd6-8045-9db10040371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 5 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   id                             1000 non-null   int64 \n",
      " 1   dataset                        1000 non-null   object\n",
      " 2   combined_instruction           1000 non-null   object\n",
      " 3   final_constraints              1000 non-null   object\n",
      " 4   instruction_difficulty_labels  1000 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 39.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "919a16e1-8662-4c74-913a-9c2fc0f4acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns= {\"dataset\" : \"source_dataset\", \"combined_instruction\":\"instruction\", \"final_constraints\":\"constraints\",\"instruction_difficulty_labels\":\"instruction_difficulty\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45ca40f0-6506-41ec-b6f0-3a01ceb940e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 5 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   id                      1000 non-null   int64 \n",
      " 1   source_dataset          1000 non-null   object\n",
      " 2   instruction             1000 non-null   object\n",
      " 3   constraints             1000 non-null   object\n",
      " 4   instruction_difficulty  1000 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 39.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3d7d4e2-f362-43b3-b2f9-3df6cc82e02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Input and Output Handling',\n",
       "  'constraint': 'Exclude any combination that contains the number 5.',\n",
       "  'instruction_part': 'Extracted from instruction'},\n",
       " {'type': 'Input and Output Handling',\n",
       "  'constraint': 'Exclude any combination that contains a repeating digit.',\n",
       "  'instruction_part': 'Extracted from instruction'},\n",
       " {'type': 'Code Structure and Modularity',\n",
       "  'constraint': 'Implement the solution without using any built-in functions or libraries to check for repeating digits.',\n",
       "  'instruction_part': 'Extracted from instruction'},\n",
       " {'type': 'Testing and Debugging',\n",
       "  'constraint': 'Create unit tests to verify that all combinations generated meet the specified constraints of excluding the number 5 and repeating digits.',\n",
       "  'instruction_part': 'Newly Generated'},\n",
       " {'type': 'Input and Output Handling',\n",
       "  'constraint': 'Ensure the output format is consistent, such as printing each combination on a new line or in a specified format.',\n",
       "  'instruction_part': 'Newly Generated'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"constraints\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd4599f4-6270-4b87-a391-803ca4d6b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a function to remove 'instruction_part' from each constraint dict\n",
    "def remove_instruction_part(constraints):\n",
    "    return [{k: v for k, v in constraint.items() if k != \"instruction_part\"} for constraint in constraints]\n",
    "\n",
    "# Apply the function to the entire column\n",
    "df[\"constraints\"] = df[\"constraints\"].apply(remove_instruction_part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c6e9e11-d37d-4a63-b52e-b83a3b9b747c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Input and Output Handling',\n",
       "  'constraint': 'Exclude any combination that contains the number 5.'},\n",
       " {'type': 'Input and Output Handling',\n",
       "  'constraint': 'Exclude any combination that contains a repeating digit.'},\n",
       " {'type': 'Code Structure and Modularity',\n",
       "  'constraint': 'Implement the solution without using any built-in functions or libraries to check for repeating digits.'},\n",
       " {'type': 'Testing and Debugging',\n",
       "  'constraint': 'Create unit tests to verify that all combinations generated meet the specified constraints of excluding the number 5 and repeating digits.'},\n",
       " {'type': 'Input and Output Handling',\n",
       "  'constraint': 'Ensure the output format is consistent, such as printing each combination on a new line or in a specified format.'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"constraints\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db8a617d-934f-4b8b-8320-60b2fbbd2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"benchmark_dataset/benchmark_data_final.jsonl\",orient=\"records\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d9137-e583-4d2b-8030-af18ec6f69f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
