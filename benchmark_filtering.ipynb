{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0df0c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dccstor/shanmukh/sravani_internship/benchmark_experiments\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3da35",
   "metadata": {},
   "source": [
    "# Importing Clients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8df430e-926a-4552-ad4b-8f9afbeabf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.compute_clients import create_clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ba4ee-33b3-4c44-926e-bbf9324b6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"benchmark_v4.csv\")\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0bdc0cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.tail(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c3d37",
   "metadata": {},
   "source": [
    "# Combined Instruction Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35b98d",
   "metadata": {},
   "source": [
    "## Prompt Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd502db-b74b-4a94-a4f7-3e80a9e6bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_prompt_v1(inst,constraints):\n",
    "    return f\"\"\"Given the following programming instruction and a list of constraints, verify if all constraints are **explicitly mentioned** in the instruction. \n",
    "\n",
    "Instruction:\n",
    "{inst}\n",
    "\n",
    "Constraints:\n",
    "{constraints}\n",
    "\n",
    "Respond with \"Yes\" if all constraints are included in the instruction. Otherwise, respond with \"No\".\n",
    "**Do not infer, assume, or deduce** that a constraint is satisfied unless it is **clearly mentioned **.\n",
    "If a constraint is only implied or suggested, mark it as **not satisfied**.\n",
    "Do **not use your own interpretation** or **general knowledge** about what the instruction might mean — judge only what is **actually written**.\n",
    "output format:\n",
    "\n",
    "{{\n",
    "   reason: why is it yes or no if it is no give the list of the constraints missing in the instruction\n",
    "   aligns: yes/no\n",
    "   }}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c5398",
   "metadata": {},
   "source": [
    "## Prompt Version 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "013ae413-6f01-4bab-ac83-c6eaa1841713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def build_user_prompt_v2(instruction, constraints):\n",
    "        return f\"\"\"You are a verifier. Your task is to evaluate whether a given instruction includes a set of constraints.\n",
    "\n",
    "You will be provided:\n",
    "- An instruction\n",
    "- A list of constraints\n",
    "\n",
    "Your task:\n",
    "- Check if each constraint is explicitly mentioned in the instruction.\n",
    "- **Do not infer, assume, or deduce** that a constraint is satisfied unless it is **clearly mentioned **.\n",
    "- If a constraint is only implied or suggested, mark it as **not satisfied**.\n",
    "- Do **not use your own interpretation** or **general knowledge** about what the instruction might mean — judge only what is **actually written**.\n",
    "- Output your reasoning in a valid JSON format like below:\n",
    "\n",
    "{{\n",
    "  \"Evaluation\": [\n",
    "    {{\n",
    "      \"Constraint\": \"<constraint text>\",\n",
    "      \"Reason\": \"explanation\",\n",
    "      \"Aligns\": [true|false]\n",
    "    }},\n",
    "    ...\n",
    "  ],\n",
    "  \"FinalDecision\": {{\n",
    "    \"Score\": \"<number of constraints met>/<total number of constraints>\",\n",
    "    \"Reason\": \"<Yes if all constraints were satisfied; No if any were violated, with explanation>\",\n",
    "    \"Aligns\": [true|false]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Do not include any extra commentary. Return only valid JSON.\n",
    "\n",
    "[Instruction]:\n",
    "{instruction}\n",
    "\n",
    "[Constraints]:\n",
    "{constraints}\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f2ee6e",
   "metadata": {},
   "source": [
    "## Prompt Version 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d21f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def build_user_prompt_v3(instruction, constraints):\n",
    "        return f\"\"\"You are a verifier. Your task is to evaluate whether a given instruction includes a set of constraints.\n",
    "\n",
    "You will be provided:\n",
    "- An instruction\n",
    "- A list of constraints\n",
    "\n",
    "Your task:\n",
    "- Check if each constraint is explicitly mentioned in the instruction.\n",
    "- **Do not infer, assume, or deduce** that a constraint is satisfied unless it is **clearly mentioned **.\n",
    "- If a constraint is only implied or suggested, mark it as **not satisfied**.\n",
    "- Do **not use your own interpretation** or **general knowledge** about what the instruction might mean — judge only what is **actually written**.\n",
    "- Output your reasoning in a valid JSON format like below:\n",
    "\n",
    "{{\n",
    "    \"FinalDecision\": {{\n",
    "    \"Score\": \"<number of constraints met>/<total number of constraints>\",\n",
    "    \"Reason\": \"<Yes if all constraints were satisfied; No if any were violated, with explanation>\",\n",
    "    \"Aligns\": [true|false]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Do not include any extra commentary. Return only valid JSON.\n",
    "\n",
    "[Instruction]:\n",
    "{instruction}\n",
    "\n",
    "[Constraints]:\n",
    "{constraints}\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d0bd8c-56e9-4892-b6b2-f34a4bd024e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 24\n",
    "filtered_relevant_constraints = df.iloc[n,9]\n",
    "combined_instruction = df.iloc[n,15]\n",
    "\n",
    "print(combined_instruction)\n",
    "print(filtered_relevant_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd3a79-9fb5-46d3-b437-78d9f1438bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_user_prompt_v1(combined_instruction, filtered_relevant_constraints)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf4dbf-b463-434c-bacc-00cdc57dff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_example = f\"\"\"\n",
    "Create a function that takes in two numbers as arguments and returns the product of the two. Additionally, your solution should have a time complexity of O(log n), where n is the larger of the two input numbers. The function should handle edge cases, such as when one or both input numbers are zero, and return the correct product. You should instead implement your own logic to calculate the product using only bitwise operations such as bitwise shift and bitwise AND, as well as basic arithmetic operations such as addition, subtraction, and division. Furthermore, the function should raise an appropriate error if the inputs are not integers. To enhance performance, the implementation should minimize the number of additions performed during the multiplication process. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_example_2 = \"\"\"\n",
    "Develop an algorithm to find the value of n where n is the index of the Fibonacci sequence. The algorithm must be implemented using a recursive approach, ensuring that it correctly returns the Fibonacci number for all valid inputs, including edge cases like n = 0 and n = 1. Additionally, the algorithm must not use any built-in mathematical functions or libraries for calculating Fibonacci numbers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e37df1",
   "metadata": {},
   "source": [
    "## Testing the Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a404bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 19 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(14)\n",
      "memory usage: 221.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"benchmark_dataset/benchmark_v9.jsonl\", lines=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0554bd76-8b56-4e9c-96fe-e1c2adb95449",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from utils.compute_clients import create_clients\n",
    "api_key = os.getenv(\"IBM_OPENAI_API_KEY\")\n",
    "client = create_clients(mode=\"GPT-azure\")\n",
    "def get_response(n=0,type=\"v1\",wrong=False):\n",
    "    \n",
    "    # combined_instruction = df.iloc[n,15]\n",
    "    if wrong:\n",
    "        combined_instruction = wrong_example_2\n",
    "        filtered_relevant_constraints = df.iloc[24,9]\n",
    "    else:\n",
    "        combined_instruction = df.iloc[n,15]\n",
    "        print\n",
    "        filtered_relevant_constraints = df.iloc[n,9]\n",
    "    if type == \"v1\":\n",
    "        prompt = build_user_prompt_v1(combined_instruction, filtered_relevant_constraints)\n",
    "    elif type ==\"v2\":\n",
    "        prompt = build_user_prompt_v2(combined_instruction, filtered_relevant_constraints)\n",
    "    elif type == \"v3\":\n",
    "        prompt = build_user_prompt_v3(combined_instruction, filtered_relevant_constraints)\n",
    "        \n",
    "    response = client.get_model_response(user_prompt=prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "961a795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 19 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(14)\n",
      "memory usage: 221.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v8.jsonl\",lines=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf5c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 215 entries, 375 to 589\n",
      "Data columns (total 19 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               215 non-null    int64  \n",
      " 1   dataset                          215 non-null    object \n",
      " 2   instruction                      215 non-null    object \n",
      " 3   code                             215 non-null    object \n",
      " 4   test                             215 non-null    object \n",
      " 5   relevant_categories              215 non-null    object \n",
      " 6   simplified_instruction           215 non-null    object \n",
      " 7   extracted_constraints            215 non-null    object \n",
      " 8   final_comprehensive_constraints  215 non-null    object \n",
      " 9   filtered_relevant_constraints    215 non-null    object \n",
      " 10  quality_scores                   215 non-null    object \n",
      " 11  relevance_score                  215 non-null    float64\n",
      " 12  objectivity_score                215 non-null    float64\n",
      " 13  atomicity_score                  215 non-null    float64\n",
      " 14  unified_quality_score            215 non-null    float64\n",
      " 15  combined_instruction             215 non-null    object \n",
      " 16  constraint_wise_presence         215 non-null    object \n",
      " 17  constraint_presence_response     215 non-null    object \n",
      " 18  final_constraints                215 non-null    object \n",
      "dtypes: float64(4), int64(1), object(14)\n",
      "memory usage: 33.6+ KB\n"
     ]
    }
   ],
   "source": [
    "filter_df = df[df[\"dataset\"]==\"xlangai/DS-1000\"]\n",
    "filter_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334084af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"FinalDecision\": {\\n    \"Score\": \"8/8\",\\n    \"Reason\": \"All constraints are explicitly mentioned in the instruction: no use of multiplication operator; no built-in functions for product; time complexity O(log n); use of bitwise operations (shift and AND); use of basic arithmetic operations (addition, subtraction, division); handling edge cases including zero inputs; raising error for non-integer inputs; minimizing additions for performance.\",\\n    \"Aligns\": true\\n  }\\n}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(14,\"v3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096d73e-c7e7-4bbf-9018-e1d5dd58d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(14,\"v2\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e424b8-83c0-4529-996b-43e5b53eef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(14,\"v1\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(24,\"v1\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(24,\"v2\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(24,\"v3\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "462f77ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 19 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(14)\n",
      "memory usage: 221.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v9_v2.jsonl\",lines=True)\n",
    "df.info()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df425817",
   "metadata": {},
   "source": [
    "# Classifying the Instruction difficulty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cbee7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difficulty_classification_prompt(instruction):\n",
    "    return f\"\"\"You are a difficulty evaluator for programming tasks.\n",
    "\n",
    "Your job is to classify a given programming instruction into one of three categories based on its overall difficulty:\n",
    "\n",
    "- Easy: The task involves basic programming logic, no complex algorithms, and minimal constraints.\n",
    "- Medium: The task involves little complexity such as use of specific data structures, standard algorithms, or multiple conditions to satisfy.\n",
    "- Hard: The task involves moderate to high complexity logic, performance optimization, advanced algorithmic thinking, or multiple non-trivial constraints.\n",
    "\n",
    "Only use the content explicitly mentioned in the instruction. Do not assume extra difficulty unless it is clearly indicated.\n",
    "\n",
    "Respond in the following JSON format:\n",
    "\n",
    "{{\"Reason\": \"<Brief explanation for the chosen difficulty>\",\n",
    "  \"Difficulty\": \"<Easy | Medium | Hard>\",\n",
    "  \n",
    "}}\n",
    "\n",
    "[Instruction]:\n",
    "{instruction}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a115aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from utils.compute_clients import create_clients\n",
    "api_key = os.getenv(\"IBM_OPENAI_API_KEY\")\n",
    "client = create_clients(mode=\"GPT-azure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e137b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_first_difficulty(text):\n",
    "    m = re.search(r'\"Difficulty\"\\s*:\\s*\"(Easy|Medium|Hard)\"', text)\n",
    "    return m.group(1) if m else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68435b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_difficulty_value(text):\n",
    "    \"\"\"\n",
    "    Use regex to find the first occurrence of \"Difficulty\": \"Easy\", \"Medium\", or \"Hard\"\n",
    "    and return it as a string (without quotes), or None if not found.\n",
    "    \"\"\"\n",
    "    matches = re.findall(\n",
    "        r'\"Difficulty\"\\s*:\\s*(\"(?:Easy|Medium|Hard)\")',\n",
    "        text\n",
    "    )\n",
    "    return matches[0].strip('\"') if matches else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "704e0a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design a class to represent a point in 2D space. It should store two variables—x and y.\n",
      "\n",
      "Additionally, implement the following methods:\n",
      "1. A method to calculate the distance between two points, taking into account the Earth's curvature. Ensure that this method handles any edge cases appropriately and uses efficient algorithms for geometric calculations to minimize computational complexity.\n",
      "2. A method to calculate the midpoint between two points, considering the elevation difference between the points. This method should also implement input validation to ensure parameters are of the expected types and within valid ranges.\n",
      "3. A method to check if a given point is inside a given rectangle, considering the rectangle's rotation and scale. Ensure that each method in the class is modular and adheres to the single responsibility principle.\n",
      "4. A method to check if a given point is inside a given circle, considering the circle's elevation and rotation.\n",
      "5. A method to rotate the point around the origin by a given angle, while maintaining the point's elevation.\n",
      "6. A method to calculate the reflection of the point across a given line in 3D space.\n",
      "7. A method to check if the point lies on a given line in 3D space, considering the line's elevation and rotation.\n",
      "8. A method to check if the point is collinear with two other given points in 3D space, considering their elevations.\n",
      "9. A method to calculate the slope of the line connecting the point to another given point, accounting for the Earth's curvature.\n",
      "10. A method to find the closest point on a given line to the point, considering the line's elevation and rotation.\n",
      "\n",
      "Ensure that your implementation optimizes the performance of the methods.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:\n",
      " ```json\n",
      "{\n",
      "  \"Reason\": \"The task requires implementing a class with multiple complex geometric methods that consider Earth's curvature, 3D space, elevation, rotation, and scaling. It involves advanced geometric calculations, input validation, modular design principles, and performance optimization, all of which indicate moderate to high complexity.\",\n",
      "  \"Difficulty\": \"Hard\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "id = 364\n",
    "combined_instruction = df[df[\"id\"]==id][\"combined_instruction\"].values[0]\n",
    "# constraints = df.iloc[n,18]\n",
    "print(combined_instruction)\n",
    "# print(constraints)\n",
    "prompt = difficulty_classification_prompt(combined_instruction)\n",
    "response = client.get_model_response(user_prompt=prompt)\n",
    "print(\"Classification:\\n\",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c7a179e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Easy'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_first_difficulty(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9279251f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Easy'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_difficulty_value(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d77f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_aligns_values(text):\n",
    "    # Use regex to find all occurrences of \"Aligns\": true or \"Aligns\": false\n",
    "    matches = re.findall(r'\"Aligns\"\\s*:\\s*(true|false)', text, re.IGNORECASE)\n",
    "    # Convert string values to boolean\n",
    "    return [m.lower() == 'true' for m in matches]\n",
    "\n",
    "# Example usage\n",
    "input_text = '''\n",
    "{\n",
    "  \"Evaluation\": [\n",
    "    {\n",
    "      \"Constraint\": \"Remove specific directories (e.g., `dist`, `build`, `.eggs`) and their contents.\",\n",
    "      \"Reason\": \"The instruction explicitly states: 'Remove specific directories (e.g., `dist`, `build`, `.eggs`) and their contents, ensuring that all specified directories are thoroughly cleaned.'\",\n",
    "      \"Aligns\": true\n",
    "    },\n",
    "    {\n",
    "      \"Constraint\": \"Recursively remove directories and files matching certain patterns (e.g., `__pycache__`, `*.pyc`).\",\n",
    "      \"Reason\": \"The instruction explicitly states: 'Recursively remove directories and files matching certain patterns (e.g., `__pycache__`, `*.pyc`), allowing for comprehensive cleanup of unwanted files.'\",\n",
    "      \"Aligns\": true\n",
    "    },\n",
    "    {\n",
    "      \"Constraint\": \"Remove files matching patterns in specific directories (e.g., `*.png` and `*.yml` files in `docs/test_to_docs`).\",\n",
    "      \"Reason\": \"The instruction explicitly states: 'Remove files matching patterns in specific directories (e.g., `*.png` and `*.yml` files in `docs/test_to_docs`), targeting specific file types for removal.'\",\n",
    "      \"Aligns\": true\n",
    "    },\n",
    "    {\n",
    "      \"Constraint\": \\\"Provide a dry-run option that lists the files and directories that would be removed without actually deleting them.\\\",\n",
    "      \"Reason\": \"The instruction explicitly states: 'Provide a dry-run option that lists the files and directories that would be removed without actually deleting them, giving users a preview of the cleanup process.'\",\n",
    "      \"Aligns\": true\n",
    "    },\n",
    "    {\n",
    "      \"Constraint\": \"Print the actions it is taking (e.g., 'Removing: dist/').\",\n",
    "      \"Reason\": \"The instruction explicitly states: 'Print the actions it is taking (e.g., \\\"Removing: dist/\\\")', enhancing the script's documentation and readability.\",\n",
    "      \"Aligns\": true\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "'''\n",
    "\n",
    "aligns = extract_aligns_values(input_text)\n",
    "print(aligns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57ddc9",
   "metadata": {},
   "source": [
    "# Verifying the Combined Instruction for benchmark_v9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd4f2568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 19 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(14)\n",
      "memory usage: 221.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correct file path and use lines=True for JSONL format\n",
    "df = pd.read_json(\"benchmark_dataset/benchmark_v9.jsonl\", lines=True)\n",
    "\n",
    "# Display basic info about the DataFrame\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9be381",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"filtered_relevant_constraints\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdaf9550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True]\n",
      "[True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "presence_raw = df[\"constraint_wise_presence\"][500]\n",
    "print(df[\"constraint_wise_presence\"][500])\n",
    "sum(df[\"constraint_wise_presence\"][500])\n",
    "presence = [p for p in presence_raw if p]\n",
    "print(presence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e18e8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 215 entries, 375 to 589\n",
      "Data columns (total 19 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               215 non-null    int64  \n",
      " 1   dataset                          215 non-null    object \n",
      " 2   instruction                      215 non-null    object \n",
      " 3   code                             215 non-null    object \n",
      " 4   test                             215 non-null    object \n",
      " 5   relevant_categories              215 non-null    object \n",
      " 6   simplified_instruction           215 non-null    object \n",
      " 7   extracted_constraints            215 non-null    object \n",
      " 8   final_comprehensive_constraints  215 non-null    object \n",
      " 9   filtered_relevant_constraints    215 non-null    object \n",
      " 10  quality_scores                   215 non-null    object \n",
      " 11  relevance_score                  215 non-null    float64\n",
      " 12  objectivity_score                215 non-null    float64\n",
      " 13  atomicity_score                  215 non-null    float64\n",
      " 14  unified_quality_score            215 non-null    float64\n",
      " 15  combined_instruction             215 non-null    object \n",
      " 16  constraint_wise_presence         215 non-null    object \n",
      " 17  constraint_presence_response     215 non-null    object \n",
      " 18  final_constraints                215 non-null    object \n",
      "dtypes: float64(4), int64(1), object(14)\n",
      "memory usage: 33.6+ KB\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df[df[\"dataset\"]==\"xlangai/DS-1000\"]\n",
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d8e5698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10 entries, 376 to 586\n",
      "Data columns (total 19 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               10 non-null     int64  \n",
      " 1   dataset                          10 non-null     object \n",
      " 2   instruction                      10 non-null     object \n",
      " 3   code                             10 non-null     object \n",
      " 4   test                             10 non-null     object \n",
      " 5   relevant_categories              10 non-null     object \n",
      " 6   simplified_instruction           10 non-null     object \n",
      " 7   extracted_constraints            10 non-null     object \n",
      " 8   final_comprehensive_constraints  10 non-null     object \n",
      " 9   filtered_relevant_constraints    10 non-null     object \n",
      " 10  quality_scores                   10 non-null     object \n",
      " 11  relevance_score                  10 non-null     float64\n",
      " 12  objectivity_score                10 non-null     float64\n",
      " 13  atomicity_score                  10 non-null     float64\n",
      " 14  unified_quality_score            10 non-null     float64\n",
      " 15  combined_instruction             10 non-null     object \n",
      " 16  constraint_wise_presence         10 non-null     object \n",
      " 17  constraint_presence_response     10 non-null     object \n",
      " 18  final_constraints                10 non-null     object \n",
      "dtypes: float64(4), int64(1), object(14)\n",
      "memory usage: 1.6+ KB\n"
     ]
    }
   ],
   "source": [
    "abnormal_ds_1000 = filtered_df[[len(x)<len(y) for x,y in zip(filtered_df[\"combined_instruction\"], filtered_df[\"instruction\"])]]\n",
    "abnormal_ds_1000.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7eaacfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1192, 1255), (1794, 1825), (858, 860), (1528, 1533), (1216, 1218), (1692, 1709), (990, 1124), (1442, 1449), (1688, 1695), (933, 936)]\n"
     ]
    }
   ],
   "source": [
    "lengths = [(len(row[\"combined_instruction\"]), len(row[\"instruction\"])) for row in abnormal_ds_1000.to_dict(orient='records')]\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e02d3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_ds_1000.to_json(\"response_outputs/testing_responses/ds_1000_abnormal_combined_instruct_length.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35e107cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489\n",
      "0.9707961309523809\n",
      "10438\n",
      "314\n",
      "10752\n"
     ]
    }
   ],
   "source": [
    "stats = df[\"constraint_wise_presence\"].to_list()\n",
    "print(len(stats))\n",
    "stat_final =[]\n",
    "for stat in stats:\n",
    "    stat_final.extend(stat)\n",
    "print(sum(stat_final)/len(stat_final))\n",
    "print(sum(stat_final))\n",
    "print(len(stat_final)-sum(stat_final))\n",
    "print(len(stat_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed931ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(df):\n",
    "    stats = df[\"constraint_wise_presence\"].to_list()\n",
    "    print(len(stats))\n",
    "    stat_final =[]\n",
    "    for stat in stats:\n",
    "        stat_final.extend(stat)\n",
    "    print(sum(stat_final)/len(stat_final))\n",
    "    print(sum(stat_final))\n",
    "    print(len(stat_final)-sum(stat_final))\n",
    "    print(len(stat_final))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe835839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489\n",
      "0.9707961309523809\n",
      "10438\n",
      "314\n",
      "10752\n"
     ]
    }
   ],
   "source": [
    "print_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e77713db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489\n",
      "0.9738701878370839\n",
      "10473\n",
      "281\n",
      "10754\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v9_v2.jsonl\", lines=True)\n",
    "print_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88fe1d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"filtered_relevant_constraints\"]= [eval(x) for x in df[\"filtered_relevant_constraints\"]]\n",
    "df.to_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v9_v2.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7c541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489\n",
      "0.9619711761971176\n",
      "10346\n",
      "409\n",
      "10755\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v8.jsonl\", lines=True)\n",
    "# print_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53893b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 153 entries, 30 to 1449\n",
      "Data columns (total 19 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               153 non-null    int64  \n",
      " 1   dataset                          153 non-null    object \n",
      " 2   instruction                      153 non-null    object \n",
      " 3   code                             153 non-null    object \n",
      " 4   test                             129 non-null    object \n",
      " 5   relevant_categories              153 non-null    object \n",
      " 6   simplified_instruction           153 non-null    object \n",
      " 7   extracted_constraints            153 non-null    object \n",
      " 8   final_comprehensive_constraints  153 non-null    object \n",
      " 9   filtered_relevant_constraints    153 non-null    object \n",
      " 10  quality_scores                   153 non-null    object \n",
      " 11  relevance_score                  153 non-null    float64\n",
      " 12  objectivity_score                153 non-null    float64\n",
      " 13  atomicity_score                  153 non-null    float64\n",
      " 14  unified_quality_score            153 non-null    float64\n",
      " 15  combined_instruction             153 non-null    object \n",
      " 16  constraint_wise_presence         153 non-null    object \n",
      " 17  constraint_presence_response     153 non-null    object \n",
      " 18  final_constraints                153 non-null    object \n",
      "dtypes: float64(4), int64(1), object(14)\n",
      "memory usage: 23.9+ KB\n",
      "153\n",
      "0.7255859375\n",
      "743\n",
      "281\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "filter_df_2 = df[df[\"constraint_wise_presence\"].apply(lambda x: sum(x)<len(x))]\n",
    "filter_df_2.info()\n",
    "stats = filter_df_2[\"constraint_wise_presence\"].to_list()\n",
    "print(len(stats))\n",
    "stat_final =[]  \n",
    "for stat in stats:\n",
    "    stat_final.extend(stat)\n",
    "print(sum(stat_final)/len(stat_final))\n",
    "print(sum(stat_final))\n",
    "print(len(stat_final)-sum(stat_final))\n",
    "print(len(stat_final))\n",
    "filter_df_2 = filter_df_2.drop(columns=[\"final_constraints\"])\n",
    "filter_df_2[\"filtered_relevant_constraints\"] = [eval(x) for x in filter_df_2[\"filtered_relevant_constraints\"]]\n",
    "filter_df_2.to_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/response_outputs/testing_responses/04_combine_instruct_eval_benchmark_v9.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78ef1a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 109 entries, 375 to 588\n",
      "Data columns (total 18 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               109 non-null    int64  \n",
      " 1   dataset                          109 non-null    object \n",
      " 2   instruction                      109 non-null    object \n",
      " 3   code                             109 non-null    object \n",
      " 4   test                             109 non-null    object \n",
      " 5   relevant_categories              109 non-null    object \n",
      " 6   simplified_instruction           109 non-null    object \n",
      " 7   extracted_constraints            109 non-null    object \n",
      " 8   final_comprehensive_constraints  109 non-null    object \n",
      " 9   filtered_relevant_constraints    109 non-null    object \n",
      " 10  quality_scores                   109 non-null    object \n",
      " 11  relevance_score                  109 non-null    float64\n",
      " 12  objectivity_score                109 non-null    float64\n",
      " 13  atomicity_score                  109 non-null    float64\n",
      " 14  unified_quality_score            109 non-null    float64\n",
      " 15  combined_instruction             109 non-null    object \n",
      " 16  constraint_wise_presence         109 non-null    object \n",
      " 17  constraint_presence_response     109 non-null    object \n",
      "dtypes: float64(4), int64(1), object(13)\n",
      "memory usage: 16.2+ KB\n",
      "109\n",
      "0.6562009419152276\n",
      "418\n",
      "219\n",
      "637\n"
     ]
    }
   ],
   "source": [
    "filter_df_3 = filter_df_2[filter_df_2[\"dataset\"]==\"xlangai/DS-1000\"]\n",
    "filter_df_3.info()\n",
    "print_stats(filter_df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "417b5ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df_3.to_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/response_outputs/testing_responses/ds_1000_combine_instruct_eval_benchmark_v9_filtered.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1431ea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 215 entries, 375 to 589\n",
      "Data columns (total 19 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               215 non-null    int64  \n",
      " 1   dataset                          215 non-null    object \n",
      " 2   instruction                      215 non-null    object \n",
      " 3   code                             215 non-null    object \n",
      " 4   test                             215 non-null    object \n",
      " 5   relevant_categories              215 non-null    object \n",
      " 6   simplified_instruction           215 non-null    object \n",
      " 7   extracted_constraints            215 non-null    object \n",
      " 8   final_comprehensive_constraints  215 non-null    object \n",
      " 9   filtered_relevant_constraints    215 non-null    object \n",
      " 10  quality_scores                   215 non-null    object \n",
      " 11  relevance_score                  215 non-null    float64\n",
      " 12  objectivity_score                215 non-null    float64\n",
      " 13  atomicity_score                  215 non-null    float64\n",
      " 14  unified_quality_score            215 non-null    float64\n",
      " 15  combined_instruction             215 non-null    object \n",
      " 16  constraint_wise_presence         215 non-null    object \n",
      " 17  constraint_presence_response     215 non-null    object \n",
      " 18  final_constraints                215 non-null    object \n",
      "dtypes: float64(4), int64(1), object(14)\n",
      "memory usage: 33.6+ KB\n",
      "215\n",
      "0.7627401837928154\n",
      "913\n",
      "284\n",
      "1197\n"
     ]
    }
   ],
   "source": [
    "filter_df_3 = df[df[\"dataset\"]==\"xlangai/DS-1000\"]\n",
    "filter_df_3.info()\n",
    "print_stats(filter_df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fcac37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n",
      "0.6562009419152276\n",
      "418\n",
      "219\n",
      "637\n"
     ]
    }
   ],
   "source": [
    "stats = filter_df_3[\"constraint_wise_presence\"].to_list()\n",
    "print(len(stats))\n",
    "stat_final =[]\n",
    "for stat in stats:\n",
    "    stat_final.extend(stat)\n",
    "print(sum(stat_final)/len(stat_final))\n",
    "print(sum(stat_final))\n",
    "print(len(stat_final)-sum(stat_final))\n",
    "print(len(stat_final))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9d258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af865973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b30829",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=[1,2]\n",
    "l2=[3,4]\n",
    "l3=l1+l2\n",
    "l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Read entire file and split into chunks (one per object)\n",
    "with open(\"benchmark_dataset/benchmark_v5.jsonl\", \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "blocks = content.strip().split('\\n\\n')\n",
    "data = []\n",
    "\n",
    "# Parse each block into JSON\n",
    "for idx,block in enumerate(blocks):\n",
    "    try:\n",
    "        data.append(json.loads(block))\n",
    "    except Exception as e:\n",
    "        print(\"index: \",idx)\n",
    "        print(\"Failed to parse block:\\n\", block)\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0204e28",
   "metadata": {},
   "source": [
    "# Filterig the constraints not in combined instruction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ac14a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 19 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(14)\n",
      "memory usage: 221.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_json(\"benchmark_dataset/benchmark_v9_v2.jsonl\", lines=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee916b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import traceback\n",
    "\n",
    "def analyze_false_constraints(\n",
    "    df,\n",
    "    constraint_col=\"filtered_relevant_constraints\",\n",
    "    presence_col=\"constraint_wise_presence\"\n",
    "):\n",
    "    # Counters\n",
    "    extracted_false = 0\n",
    "    newly_generated_false = 0\n",
    "    total_false = 0\n",
    "    track =0 \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Read raw values\n",
    "            constraints_raw = row[constraint_col]\n",
    "            presence_raw = row[presence_col]\n",
    "\n",
    "            # Clean markdown formatting if needed\n",
    "            if isinstance(constraints_raw, str):\n",
    "                constraints_raw = constraints_raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            if isinstance(presence_raw, str):\n",
    "                presence_raw = presence_raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "            # Parse\n",
    "            constraints = ast.literal_eval(constraints_raw) if isinstance(constraints_raw, str) else constraints_raw\n",
    "            \n",
    "            # presence = ast.literal_eval(presence_raw) if isinstance(presence_raw, str) else presence_raw\n",
    "            presence = [p for p in presence_raw if p]\n",
    "\n",
    "                \n",
    "            if  len(constraints) != len(presence):\n",
    "                \n",
    "                print(f\"Row with id {row['id']} has length mismatch: {len(constraints)} constraints but {len(presence)} presence values\")\n",
    "                # row[presence_col].append(True)\n",
    "                # df.to_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v9_v2.jsonl\", orient=\"records\", lines=True)\n",
    "                print(f\"Length mismatch at row {idx}\")\n",
    "                continue\n",
    "          \n",
    "\n",
    "            # Count false cases\n",
    "            for c, p in zip(constraints, presence):\n",
    "                if not p:\n",
    "                    total_false += 1\n",
    "                    if c[\"instruction_part\"] == \"Extracted from instruction\":\n",
    "                        extracted_false += 1\n",
    "                    elif c[\"instruction_part\"] == \"Newly Generated\":\n",
    "                        newly_generated_false += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"\\nError at row {idx}: {e}\")\n",
    "            print(\"Raw constraints:\", row[constraint_col])\n",
    "            print(\"Raw presence:   \", row[presence_col])\n",
    "            continue\n",
    "\n",
    "    # Compute percentages\n",
    "    if total_false > 0:\n",
    "        perc_extracted = (extracted_false / total_false) * 100\n",
    "        perc_generated = (newly_generated_false / total_false) * 100\n",
    "    else:\n",
    "        perc_extracted = perc_generated = 0\n",
    "\n",
    "    # Print result\n",
    "    print(f\"\\nTotal 'False' constraints: {total_false}\")\n",
    "    print(f\"Extracted from instruction: {extracted_false} ({perc_extracted:.2f}%)\")\n",
    "    print(f\"Newly Generated: {newly_generated_false} ({perc_generated:.2f}%)\")\n",
    "    print(\"track: \",track)\n",
    "\n",
    "    return {\n",
    "        \"total_false\": total_false,\n",
    "        \"extracted_false\": extracted_false,\n",
    "        \"newly_generated_false\": newly_generated_false,\n",
    "        \"perc_extracted\": perc_extracted,\n",
    "        \"perc_generated\": perc_generated\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4e6dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_aligns_values(text):\n",
    "    # Use regex to find all occurrences of \"Aligns\": true or \"Aligns\": false\n",
    "    matches = re.findall(r'\"Aligns\"\\s*:\\s*(true|false)', text, re.IGNORECASE)\n",
    "    # Convert string values to boolean\n",
    "    return [m.lower() == 'true' for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb667d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0ce2c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\" {\\n      \\\"Constraint\\\": \\\\\\\"The function must utilize the 'psutil' library to check for running processes.\\\\\\\",\\n      \\\\\\\"Reason\\\\\\\": \\\\\\\"The instruction requires using the 'psutil' library to check if the process is running.\\\\\\\",\\n      \\\\\\\"Aligns\\\\\\\": true\\n    }\"\"\"\n",
    "extract_aligns_values(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ce601f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 'False' constraints: 281\n",
      "Extracted from instruction: 22 (7.83%)\n",
      "Newly Generated: 257 (91.46%)\n",
      "track:  0\n"
     ]
    }
   ],
   "source": [
    "result = analyze_false_constraints(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "198e892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"final_constraints\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e421d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def filter_constraints(row):\n",
    "    try:\n",
    "        # Clean and parse\n",
    "        constraints_raw = row[\"filtered_relevant_constraints\"]\n",
    "        presence_raw = row[\"constraint_wise_presence\"]\n",
    "\n",
    "        if isinstance(constraints_raw, str):\n",
    "            constraints_raw = constraints_raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        if isinstance(presence_raw, str):\n",
    "            presence_raw = presence_raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "        constraints = ast.literal_eval(constraints_raw) if isinstance(constraints_raw, str) else constraints_raw\n",
    "        presence = presence_raw\n",
    "\n",
    "        # Filter where presence is True\n",
    "        filtered = [c for c, p in zip(constraints, presence) if p]\n",
    "        return filtered\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in row: {e}\")\n",
    "        return []\n",
    "\n",
    "# Apply to DataFrame\n",
    "df[\"final_constraints\"] = df.apply(filter_constraints, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ac5b6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 'False' constraints: 0\n",
      "Extracted from instruction: 0 (0.00%)\n",
      "Newly Generated: 0 (0.00%)\n",
      "track:  0\n"
     ]
    }
   ],
   "source": [
    "result = analyze_false_constraints(df_2,\"final_constraints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20d8898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v9_v2.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dbb72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = analyze_false_constraints(df,\"final_constraints\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"benchmark_dataset/benchmark_v7.jsonl\"\n",
    "df.to_json(output_path, orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows where final_constraints is an empty list\n",
    "empty_final_constraints_df = df[df[\"final_constraints\"].apply(lambda x: isinstance(x, list) and len(x) == 0)]\n",
    "\n",
    "# Display count and optionally inspect them\n",
    "print(f\"Total rows with empty final_constraints: {len(empty_final_constraints_df)}\")\n",
    "\n",
    "# Optional: Show some rows for inspection\n",
    "print(empty_final_constraints_df[[\"id\", \"instruction\", \"final_constraints\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1824a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"id\": 412,\n",
    "    \"dataset\": \"xlangai\\/DS-1000\",\n",
    "    \"instruction\": \"Problem:\\nI have pandas df with say, 100 rows, 10 columns, (actual data is huge). I also have row_index list which contains, which rows to be considered to take sum. I want to calculate sum on say columns 2,5,6,7 and 8. Can we do it with some function for dataframe object?\\nWhat I know is do a for loop, get value of row for each element in row_index and keep doing sum. Do we have some direct function where we can pass row_list, and column_list and axis, for ex df.sumAdvance(row_list,column_list,axis=0) ?\\nI have seen DataFrame.sum() but it didn't help I guess.\\n  a b c d q \\n0 1 2 3 0 5\\n1 1 2 3 4 5\\n2 1 1 1 6 1\\n3 1 0 0 0 0\\n\\n\\nI want sum of 0, 2, 3 rows for each a, b, d columns \\na    3.0\\nb    3.0\\nd    6.0\\n\\n\\nA:\\n<code>\\nimport pandas as pd\\n\\n\\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\\nrow_list = [0,2,3]\\ncolumn_list = ['a','b','d']\\n<\\/code>\\nresult = ... # put solution in this variable\\nBEGIN SOLUTION\\n<code>\",\n",
    "    \"code\": \"def g(df, row_list, column_list):\\n    return df[column_list].iloc[row_list].sum(axis=0)\\n\\nresult = g(df.copy(), row_list, column_list)\",\n",
    "    \"test\": \"import pandas as pd\\nimport numpy as np\\nimport copy\\nimport tokenize, io\\n\\n\\ndef generate_test_case(test_case_id):\\n    def generate_ans(data):\\n        data = data\\n        df, row_list, column_list = data\\n        return df[column_list].iloc[row_list].sum(axis=0)\\n\\n    def define_test_input(test_case_id):\\n        if test_case_id == 1:\\n            df = pd.DataFrame(\\n                {\\n                    \\\"a\\\": [1, 1, 1, 1],\\n                    \\\"b\\\": [2, 2, 1, 0],\\n                    \\\"c\\\": [3, 3, 1, 0],\\n                    \\\"d\\\": [0, 4, 6, 0],\\n                    \\\"q\\\": [5, 5, 1, 0],\\n                }\\n            )\\n            row_list = [0, 2, 3]\\n            column_list = [\\\"a\\\", \\\"b\\\", \\\"d\\\"]\\n        if test_case_id == 2:\\n            df = pd.DataFrame(\\n                {\\n                    \\\"a\\\": [1, 1, 1, 1],\\n                    \\\"b\\\": [2, 2, 1, 0],\\n                    \\\"c\\\": [3, 3, 1, 0],\\n                    \\\"d\\\": [0, 4, 6, 0],\\n                    \\\"q\\\": [5, 5, 1, 0],\\n                }\\n            )\\n            row_list = [0, 1, 3]\\n            column_list = [\\\"a\\\", \\\"c\\\", \\\"q\\\"]\\n        return df, row_list, column_list\\n\\n    test_input = define_test_input(test_case_id)\\n    expected_result = generate_ans(copy.deepcopy(test_input))\\n    return test_input, expected_result\\n\\n\\ndef exec_test(result, ans):\\n    try:\\n        pd.testing.assert_series_equal(result, ans, check_dtype=False)\\n        return 1\\n    except:\\n        return 0\\n\\n\\nexec_context = r\\\"\\\"\\\"\\nimport pandas as pd\\nimport numpy as np\\ndf, row_list, column_list = test_input\\n[insert]\\n\\\"\\\"\\\"\\n\\n\\ndef test_execution(solution: str):\\n    code = exec_context.replace(\\\"[insert]\\\", solution)\\n    for i in range(2):\\n        test_input, expected_result = generate_test_case(i + 1)\\n        test_env = {\\\"test_input\\\": test_input}\\n        exec(code, test_env)\\n        assert exec_test(test_env[\\\"result\\\"], expected_result)\\n\\n\\ndef test_string(solution: str):\\n    tokens = []\\n    for token in tokenize.tokenize(io.BytesIO(solution.encode(\\\"utf-8\\\")).readline):\\n        tokens.append(token.string)\\n    assert \\\"while\\\" not in tokens and \\\"for\\\" not in tokens\",\n",
    "    \"relevant_categories\": \"['Code Structure and Modularity', 'Input and Output Handling', 'Data Processing and Transformation', 'Library and API Usage', 'Performance and Optimization', 'Documentation and Readability', 'Mathematical Computation']\",\n",
    "    \"simplified_instruction\": \"Problem: I have pandas df with say, 100 rows, 10 columns, (actual data is huge). I also have row_index list which contains, which rows to be considered to take sum. I want to calculate sum on say columns 2,5,6,7 and 8. Can we do it with some function for dataframe object? What I know is do a for loop, get value of row for each element in row_index and keep doing sum. Do we have some direct function where we can pass row_list, and column_list and axis, for ex df.sumAdvance(row_list,column_list,axis=0) ? I have seen DataFrame.sum() but it didn't help I guess. I want sum of 0, 2, 3 rows for each a, b, d columns a    3.0 b    3.0 d    6.0 A:\",\n",
    "    \"extracted_constraints\": \"[{'type': 'Library and API Usage', 'constraint': 'Use a function for dataframe object to calculate the sum.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Library and API Usage', 'constraint': 'The function should allow passing row_list and column_list.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Library and API Usage', 'constraint': 'The function should specify an axis parameter.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Data Processing and Transformation', 'constraint': 'Calculate the sum for specific columns (2, 5, 6, 7, and 8).', 'instruction_part': 'Extracted from instruction'}, {'type': 'Data Processing and Transformation', 'constraint': 'Calculate the sum for rows specified in row_index.', 'instruction_part': 'Extracted from instruction'}]\",\n",
    "    \"final_comprehensive_constraints\": \"[{'type': 'Library and API Usage', 'constraint': 'Use a function for dataframe object to calculate the sum.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Library and API Usage', 'constraint': 'The function should allow passing row_list and column_list.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Library and API Usage', 'constraint': 'The function should specify an axis parameter.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Data Processing and Transformation', 'constraint': 'Calculate the sum for specific columns (2, 5, 6, 7, and 8).', 'instruction_part': 'Extracted from instruction'}, {'type': 'Data Processing and Transformation', 'constraint': 'Calculate the sum for rows specified in row_index.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Performance and Optimization', 'constraint': 'Ensure the function is optimized for large DataFrames with hundreds of thousands of rows.', 'instruction_part': 'Newly Generated'}, {'type': 'Documentation and Readability', 'constraint': 'Include docstrings in the function to explain parameters and return values.', 'instruction_part': 'Newly Generated'}, {'type': 'Input and Output Handling', 'constraint': 'Validate that row_list and column_list contain valid indices and column names before performing the sum.', 'instruction_part': 'Newly Generated'}, {'type': 'Mathematical Computation', 'constraint': 'Ensure the sum calculation handles missing values appropriately, either by ignoring them or filling them with zeros.', 'instruction_part': 'Newly Generated'}, {'type': 'Code Structure and Modularity', 'constraint': 'Structure the function to allow for easy extension, such as adding more operations beyond summation in the future.', 'instruction_part': 'Newly Generated'}]\",\n",
    "    \"filtered_relevant_constraints\": \"[{'type': 'Library and API Usage', 'constraint': 'Use a function for dataframe object to calculate the sum.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Library and API Usage', 'constraint': 'The function should allow passing row_list and column_list.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Library and API Usage', 'constraint': 'The function should specify an axis parameter.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Data Processing and Transformation', 'constraint': 'Calculate the sum for rows specified in row_index.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Input and Output Handling', 'constraint': 'Validate that row_list and column_list contain valid indices and column names before performing the sum.', 'instruction_part': 'Newly Generated'}, {'type': 'Mathematical Computation', 'constraint': 'Ensure the sum calculation handles missing values appropriately, either by ignoring them or filling them with zeros.', 'instruction_part': 'Newly Generated'}]\",\n",
    "    \"quality_scores\": \"{'constraint_evaluations': [{'constraint_text': 'Use a function for dataframe object to calculate the sum.', 'atomicity_score': 5, 'relevance_score': 5, 'objectivity_score': 5, 'reasoning': 'This constraint is atomic as it specifies a single requirement: to use a function for the DataFrame object. It is highly relevant to the task of calculating a sum using a DataFrame and is objective since it can be clearly evaluated by checking if a function is used.'}, {'constraint_text': 'The function should allow passing row_list and column_list.', 'atomicity_score': 5, 'relevance_score': 5, 'objectivity_score': 5, 'reasoning': \\\"This constraint is atomic as it focuses on a single requirement regarding the function's parameters. It is relevant because it directly addresses the need to specify which rows and columns to sum. It is also objective, as it can be verified by checking the function's signature.\\\"}, {'constraint_text': 'The function should specify an axis parameter.', 'atomicity_score': 5, 'relevance_score': 5, 'objectivity_score': 5, 'reasoning': 'This constraint is atomic, focusing solely on the requirement for an axis parameter. It is relevant to the task since the axis is crucial for determining how the sum is calculated. It is objective, as the presence of an axis parameter can be easily checked.'}, {'constraint_text': 'Calculate the sum for rows specified in row_index.', 'atomicity_score': 5, 'relevance_score': 5, 'objectivity_score': 5, 'reasoning': 'This constraint is atomic as it specifies a single requirement regarding the rows to be summed. It is relevant because it directly relates to the task of summing specific rows. It is objective, as it can be verified by checking the implementation of the sum calculation.'}, {'constraint_text': 'Validate that row_list and column_list contain valid indices and column names before performing the sum.', 'atomicity_score': 5, 'relevance_score': 4, 'objectivity_score': 5, 'reasoning': 'This constraint is atomic as it focuses on a single validation requirement. It is relevant, but slightly less so than others, as it pertains to input validation rather than the core summation task. It is objective, as the validation can be clearly defined and checked.'}, {'constraint_text': 'Ensure the sum calculation handles missing values appropriately, either by ignoring them or filling them with zeros.', 'atomicity_score': 5, 'relevance_score': 4, 'objectivity_score': 5, 'reasoning': 'This constraint is atomic as it specifies a single requirement regarding handling missing values. It is relevant, but slightly less so than others, as it addresses a specific case rather than the main functionality. It is objective, as the handling of missing values can be clearly defined and tested.'}], 'avg_atomicity': 5.0, 'avg_relevance': 4.67, 'avg_objectivity': 5.0, 'unified_quality_score': 4.89, 'overall_analysis': 'The overall quality of the constraint set is high, with all constraints scoring well in atomicity and objectivity. Relevance is also strong, though slightly lower for the last two constraints, which focus on validation and handling of missing values rather than the core summation functionality. The constraints are clear, measurable, and directly aligned with the task, making them effective for guiding the implementation.'}\",\n",
    "    \"relevance_score\": 4.67,\n",
    "    \"objectivity_score\": 5.0,\n",
    "    \"atomicity_score\": 5.0,\n",
    "    \"unified_quality_score\": 4.89,\n",
    "    \"combined_instruction\": \"Problem:\\nI have pandas df with say, 100 rows, 10 columns, (actual data is huge). I also have row_index list which contains, which rows to be considered to take sum. I want to calculate sum on say columns 2,5,6,7 and 8. Can we do it with some function for dataframe object to calculate the sum? The function should allow passing row_list and column_list, and specify an axis parameter. What I know is do a for loop, get value of row for each element in row_index and keep doing sum. Do we have some direct function where we can pass row_list, and column_list and axis, for ex df.sumAdvance(row_list,column_list,axis=0) ? I have seen DataFrame.sum() but it didn't help I guess. \\n\\nI want sum of 0, 2, 3 rows for each a, b, d columns, ensuring that the sum calculation handles missing values appropriately, either by ignoring them or filling them with zeros. Additionally, validate that row_list and column_list contain valid indices and column names before performing the sum.\\n\\na    3.0\\nb    3.0\\nd    6.0\\n\\nA:\\n<code>\\nimport pandas as pd\\n\\n\\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\\nrow_list = [0,2,3]\\ncolumn_list = ['a','b','d']\\n<\\/code>\\nresult = ... # put solution in this variable\\nBEGIN SOLUTION\\n<code>\\n    \\n<\\/code>\",\n",
    "    \"constraint_wise_presence\": [\n",
    "        false,\n",
    "        false,\n",
    "        false,\n",
    "        false,\n",
    "        false,\n",
    "        false\n",
    "    ],\n",
    "    \"constraint_presence_response\": \"```json\\n{\\n  \\\"Evaluation\\\": [\\n    {\\n      \\\"Constraint\\\": \\\"Use a function for dataframe object to calculate the sum.\\\",\\n      \\\"Reason\\\": \\\"The instruction mentions the desire for a direct function to calculate the sum, such as df.sumAdvance, but does not provide an actual function implementation.\\\",\\n      \\\"Aligns\\\": false\\n    },\\n    {\\n      \\\"Constraint\\\": \\\"The function should allow passing row_list and column_list.\\\",\\n      \\\"Reason\\\": \\\"The instruction specifies that the function should allow passing row_list and column_list, but does not provide an implementation of such a function.\\\",\\n      \\\"Aligns\\\": false\\n    },\\n    {\\n      \\\"Constraint\\\": \\\"The function should specify an axis parameter.\\\",\\n      \\\"Reason\\\": \\\"The instruction mentions the need for an axis parameter in the function, but does not provide an implementation of such a function.\\\",\\n      \\\"Aligns\\\": false\\n    },\\n    {\\n      \\\"Constraint\\\": \\\"Calculate the sum for rows specified in row_index.\\\",\\n      \\\"Reason\\\": \\\"The instruction specifies the need to calculate the sum for specific rows using row_index, but does not provide an implementation of this calculation.\\\",\\n      \\\"Aligns\\\": false\\n    },\\n    {\\n      \\\"Constraint\\\": \\\"Validate that row_list and column_list contain valid indices and column names before performing the sum.\\\",\\n      \\\"Reason\\\": \\\"The instruction mentions the need to validate row_list and column_list, but does not provide an implementation of this validation.\\\",\\n      \\\"Aligns\\\": false\\n    },\\n    {\\n      \\\"Constraint\\\": \\\"Ensure the sum calculation handles missing values appropriately, either by ignoring them or filling them with zeros.\\\",\\n      \\\"Reason\\\": \\\"The instruction specifies handling missing values appropriately, but does not provide an implementation of this handling.\\\",\\n      \\\"Aligns\\\": false\\n    }\\n  ]\\n}\\n```\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"final_constraints\"].apply(lambda x: not (isinstance(x, list) and len(x) == 0))]\n",
    "output_path = \"benchmark_dataset/benchmark_v7.jsonl\"\n",
    "df.to_json(output_path, orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20804bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"benchmark_dataset/benchmark_v7.jsonl\",lines=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3cc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = df[\"constraint_wise_presence\"].to_list()\n",
    "print(len(stats))\n",
    "stat_final =[]\n",
    "for stat in stats:\n",
    "    stat_final.extend(stat)\n",
    "print(sum(stat_final)/len(stat_final))\n",
    "print(len(stat_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = df[\"final_constraints\"].to_list()\n",
    "print(constraints[0])\n",
    "print(len(constraints))\n",
    "total = []\n",
    "for i in constraints:\n",
    "    total.extend(i)\n",
    "print(len(total))\n",
    "print(total[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    num_true = sum(row[\"constraint_wise_presence\"])\n",
    "    num_constraints = len(row[\"final_constraints\"])\n",
    "    if num_true != num_constraints:\n",
    "        print(f\"Mismatch at row {idx}: {num_true} True vs {num_constraints} constraints\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten presence lists and count True values\n",
    "stats = df[\"constraint_wise_presence\"].to_list()\n",
    "stat_final = []\n",
    "for stat in stats:\n",
    "    stat_final.extend(stat)\n",
    "total_true = sum(stat_final)\n",
    "\n",
    "# Flatten final_constraints and count entries\n",
    "constraints = df[\"final_constraints\"].to_list()\n",
    "total_constraints = []\n",
    "for c_list in constraints:\n",
    "    total_constraints.extend(c_list)\n",
    "total_len = len(total_constraints)\n",
    "\n",
    "# Print results\n",
    "print(f\"Total number of `True` values in constraint_wise_presence: {total_true}\")\n",
    "print(f\"Total number of constraints in final_constraints: {total_len}\")\n",
    "print(f\"Do they match? {'✅ Yes' if total_true == total_len else '❌ No'}\")\n",
    "\n",
    "# Optional: check mismatched rows\n",
    "mismatches = []\n",
    "for idx, row in df.iterrows():\n",
    "    num_true = sum(row[\"constraint_wise_presence\"])\n",
    "    num_constraints = len(row[\"final_constraints\"])\n",
    "    if num_true != num_constraints:\n",
    "        mismatches.append((idx, num_true, num_constraints))\n",
    "\n",
    "print(f\"Total mismatched rows: {len(mismatches)}\")\n",
    "if mismatches:\n",
    "    print(\"Sample mismatches (index, true_count, constraint_count):\")\n",
    "    print(mismatches[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_alignment_scores_regex(response):\n",
    "    results = []\n",
    "    pattern = re.compile(r'\"Aligns\"\\s*:\\s*(true|false)', re.IGNORECASE)\n",
    "\n",
    "\n",
    "    try:\n",
    "        matches = pattern.findall(response)\n",
    "        aligns_row = [1 if m.lower() == \"true\" else 0 for m in matches]\n",
    "        results.append(aligns_row)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error parsing response: {e}\")\n",
    "        print(response)\n",
    "      \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d55814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_alignment_scores_from_evaluation(response_list):\n",
    "    results = []\n",
    "\n",
    "    for response in response_list:\n",
    "        try:\n",
    "            # Extract the Evaluation block only\n",
    "            eval_block_match = re.search(r'\"Evaluation\"\\s*:\\s*\\[(.*?)\\](?=,|\\s*[\"}])', response, re.DOTALL)\n",
    "            if not eval_block_match:\n",
    "        \n",
    "                results.append([])\n",
    "                continue\n",
    "\n",
    "            eval_block = eval_block_match.group(1)\n",
    "\n",
    "            # Now extract Aligns values inside this block\n",
    "            aligns = re.findall(r'\"Aligns\"\\s*:\\s*(true|false)', eval_block, flags=re.IGNORECASE)\n",
    "            aligns_row = [1 if val.lower() == \"true\" else 0 for val in aligns]\n",
    "            results.append(aligns_row)\n",
    "\n",
    "        except Exception as e:\n",
    "      \n",
    "            results.append([])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eaa0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "response =[ \"\"\"{'Evaluation': [{'Constraint': 'The `PluginManager` should be able to register multiple versions of plugins.', 'Reason': 'The `PluginManager` class has a `register_plugin` method that adds plugins to a dictionary keyed by their version, allowing multiple plugin versions to be registered.', 'Aligns': True}, {'Constraint': 'Each plugin version should have a unique identifier.', 'Reason': \"Each plugin instance has a `version` attribute used as a unique key in the `PluginManager`'s `plugins` dictionary, ensuring uniqueness.\", 'Aligns': True}, {'Constraint': 'The `PluginManager` should provide a method to get a plugin by its version identifier.', 'Reason': 'The `get_plugin` method in `PluginManager` retrieves a plugin by its version identifier.', 'Aligns': True}, {'Constraint': 'The `PluginManager` should provide a method to list all registered plugins along with their capabilities.', 'Reason': 'The `list_plugins` method returns a dictionary mapping plugin versions to their capabilities.', 'Aligns': True}, {'Constraint': 'Implement error handling for cases where a plugin version is not found.', 'Reason': 'The `get_plugin` method raises a `ValueError` with a descriptive message if the requested plugin version is not found.', 'Aligns': True}, {'Constraint': 'Use the provided `Plugin_v1` class as a base class for creating a new plugin version `Plugin_v2`.', 'Reason': '`Plugin_v2` inherits from `Plugin_v1` as shown in the class definition.', 'Aligns': True}, {'Constraint': 'The `Plugin_v2` should override the `capabilities` method to return its new capabilities.', 'Reason': '`Plugin_v2` overrides the `capabilities` method to return its capabilities set, which includes the added import and export capabilities.', 'Aligns': True}, {'Constraint': 'The `PluginManager` should raise a descriptive exception if an attempt is made to register a plugin with an existing version identifier.', 'Reason': 'The `register_plugin` method raises a `ValueError` with a descriptive message if a plugin with the same version is already registered.', 'Aligns': True}], 'FinalDecision': {'Score': '8/8', 'Reason': 'Yes, all constraints were satisfied as the response implements all required features and error handling as specified.', 'Aligns': True}}\n",
    "\"\"\"]\n",
    "extract_alignment_scores_from_evaluation(response)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "response =[ \"\"\"{'Evaluation': [{'Constraint': 'The `PluginManager` should be able to register multiple versions of plugins.', 'Reason': 'The `PluginManager` class has a `register_plugin` method that adds plugins to a dictionary keyed by their version, allowing multiple plugin versions to be registered.', 'Aligns': True}, {'Constraint': 'Each plugin version should have a unique identifier.', 'Reason': \"Each plugin instance has a `version` attribute used as a unique key in the `PluginManager`'s `plugins` dictionary, ensuring uniqueness.\", 'Aligns': True}, {'Constraint': 'The `PluginManager` should provide a method to get a plugin by its version identifier.', 'Reason': 'The `get_plugin` method in `PluginManager` retrieves a plugin by its version identifier.', 'Aligns': True}, {'Constraint': 'The `PluginManager` should provide a method to list all registered plugins along with their capabilities.', 'Reason': 'The `list_plugins` method returns a dictionary mapping plugin versions to their capabilities.', 'Aligns': True}, {'Constraint': 'Implement error handling for cases where a plugin version is not found.', 'Reason': 'The `get_plugin` method raises a `ValueError` with a descriptive message if the requested plugin version is not found.', 'Aligns': True}, {'Constraint': 'Use the provided `Plugin_v1` class as a base class for creating a new plugin version `Plugin_v2`.', 'Reason': '`Plugin_v2` inherits from `Plugin_v1` as shown in the class definition.', 'Aligns': True}, {'Constraint': 'The `Plugin_v2` should override the `capabilities` method to return its new capabilities.', 'Reason': '`Plugin_v2` overrides the `capabilities` method to return its capabilities set, which includes the added import and export capabilities.', 'Aligns': True}, {'Constraint': 'The `PluginManager` should raise a descriptive exception if an attempt is made to register a plugin with an existing version identifier.', 'Reason': 'The `register_plugin` method raises a `ValueError` with a descriptive message if a plugin with the same version is already registered.', 'Aligns': True}], 'FinalDecision': {'Score': '8/8', 'Reason': 'Yes, all constraints were satisfied as the response implements all required features and error handling as specified.', 'Aligns': True}}\n",
    "\"\"\"]\n",
    "print(extract_alignment_scores_from_evaluation(response)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/metrics_outputs/granite-3.1-2b-instruct_results_correctness_metrics.jsonl\",lines=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300af986",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d3ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "sampled_df = df.groupby('dataset', group_keys=False).apply(lambda x: x.sample(n=min(n, len(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe67d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768be5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.to_json(\"response_outputs/testing_responses/granite-3.1-2b-instruct_results_correctness_metrics.jsonl\",orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be202c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "def print_row_columns(data, row_num):\n",
    "    if row_num < 0 or row_num >= len(data):\n",
    "        print(f\"Invalid row number. Please choose between 0 and {len(data) - 1}.\")\n",
    "        return\n",
    "    row = data[row_num]\n",
    "    print(f\"Row {row_num}:\")\n",
    "    for key, value in row.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/dccstor/shanmukh/sravani_internship/benchmark_experiments/response_outputs/testing_responses/granite-3.1-2b-instruct_results_correctness_metrics.jsonl\"  # Replace with your actual .jsonl file path\n",
    "    row_number = int(input(\"Enter the row number: \"))\n",
    "\n",
    "    data = read_jsonl_file(file_path)\n",
    "    print_row_columns(data, row_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    " print_row_columns(data, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca48de7",
   "metadata": {},
   "source": [
    "# New Prompt for LLM as Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from utils.compute_clients import LLMClient\n",
    "from utils.compute_clients import create_clients\n",
    "\n",
    "\n",
    "def evaluate_response_strict_alignment(instruction: str, constraints: List[str], response: str, client: LLMClient) -> dict:\n",
    "    \"\"\"Evaluate whether a response strictly satisfies each constraint in the list using an LLM verifier.\"\"\"\n",
    "    \n",
    "    # Build the strict verifier system prompt\n",
    "    system_prompt = \"\"\"You are a verifier. Your task is to evaluate whether a given response satisfies a set of constraints for a specific instruction.\n",
    "\n",
    "You will be provided:\n",
    "- An instruction\n",
    "- A list of constraints\n",
    "- A response to the instruction\n",
    "\n",
    "Your task:\n",
    "- Analyze the response against each constraint independently.\n",
    "- For each constraint, determine whether it is strictly and completely satisfied.\n",
    "- If any part of a constraint is only partially fulfilled or ambiguous, mark it as not satisfied.\n",
    "- Provide a brief but clear explanation of your judgment for each constraint.\n",
    "- Do not make assumptions beyond the constraint. Only base your judgment on what is explicitly implemented or stated in the response.\n",
    "\n",
    "Output your evaluation in a valid JSON format like below:\n",
    "{\n",
    "  \"Evaluation\": [\n",
    "    {\n",
    "      \"Constraint\": \"<constraint text>\",\n",
    "      \"Reason\": \"explanation\",\n",
    "      \"Aligns\": [true|false]\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "Do not include any text outside this JSON object.\"\"\"\n",
    "\n",
    "    # Create the user prompt\n",
    "    user_prompt = f\"\"\"[Instruction]:\n",
    "{instruction}\n",
    "\n",
    "[Constraints]:\n",
    "{json.dumps(constraints, indent=2)}\n",
    "\n",
    "[Response]:\n",
    "```python\n",
    "{response}\n",
    "```\"\"\"\n",
    "\n",
    "    # Send to the model and return the parsed JSON output\n",
    "    model_output = client.get_model_response(\n",
    "        user_prompt=user_prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        temperature=0,\n",
    "        max_new_tokens=2048\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        return json.loads(model_output)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Model output was not valid JSON\", \"raw_output\": model_output}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d75259",
   "metadata": {},
   "source": [
    "# New Prompt for code correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "75acb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from utils.compute_clients import LLMClient\n",
    "from utils.compute_clients import create_clients\n",
    "\n",
    "\n",
    "def evaluate_response_code_correctness(instruction: str, constraints: List[str], response: str, client: LLMClient) -> dict:\n",
    "    prompt = f\"\"\"You are an expert Python developer and uncompromising code reviewer.  \n",
    "Your task is to evaluate whether the provided Python code strictly and robustly follows the instruction, based only on three dimensions:\n",
    "\n",
    "1. Syntax Correctness:  \n",
    "   - The code must parse and compile without any syntax errors.\n",
    "\n",
    "2. Semantic Correctness:  \n",
    "   - The code must run and produce the intended effect exactly as described.  \n",
    "   - Any hidden bugs or logical mistakes count as failures.\n",
    "\n",
    "3. Constraint Correctness:  \n",
    "   - The code must satisfy *every* constraint explicitly stated in the instruction, including any edge-case, privacy, or consistency requirements.  \n",
    "   - If the instruction implies or adds new requirements (e.g. key ordering, redaction of sensitive data), those must be met exactly.\n",
    "\n",
    "Important Review Principles:\n",
    "- **Be conservative.** Only label “Completely Correct” if you are *absolutely certain* that all dimensions are fully satisfied—no exceptions.  \n",
    "- **Downgrade on any violation.**  \n",
    "  - If syntax or runtime errors exist, mark as **Wrong**.  \n",
    "  - If the code runs but omits or mis-implements any single required constraint, mark as **Partially Correct** at best.  \n",
    "- **No leniency for extra features.** Any extra functionality that interferes with, overrides, or exposes data beyond the instruction is a constraint violation.\n",
    "\n",
    "Evaluation Labels:\n",
    "- **Completely Correct**: Perfect in syntax, semantics, and constraint adherence—no deviations.  \n",
    "- **Partially Correct**: Runs without errors but misses or mis-implements one or more explicit requirements.  \n",
    "- **Wrong**: Contains syntax/runtime errors or fails to implement the core instruction at all.\n",
    "\n",
    "**If you have any doubt about full compliance, choose the lower category.**\n",
    "\n",
    "Return *only* this dictionary—no further explanation:\n",
    "{{  \n",
    "  \"reason\": \"<Concise justification for your label>\",  \n",
    "  \"correctness\": \"Completely Correct\" | \"Partially Correct\" | \"Wrong\"  \n",
    "}}\n",
    "Input:\n",
    "Instruction:  \n",
    "{instruction}\n",
    "\n",
    "Generated Code:  \n",
    "```python\n",
    "{response}\n",
    "```\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Send to the model and return the parsed JSON output\n",
    "    model_output = client.get_model_response(\n",
    "        user_prompt=prompt,\n",
    "        system_prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_new_tokens=2048\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        return json.loads(model_output)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Model output was not valid JSON\", \"raw_output\": model_output}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fe62698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': \"The code is syntactically correct and runs without errors. It correctly returns 'Not Prime' for non-integers (including decimals), negative numbers, and numbers <= 1. It returns 'Prime' or 'Not Prime' exactly as required. It uses O(sqrt(n)) time complexity, checks only odd divisors after 2, and uses constant space. All constraints, including handling of decimal inputs and skipping even numbers > 2, are met exactly.\", 'correctness': 'Completely Correct'}\n"
     ]
    }
   ],
   "source": [
    "instruction =before_file[\"combined_instruction\"][3]\n",
    "constraints = before_file[\"final_constraints\"][3]\n",
    "response = before_file[\"response\"][3]\n",
    "\n",
    "client = create_clients(mode=\"GPT-azure\")\n",
    "result = evaluate_response_code_correctness(instruction, constraints, response, client)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4408bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_aligns_values(text):\n",
    "\n",
    "    # Use regex to find all occurrences of \"Aligns\": true or \"Aligns\": false\n",
    "    matches = re.findall(r'\"[Aligns]\"\\s*:\\s*(true|false)', text, re.IGNORECASE)\n",
    "    # Convert string values to boolean\n",
    "    return [m.lower() == 'true' for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce2de429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_correctness_values(text):\n",
    "\n",
    "    # Use regex to find all occurrences of \"correctness\": \"Completely Correct\", \"Partially Correct\", or \"Wrong\"\n",
    "    matches = re.findall(r'\"correctness\"\\s*:\\s*(\"(?:Completely Correct|Partially Correct|Wrong)\")', text, re.IGNORECASE)\n",
    "    # Extract the matched strings and return them\n",
    "    return matches[0].strip('\"') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2dbf4ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completely Correct\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"```json                                                                                              \n",
    "{                                                                                                                                                                                          \n",
    "  \"reason\": \"The code contains multiple critical issues: The 'routes.py' file attempts to access 'socketio.get_store().get_totals()', but 'socketio' does not have a 'get_store()' method, \n",
    "causing runtime errors. The 'handle_connect' event handler is defined in 'routes.py' but never imported or registered, so clients will not receive the initial vote totals on connection, v\n",
    "iolating the requirement that the HTML page displays current vote totals and updates in real-time. In 'index.html', the JavaScript incorrectly updates all three <p> elements with the same\n",
    " combined text, causing display errors. The vote form's submit button lacks type=\"submit\", and the form submission is not properly handled to prevent page reload. The tests in 'tests/test\n",
    "_routes.py' are nonsensical and do not test the actual application logic. These issues mean the code will not run correctly or produce the intended effect exactly as described, failing se\n",
    "mantic and constraint correctness. Syntax is mostly correct, but runtime errors and missing features downgrade correctness to Partially Correct.\",                                         \n",
    "  \"correctness\": \"Completely Correct\"                                                                                                                                                       \n",
    "}                                                                                                                                                                                          \n",
    "```  \"\"\"\n",
    "print(extract_correctness_values(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "77dbe55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 22 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      " 19  response                         1489 non-null   object \n",
      " 20  correctness_level                1489 non-null   object \n",
      " 21  code_correctness_response        1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(17)\n",
      "memory usage: 256.0+ KB\n"
     ]
    }
   ],
   "source": [
    "after_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d1c5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_df[\"correctness_level\"] = after_df[\"code_correctness_response\"].apply(extract_correctness_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce56bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 22 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      " 19  response                         1489 non-null   object \n",
      " 20  correctness_level                1489 non-null   object \n",
      " 21  code_correctness_response        1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(17)\n",
      "memory usage: 256.0+ KB\n"
     ]
    }
   ],
   "source": [
    "after_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d95cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8843176",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = sampled_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d9c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sampled_df[\"response\"][0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction =df[\"combined_instruction\"][3]\n",
    "constraints = df[\"final_constraints\"][3]\n",
    "response = df[\"response\"][3]\n",
    "\n",
    "client = create_clients(mode=\"GPT-azure\")\n",
    "result = evaluate_response_strict_alignment(instruction, constraints, response, client)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050c044",
   "metadata": {},
   "source": [
    "## evaluate_row_constraints_strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c779455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Tuple\n",
    "from utils.compute_clients import create_clients, LLMClient\n",
    "\n",
    "def evaluate_row_constraints_strict(n: int, df) -> Tuple[dict, list, str]:\n",
    "    \"\"\"Evaluate constraint adherence for row `n` in the dataframe using strict alignment check.\"\"\"\n",
    "\n",
    "    # Extract instruction, constraints, and response\n",
    "    print(\"ID:\",n)\n",
    "    print(\"dataset:\",df[\"dataset\"][n])\n",
    "    og_instruction = df[\"instruction\"][n]\n",
    "    print(\"Og_instruction:\\n\",og_instruction)\n",
    "    print(\"------------------------------------------------------\\n\")\n",
    "\n",
    "    instruction = df[\"combined_instruction\"][n]\n",
    "    print(\"combined_instruction:\\n\", instruction)\n",
    "    print(\"------------------------------------------------------\\n\")\n",
    "    constraints = df[\"final_constraints\"][n]\n",
    "    for con in constraints:\n",
    "        print(\"Constraint:\\n\", con)\n",
    "    print(\"------------------------------------------------------\\n\")\n",
    "\n",
    "    response = df[\"response\"][n]\n",
    "    print(\"Response: \\n\",response)\n",
    "\n",
    "    # print(\"Previous List :\\n\", df[\"Constraint_adherence\"][n])\n",
    "\n",
    "    # # Create client\n",
    "    # client = create_clients(mode=\"GPT-azure\")\n",
    "\n",
    "    # # Call the strict evaluation function\n",
    "    # result = evaluate_response_strict_alignment(instruction, constraints, response, client)\n",
    "\n",
    "    # # Extract true/false list\n",
    "    # if \"Evaluation\" in result:\n",
    "    #     alignments = [entry.get(\"Aligns\", \"nan\") for entry in result[\"Evaluation\"]]\n",
    "    # else:\n",
    "    #     alignments = []\n",
    "\n",
    "    # print(\"Alignment Booleans:\", alignments)\n",
    "    # # print(\"Response Code:\\n\", code_response)\n",
    "    # print(\"Full Evaluation JSON:\\n\", )\n",
    "    # for res in result.get(\"Evaluation\"):\n",
    "    #     print(\"reasoning:\\n\",res)\n",
    "\n",
    "    print(\"Correctness: \\n\",df[\"correctness_level\"][n])\n",
    "\n",
    "    print(\"Correctness Reason: \\n\",df[\"code_correctness_response\"][n])\n",
    "    # return result, alignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2140c066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 61 entries, 377 to 589\n",
      "Data columns (total 22 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               61 non-null     int64  \n",
      " 1   dataset                          61 non-null     object \n",
      " 2   instruction                      61 non-null     object \n",
      " 3   code                             61 non-null     object \n",
      " 4   test                             61 non-null     object \n",
      " 5   relevant_categories              61 non-null     object \n",
      " 6   simplified_instruction           61 non-null     object \n",
      " 7   extracted_constraints            61 non-null     object \n",
      " 8   final_comprehensive_constraints  61 non-null     object \n",
      " 9   filtered_relevant_constraints    61 non-null     object \n",
      " 10  quality_scores                   61 non-null     object \n",
      " 11  relevance_score                  61 non-null     float64\n",
      " 12  objectivity_score                61 non-null     float64\n",
      " 13  atomicity_score                  61 non-null     float64\n",
      " 14  unified_quality_score            61 non-null     float64\n",
      " 15  combined_instruction             61 non-null     object \n",
      " 16  constraint_wise_presence         61 non-null     object \n",
      " 17  constraint_presence_response     61 non-null     object \n",
      " 18  final_constraints                61 non-null     object \n",
      " 19  response                         61 non-null     object \n",
      " 20  correctness_level                61 non-null     object \n",
      " 21  code_correctness_response        61 non-null     object \n",
      "dtypes: float64(4), int64(1), object(17)\n",
      "memory usage: 11.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 123 entries, 590 to 963\n",
      "Data columns (total 22 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               123 non-null    int64  \n",
      " 1   dataset                          123 non-null    object \n",
      " 2   instruction                      123 non-null    object \n",
      " 3   code                             123 non-null    object \n",
      " 4   test                             123 non-null    object \n",
      " 5   relevant_categories              123 non-null    object \n",
      " 6   simplified_instruction           123 non-null    object \n",
      " 7   extracted_constraints            123 non-null    object \n",
      " 8   final_comprehensive_constraints  123 non-null    object \n",
      " 9   filtered_relevant_constraints    123 non-null    object \n",
      " 10  quality_scores                   123 non-null    object \n",
      " 11  relevance_score                  123 non-null    float64\n",
      " 12  objectivity_score                123 non-null    float64\n",
      " 13  atomicity_score                  123 non-null    float64\n",
      " 14  unified_quality_score            123 non-null    float64\n",
      " 15  combined_instruction             123 non-null    object \n",
      " 16  constraint_wise_presence         123 non-null    object \n",
      " 17  constraint_presence_response     123 non-null    object \n",
      " 18  final_constraints                123 non-null    object \n",
      " 19  response                         123 non-null    object \n",
      " 20  correctness_level                123 non-null    object \n",
      " 21  code_correctness_response        123 non-null    object \n",
      "dtypes: float64(4), int64(1), object(17)\n",
      "memory usage: 22.1+ KB\n"
     ]
    }
   ],
   "source": [
    "after_df_wrong = after_df[after_df[\"correctness_level\"] == \"Wrong\"]\n",
    "after_df_wrong_ds = after_df_wrong[after_df_wrong[\"dataset\"] == \"xlangai/DS-1000\"]\n",
    "after_df_wrong_ds.info()\n",
    "big_code_df = after_df_wrong[after_df_wrong[\"dataset\"] == \"bigcode/bigcodebench\"]\n",
    "big_code_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "39f7bf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 2\n",
      "dataset: bigcode/bigcodebench\n",
      "Og_instruction:\n",
      " Find the latest log file in a specified directory that matches a given regex pattern. This function searches through all files in the specified directory, filters them based on the provided regex pattern, and returns the path to the most recent log file based on modification time. If no files match the pattern or the directory is empty, the function returns None.\n",
      "The function should output with:\n",
      "    str or None: The path to the most recent log file that matches the pattern, or None if no matching files are found.\n",
      "You should write self-contained code starting with:\n",
      "```\n",
      "import os\n",
      "import re\n",
      "def task_func(pattern, log_dir='/var/log/'):\n",
      "```\n",
      "------------------------------------------------------\n",
      "\n",
      "combined_instruction:\n",
      " Find the latest log file in a specified directory that matches a given regex pattern. This function searches through all files in the specified directory, filters them based on the provided regex pattern, and returns the path to the most recent log file based on modification time. The function should ensure that it only processes files with a '.log' extension to avoid unnecessary errors. If the directory is empty, the function should return None immediately, without attempting to process any files. The function must handle cases where the specified directory does not exist by raising a FileNotFoundError. If the regex pattern is invalid, the function should raise a re.error exception. The function should efficiently handle directories with a large number of files, ensuring that the sorting operation does not exceed O(n log n) complexity. The function should filter log files based on the regex pattern before sorting them by modification time. If no files match the pattern or the directory is empty, the function returns None. The function should output with:\n",
      "    str or None: The path to the most recent log file that matches the pattern, or None if no matching files are found.\n",
      "You should write self-contained code starting with:\n",
      "```\n",
      "import os\n",
      "import re\n",
      "def task_func(pattern, log_dir='/var/log/'):\n",
      "```\n",
      "------------------------------------------------------\n",
      "\n",
      "Constraint:\n",
      " {'type': 'Input and Output Handling', 'constraint': 'The function should output with: str or None: The path to the most recent log file that matches the pattern, or None if no matching files are found.', 'instruction_part': 'Extracted from instruction'}\n",
      "Constraint:\n",
      " {'type': 'Code Structure and Modularity', 'constraint': \"You should write self-contained code starting with: import os\\nimport re\\ndef task_func(pattern, log_dir='/var/log/'):\", 'instruction_part': 'Extracted from instruction'}\n",
      "Constraint:\n",
      " {'type': 'Error Handling and Robustness', 'constraint': 'The function must handle cases where the specified directory does not exist by raising a FileNotFoundError.', 'instruction_part': 'Newly Generated'}\n",
      "Constraint:\n",
      " {'type': 'Error Handling and Robustness', 'constraint': 'If the regex pattern is invalid, the function should raise a re.error exception.', 'instruction_part': 'Newly Generated'}\n",
      "Constraint:\n",
      " {'type': 'Performance and Optimization', 'constraint': 'The function should efficiently handle directories with a large number of files, ensuring that the sorting operation does not exceed O(n log n) complexity.', 'instruction_part': 'Newly Generated'}\n",
      "Constraint:\n",
      " {'type': 'File and Data Management', 'constraint': \"The function should ensure that it only processes files with a '.log' extension to avoid unnecessary errors.\", 'instruction_part': 'Newly Generated'}\n",
      "Constraint:\n",
      " {'type': 'Input and Output Handling', 'constraint': 'The function should return None immediately if the directory is empty, without attempting to process any files.', 'instruction_part': 'Newly Generated'}\n",
      "Constraint:\n",
      " {'type': 'Data Processing and Transformation', 'constraint': 'The function should filter log files based on the regex pattern before sorting them by modification time.', 'instruction_part': 'Newly Generated'}\n",
      "------------------------------------------------------\n",
      "\n",
      "Response: \n",
      " ```python\n",
      "import os\n",
      "import re\n",
      "import glob\n",
      "\n",
      "def task_func(pattern, log_dir='/var/log/'):\n",
      "    \"\"\"\n",
      "    Find the latest log file in a specified directory that matches a given regex pattern.\n",
      "\n",
      "    Args:\n",
      "    pattern (str): A regex pattern to filter log files.\n",
      "    log_dir (str): The directory to search for log files. Defaults to '/var/log/'.\n",
      "\n",
      "    Returns:\n",
      "    str or None: The path to the most recent log file that matches the pattern, or None if no matching files are found.\n",
      "    \"\"\"\n",
      "\n",
      "    # Check if the directory exists\n",
      "    if not os.path.exists(log_dir):\n",
      "        raise FileNotFoundError(f\"The directory '{log_dir}' does not exist.\")\n",
      "\n",
      "    # Compile the regex pattern to check for validity\n",
      "    try:\n",
      "        regex = re.compile(pattern)\n",
      "    except re.error as e:\n",
      "        raise re.error(f\"Invalid regex pattern: {e}\")\n",
      "\n",
      "    # Use glob to efficiently find all log files in the directory\n",
      "    log_files = glob.glob(os.path.join(log_dir, '*.log'))\n",
      "\n",
      "    # Filter log files based on the regex pattern\n",
      "    matching_files = [file for file in log_files if regex.search(os.path.basename(file))]\n",
      "\n",
      "    # If no files match the pattern or the directory is empty, return None\n",
      "    if not matching_files:\n",
      "        return None\n",
      "\n",
      "    # Sort the matching files by modification time\n",
      "    # This operation has a time complexity of O(n log n) due to the sorting\n",
      "    matching_files.sort(key=os.path.getmtime)\n",
      "\n",
      "    # Return the path to the most recent log file\n",
      "    return matching_files[-1]\n",
      "```\n",
      "\n",
      "This function uses the `glob` module to efficiently find all log files in the specified directory. It then filters these files based on the provided regex pattern and sorts the matching files by modification time. The most recent log file is returned as a string. If no files match the pattern or the directory is empty, the function returns `None`. The function also checks for the existence of the directory and raises a `FileNotFoundError` if it does not exist. If the regex pattern is invalid, it raises a `re.error` exception.\n",
      "Correctness: \n",
      " Wrong\n",
      "Correctness Reason: \n",
      " ```json\n",
      "{\n",
      "  \"reason\": \"The code contains syntax errors due to nested triple backticks and an extra '```python' line at the start, which would cause a syntax error if run as-is. Additionally, the instruction requires the function to return None if the directory is empty or no files match, but the code raises FileNotFoundError if the directory does not exist, which is not specified in the instruction and thus violates constraint correctness. Therefore, the code is not fully correct.\",\n",
      "  \"correctness\": \"Wrong\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "big_code_df = big_code_df.reset_index()\n",
    "evaluate_row_constraints_strict(2, big_code_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6349a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "277ba829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: ajibawa-2023/Python-Code-23k-ShareGPT\n",
      "Og_instruction:\n",
      " Please write a code to delete all items from a list with a value of 3 and calculate the sum of the remaining items. Additionally, you should handle cases where the list is empty or contains only items with a value of 3. The time complexity of your solution should be O(n), where n is the length of the list. You are not allowed to use any built-in functions or methods that directly solve this problem (e.g. using filter(), list comprehension, etc.).\n",
      "\n",
      "mylist = [2, 3, 3, 4, 5, 6, 4, 7]\n",
      "\n",
      "#Expected Output\n",
      "The sum of the remaining items after deleting all items with a value of 3 is 31.\n",
      "------------------------------------------------------\n",
      "\n",
      "combined_instruction:\n",
      " Please write a code to delete all items from a list with a value of 3 and calculate the sum of the remaining items. The function must correctly delete all occurrences of the value 3 from the list. Additionally, you should handle cases where the list is empty or contains only items with a value of 3, and the function should return 0 if the list is empty or contains only items with a value of 3. The function must accurately calculate the sum of the remaining items after deletion. The time complexity of your solution should be O(n), where n is the length of the list. You are not allowed to use any built-in functions or methods that directly solve this problem (e.g. using filter(), list comprehension, etc.).\n",
      "\n",
      "mylist = [2, 3, 3, 4, 5, 6, 4, 7]\n",
      "\n",
      "#Expected Output\n",
      "The sum of the remaining items after deleting all items with a value of 3 is 31.\n",
      "------------------------------------------------------\n",
      "\n",
      "Constraint:\n",
      " {'type': 'Error Handling and Robustness', 'constraint': 'Handle cases where the list is empty.', 'instruction_part': 'Extracted from instruction'}\n",
      "Constraint:\n",
      " {'type': 'Error Handling and Robustness', 'constraint': 'Handle cases where the list contains only items with a value of 3.', 'instruction_part': 'Extracted from instruction'}\n",
      "Constraint:\n",
      " {'type': 'Performance and Optimization', 'constraint': 'The time complexity of your solution should be O(n), where n is the length of the list.', 'instruction_part': 'Extracted from instruction'}\n",
      "Constraint:\n",
      " {'type': 'Code Structure and Modularity', 'constraint': 'You are not allowed to use any built-in functions or methods that directly solve this problem (e.g. using filter(), list comprehension, etc.).', 'instruction_part': 'Extracted from instruction'}\n",
      "Constraint:\n",
      " {'type': 'Data Processing and Transformation', 'constraint': 'The function must correctly delete all occurrences of the value 3 from the list.', 'instruction_part': 'Newly Generated'}\n",
      "Constraint:\n",
      " {'type': 'Mathematical Computation', 'constraint': 'The function must accurately calculate the sum of the remaining items after deletion.', 'instruction_part': 'Newly Generated'}\n",
      "Constraint:\n",
      " {'type': 'Input and Output Handling', 'constraint': 'The function should return 0 if the list is empty or contains only items with a value of 3.', 'instruction_part': 'Newly Generated'}\n",
      "------------------------------------------------------\n",
      "\n",
      "Response: \n",
      " Here's a Python function that accomplishes the task:\n",
      "\n",
      "```python\n",
      "def delete_and_sum(mylist):\n",
      "    \"\"\"\n",
      "    Deletes all items with a value of 3 from the list and returns the sum of the remaining items.\n",
      "\n",
      "    Args:\n",
      "        mylist (list): The input list.\n",
      "\n",
      "    Returns:\n",
      "        int: The sum of the remaining items after deletion.\n",
      "    \"\"\"\n",
      "    # Initialize sum and index variables\n",
      "    total_sum = 0\n",
      "    index = 0\n",
      "\n",
      "    # Loop through the list until we've processed all elements\n",
      "    while index < len(mylist):\n",
      "        # If the current element is not 3, add it to the sum\n",
      "        if mylist[index] != 3:\n",
      "            total_sum += mylist[index]\n",
      "        # If the current element is 3, remove it from the list\n",
      "        else:\n",
      "            # Remove the element at the current index\n",
      "            mylist.pop(index)\n",
      "            # Decrement the index to avoid skipping elements\n",
      "            index -= 1\n",
      "        # Move to the next element\n",
      "        index += 1\n",
      "\n",
      "    # Return the sum of the remaining elements\n",
      "    return total_sum\n",
      "\n",
      "mylist = [2, 3, 3, 4, 5, 6, 4, 7]\n",
      "print(\"The sum of the remaining items after deleting all items with a value of 3 is\", delete_and_sum(mylist))\n",
      "```\n",
      "\n",
      "This function works by iterating through the list and adding elements to the sum if they're not equal to 3. If an element is equal to 3, it's removed from the list using the `pop()` method, and the index is decremented to avoid skipping elements. This process continues until all elements have been processed, at which point the function returns the sum of the remaining elements.\n",
      "\n",
      "The time complexity of this solution is O(n), where n is the length of the list, because we're making a single pass through the list. The space complexity is O(1), because we're not using any additional data structures that scale with the input size.\n",
      "Correctness: \n",
      " Wrong\n",
      "Correctness Reason: \n",
      " ```json\n",
      "{\n",
      "  \"reason\": \"The code has a syntax error due to nested triple backticks and an extraneous string outside the code block. Ignoring formatting issues, the logic inside the function is flawed: decrementing the index after popping causes skipping elements because the index is immediately incremented again, leading to incorrect removal of all 3s. For example, with consecutive 3s, some 3s remain. Also, the code modifies the input list in place, which is not explicitly forbidden but may be unexpected. The code runs but does not correctly delete all 3s, thus failing semantic correctness.\",\n",
      "  \"correctness\": \"Wrong\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "evaluate_row_constraints_strict(70, after_df_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a5cfebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 37\n",
      "dataset: xlangai/DS-1000\n",
      "Og_instruction:\n",
      " Problem:\n",
      "Let's say I have 5 columns.\n",
      "pd.DataFrame({\n",
      "'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n",
      "'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n",
      "'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
      "'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n",
      "\n",
      "\n",
      "Is there a function to know the type of relationship each par of columns has? (one-to-one, one-to-many, many-to-one, many-to-many)\n",
      "An list output like:\n",
      "['Column1 Column2 one-to-many',\n",
      " 'Column1 Column3 one-to-many',\n",
      " 'Column1 Column4 one-to-one',\n",
      " 'Column1 Column5 one-to-many',\n",
      " 'Column2 Column1 many-to-one',\n",
      " 'Column2 Column3 many-to-many',\n",
      " 'Column2 Column4 many-to-one',\n",
      " 'Column2 Column5 many-to-many',\n",
      " 'Column3 Column1 many-to-one',\n",
      " 'Column3 Column2 many-to-many',\n",
      " 'Column3 Column4 many-to-one',\n",
      " 'Column3 Column5 many-to-many',\n",
      " 'Column4 Column1 one-to-one',\n",
      " 'Column4 Column2 one-to-many',\n",
      " 'Column4 Column3 one-to-many',\n",
      " 'Column4 Column5 one-to-many',\n",
      " 'Column5 Column1 many-to-one',\n",
      " 'Column5 Column2 many-to-many',\n",
      " 'Column5 Column3 many-to-many',\n",
      " 'Column5 Column4 many-to-one']\n",
      "\n",
      "\n",
      "A:\n",
      "<code>\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "df = pd.DataFrame({\n",
      "    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n",
      "    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n",
      "    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
      "    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n",
      "</code>\n",
      "result = ... # put solution in this variable\n",
      "BEGIN SOLUTION\n",
      "<code>\n",
      "------------------------------------------------------\n",
      "\n",
      "combined_instruction:\n",
      " Problem:\n",
      "Let's say I have 5 columns.\n",
      "\n",
      "pd.DataFrame({\n",
      "'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n",
      "'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n",
      "'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
      "'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n",
      "\n",
      "Is there a function to know the type of relationship each pair of columns has? (one-to-one, one-to-many, many-to-one, many-to-many) The function must accept a pandas DataFrame and two column names as input parameters, returning a string that describes the relationship between the two columns. An list output like:\n",
      "['Column1 Column2 one-to-many',\n",
      " 'Column1 Column3 one-to-many',\n",
      " 'Column1 Column4 one-to-one',\n",
      " 'Column1 Column5 one-to-many',\n",
      " 'Column2 Column1 many-to-one',\n",
      " 'Column2 Column3 many-to-many',\n",
      " 'Column2 Column4 many-to-one',\n",
      " 'Column2 Column5 many-to-many',\n",
      " 'Column3 Column1 many-to-one',\n",
      " 'Column3 Column2 many-to-many',\n",
      " 'Column3 Column4 many-to-one',\n",
      " 'Column3 Column5 many-to-many',\n",
      " 'Column4 Column1 one-to-one',\n",
      " 'Column4 Column2 one-to-many',\n",
      " 'Column4 Column3 one-to-many',\n",
      " 'Column4 Column5 one-to-many',\n",
      " 'Column5 Column1 many-to-one',\n",
      " 'Column5 Column2 many-to-many',\n",
      " 'Column5 Column3 many-to-many',\n",
      " 'Column5 Column4 many-to-one']\n",
      "\n",
      "The function must accurately determine the relationship type based on the counts of unique values in the specified columns, ensuring that the logic accounts for all possible relationships. A:\n",
      "<code>\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "df = pd.DataFrame({\n",
      "    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n",
      "    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n",
      "    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
      "    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n",
      "</code>\n",
      "result = ... # put solution in this variable\n",
      "BEGIN SOLUTION\n",
      "<code>\n",
      "------------------------------------------------------\n",
      "\n",
      "Constraint:\n",
      " {'type': 'Input and Output Handling', 'constraint': 'The function must accept a pandas DataFrame and two column names as input parameters, returning a string that describes the relationship between the two columns.', 'instruction_part': 'Newly Generated'}\n",
      "Constraint:\n",
      " {'type': 'Data Processing and Transformation', 'constraint': 'The function must accurately determine the relationship type based on the counts of unique values in the specified columns, ensuring that the logic accounts for all possible relationships.', 'instruction_part': 'Newly Generated'}\n",
      "Constraint:\n",
      " {'type': 'Reproducibility and Consistency', 'constraint': \"The output format must be consistent, providing a list of relationship strings in the format 'ColumnA ColumnB relationship_type' for all pairs of columns.\", 'instruction_part': 'Newly Generated'}\n",
      "------------------------------------------------------\n",
      "\n",
      "Response: \n",
      " Here's a Python function that uses the pandas library to determine the relationship type between two columns in a DataFrame. This function works by counting the unique values in each column and comparing these counts to determine the relationship type.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "def determine_relationship(df, col1, col2):\n",
      "    # Get the counts of unique values in each column\n",
      "    count1 = df[col1].nunique()\n",
      "    count2 = df[col2].nunique()\n",
      "\n",
      "    # Determine the relationship type based on the counts\n",
      "    if count1 == 1 and count2 > 1:\n",
      "        return f\"{col1} {col2} one-to-many\"\n",
      "    elif count1 > 1 and count2 == 1:\n",
      "        return f\"{col2} {col1} many-to-one\"\n",
      "    elif count1 == 1 and count2 == 1:\n",
      "        return f\"{col1} {col2} one-to-one\"\n",
      "    elif count1 > 1 and count2 > 1:\n",
      "        # Check if there are any duplicate pairs\n",
      "        if len(df[[col1, col2]].drop_duplicates()) == len(df):\n",
      "            return f\"{col1} {col2} many-to-many\"\n",
      "        else:\n",
      "            return f\"{col1} {col2} many-to-one\"\n",
      "\n",
      "# Create the DataFrame\n",
      "df = pd.DataFrame({\n",
      "    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n",
      "    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n",
      "    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
      "    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n",
      "\n",
      "# Call the function to determine the relationships\n",
      "result = []\n",
      "for col1 in df.columns:\n",
      "    for col2 in df.columns:\n",
      "        if col1 != col2:\n",
      "            result.append(determine_relationship(df, col1, col2))\n",
      "\n",
      "# Print the result\n",
      "print(result)\n",
      "```\n",
      "\n",
      "This function works by comparing the counts of unique values in each column. If one column has a unique value and the other column has more than one unique value, it's a one-to-many relationship. If one column has more than one unique value and the other column has a unique value, it's a many-to-one relationship. If both columns have a unique value, it's a one-to-one relationship. If both columns have more than one unique value, it's a many-to-many relationship unless there are any duplicate pairs, in which case it's a many-to-one relationship.\n",
      "\n",
      "The function is then called for each pair of columns in the DataFrame, and the results are stored in a list. This list is then printed to the console.\n",
      "Correctness: \n",
      " Wrong\n",
      "Correctness Reason: \n",
      " {\"reason\": \"The code contains syntax errors due to improper formatting and inclusion of explanatory text inside code blocks. The logic in the function determine_relationship is flawed and does not correctly identify the relationship types as per the example output. Specifically, it misclassifies relationships and does not handle the directionality correctly. The code also does not produce the exact required output list. Therefore, it fails both syntax and semantic correctness.\", \"correctness\": \"Wrong\"}\n"
     ]
    }
   ],
   "source": [
    "evaluate_row_constraints_strict(37, after_df_wrong_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29151bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 25 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      " 19  response                         1489 non-null   object \n",
      " 20  constraint_adherence_responses   1489 non-null   object \n",
      " 21  Constraint_adherence             1489 non-null   object \n",
      " 22  correctness_level                1489 non-null   object \n",
      " 23  correctness_reason               1489 non-null   object \n",
      " 24  code_correctness_response        1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(20)\n",
      "memory usage: 290.9+ KB\n"
     ]
    }
   ],
   "source": [
    "before_file = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/LLMjudge_outputs/code_correctness/granite-3.1-8b-instruct_results_correctness.jsonl\", lines=True)\n",
    "before_file.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b8ce963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 22 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      " 19  response                         1489 non-null   object \n",
      " 20  correctness_level                1489 non-null   object \n",
      " 21  code_correctness_response        1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(17)\n",
      "memory usage: 256.0+ KB\n"
     ]
    }
   ],
   "source": [
    "after_df = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/LLMjudge_outputs_test/code_correctness/llama-3.1-8b-instruct_with_correctness.jsonl\", lines=True)\n",
    "after_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cc9720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correctness(df,colname=\"correctness_level\"):\n",
    "    completely_correct = df[colname].value_counts().get(\"Completely Correct\", 0)\n",
    "    partially_correct = df[colname].value_counts().get(\"Partially Correct\", 0)\n",
    "    wrong = df[colname].value_counts().get(\"Wrong\", 0)\n",
    "    # Percetages of each correctness level\n",
    "    total = len(df)\n",
    "    completely_correct_percentage = (completely_correct / total) * 100 if total > 0 else 0\n",
    "    partially_correct_percentage = (partially_correct / total) * 100 if total > 0 else 0\n",
    "    wrong_percentage = (wrong / total) * 100 if total > 0 else 0\n",
    "    print(f\"Completely Correct: {completely_correct} ({completely_correct_percentage:.2f}%)\")\n",
    "    print(f\"Partially Correct: {partially_correct} ({partially_correct_percentage:.2f}%)\")\n",
    "    print(f\"Wrong: {wrong} ({wrong_percentage:.2f}%)\")\n",
    "    # Per uique value in the dataset column\n",
    "    unique_values = df[\"dataset\"].unique()\n",
    "    print(f\"Unique datasets: {len(unique_values)}\")\n",
    "    print(\"Datasets:\", unique_values)\n",
    "    # Print all 3 correctness levels and their percentages for each uique value in the dataset column\n",
    "    df_2 = df.groupby(\"dataset\")[colname].value_counts(normalize=True).unstack(fill_value=0) * 100\n",
    "    print(\"\\nCorrectness levels by dataset:\")\n",
    "    # print(df_2)\n",
    "    # Percentages of each correctness level for each unique value in the dataset column\n",
    "    for dataset in unique_values:\n",
    "        dataset_df = df[df[\"dataset\"] == dataset]\n",
    "        completely_correct = dataset_df[colname].value_counts().get(\"Completely Correct\", 0)\n",
    "        partially_correct = dataset_df[colname].value_counts().get(\"Partially Correct\", 0)\n",
    "        wrong = dataset_df[colname].value_counts().get(\"Wrong\", 0)\n",
    "        total = len(dataset_df)\n",
    "        completely_correct_percentage = (completely_correct / total) * 100 if total > 0 else 0\n",
    "        partially_correct_percentage = (partially_correct / total) * 100 if total > 0 else 0\n",
    "        wrong_percentage = (wrong / total) * 100 if total > 0 else 0\n",
    "        print(f\"\\nDataset: {dataset}\")\n",
    "        print(f\"Completely Correct: {completely_correct} ({completely_correct_percentage:.2f}%)\")\n",
    "        print(f\"Partially Correct: {partially_correct} ({partially_correct_percentage:.2f}%)\")\n",
    "        print(f\"Wrong: {wrong} ({wrong_percentage:.2f}%)\")\n",
    "    return [completely_correct, partially_correct, wrong, completely_correct_percentage, partially_correct_percentage, wrong_percentage]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f3150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completely Correct: 942 (63.26%)\n",
      "Partially Correct: 128 (8.60%)\n",
      "Wrong: 419 (28.14%)\n",
      "Unique datasets: 4\n",
      "Datasets: ['ajibawa-2023/Python-Code-23k-ShareGPT' 'xlangai/DS-1000'\n",
      " 'bigcode/bigcodebench' 'Multilingual-Multimodal-NLP/McEval-Instruct']\n",
      "\n",
      "Correctness levels by dataset:\n",
      "\n",
      "Dataset: ajibawa-2023/Python-Code-23k-ShareGPT\n",
      "Completely Correct: 222 (59.20%)\n",
      "Partially Correct: 39 (10.40%)\n",
      "Wrong: 114 (30.40%)\n",
      "\n",
      "Dataset: xlangai/DS-1000\n",
      "Completely Correct: 127 (59.07%)\n",
      "Partially Correct: 8 (3.72%)\n",
      "Wrong: 80 (37.21%)\n",
      "\n",
      "Dataset: bigcode/bigcodebench\n",
      "Completely Correct: 276 (73.60%)\n",
      "Partially Correct: 27 (7.20%)\n",
      "Wrong: 72 (19.20%)\n",
      "\n",
      "Dataset: Multilingual-Multimodal-NLP/McEval-Instruct\n",
      "Completely Correct: 317 (60.50%)\n",
      "Partially Correct: 54 (10.31%)\n",
      "Wrong: 153 (29.20%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.int64(317),\n",
       " np.int64(54),\n",
       " np.int64(153),\n",
       " np.float64(60.49618320610687),\n",
       " np.float64(10.305343511450381),\n",
       " np.float64(29.198473282442748)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_correctness(before_file, colname=\"correctness_level\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6c4bb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completely Correct: 475 (31.90%)\n",
      "Partially Correct: 510 (34.25%)\n",
      "Wrong: 504 (33.85%)\n",
      "Unique datasets: 4\n",
      "Datasets: ['ajibawa-2023/Python-Code-23k-ShareGPT' 'xlangai/DS-1000'\n",
      " 'bigcode/bigcodebench' 'Multilingual-Multimodal-NLP/McEval-Instruct']\n",
      "\n",
      "Correctness levels by dataset:\n",
      "\n",
      "Dataset: ajibawa-2023/Python-Code-23k-ShareGPT\n",
      "Completely Correct: 119 (31.73%)\n",
      "Partially Correct: 123 (32.80%)\n",
      "Wrong: 133 (35.47%)\n",
      "\n",
      "Dataset: xlangai/DS-1000\n",
      "Completely Correct: 76 (35.35%)\n",
      "Partially Correct: 78 (36.28%)\n",
      "Wrong: 61 (28.37%)\n",
      "\n",
      "Dataset: bigcode/bigcodebench\n",
      "Completely Correct: 143 (38.13%)\n",
      "Partially Correct: 109 (29.07%)\n",
      "Wrong: 123 (32.80%)\n",
      "\n",
      "Dataset: Multilingual-Multimodal-NLP/McEval-Instruct\n",
      "Completely Correct: 137 (26.15%)\n",
      "Partially Correct: 200 (38.17%)\n",
      "Wrong: 187 (35.69%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.int64(137),\n",
       " np.int64(200),\n",
       " np.int64(187),\n",
       " np.float64(26.14503816793893),\n",
       " np.float64(38.16793893129771),\n",
       " np.float64(35.68702290076336)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_correctness(after_df, colname=\"correctness_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cc51716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_df.to_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/LLMjudge_outputs_test/code_correctness/llama-3.1-8b-instruct_with_correctness.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b504415",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sampled_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9dd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, alignments = evaluate_row_constraints_strict(5, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d084269",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, alignments = evaluate_row_constraints_strict(4, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f09dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, alignments = evaluate_row_constraints_strict(5, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, alignments = evaluate_row_constraints_strict(9, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb05c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, alignments = evaluate_row_constraints_strict(7, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe4a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "area = 78.65\n",
    "print(\"{:.2f}\".format(area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(num):\n",
    "    if num < 2:\n",
    "        return False\n",
    "    for i in range(2, int(num ** 0.5) + 1):\n",
    "        if num % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_prime_sum_array(N, M, X):\n",
    "    prime_sum_array = []\n",
    "    current_number = M\n",
    "    sum_so_far = 0\n",
    "\n",
    "    while len(prime_sum_array) < N and current_number <= X:\n",
    "        if is_prime(sum_so_far + current_number):\n",
    "            prime_sum_array.append(current_number)\n",
    "            sum_so_far += current_number\n",
    "        current_number += M\n",
    "\n",
    "    if not is_prime(sum_so_far):\n",
    "        return []\n",
    "\n",
    "    return prime_sum_array\n",
    "\n",
    "# Example usage\n",
    "N = 5\n",
    "M = 3\n",
    "X = 20\n",
    "\n",
    "result = find_prime_sum_array(N, M, X)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56acc50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming df is loaded and final_constraints parsed:\n",
    "# If not parsed yet, do:\n",
    "df['final_constraints'] = df['final_constraints'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Gather all instruction_part values\n",
    "parts = []\n",
    "for entry in df['final_constraints']:\n",
    "    for rec in entry:\n",
    "        parts.append(rec.get('instruction_part'))\n",
    "\n",
    "# Count frequencies\n",
    "part_counts = Counter(parts)\n",
    "\n",
    "print(\"Instruction Part Counts:\")\n",
    "for part, count in part_counts.items():\n",
    "    print(f\"- {part}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a9df2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# 1. Load & parse\n",
    "df = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v7.jsonl\", lines=True)  # or pd.read_csv(...)\n",
    "df['final_constraints'] = df['final_constraints'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# 2. Normalize the instruction_part labels in the DataFrame itself\n",
    "def normalize_parts(recs):\n",
    "    for rec in recs:\n",
    "        part = rec.get(\"instruction_part\", \"\")\n",
    "        if part.startswith(\"Original source\"):\n",
    "            rec[\"instruction_part\"] = \"Extracted from instruction\"\n",
    "        elif part == \"Combined/Refined\":\n",
    "            rec[\"instruction_part\"] = \"Newly Generated\"\n",
    "    return recs\n",
    "\n",
    "df['final_constraints'] = df['final_constraints'].apply(normalize_parts)\n",
    "\n",
    "# 3. (Optional) Write the cleaned DataFrame back out:\n",
    "df.to_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v7.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def normalize_parts(constraints_list):\n",
    "    \"\"\"\n",
    "    Normalize any stray instruction_part labels in a list of constraint dicts:\n",
    "      - \"Original source: ...\" → \"Extracted from instruction\"\n",
    "      - \"Combined/Refined\"    → \"Newly Generated\"\n",
    "    \"\"\"\n",
    "    for rec in constraints_list:\n",
    "        part = rec.get(\"instruction_part\", \"\")\n",
    "        if part.startswith(\"Original source\"):\n",
    "            rec[\"instruction_part\"] = \"Extracted from instruction\"\n",
    "        elif part == \"Combined/Refined\":\n",
    "            rec[\"instruction_part\"] = \"Newly Generated\"\n",
    "    return constraints_list\n",
    "\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Traverse the folder, normalize 'final_constraints' in each .jsonl/.csv file,\n",
    "    and overwrite the files in place.\n",
    "    \"\"\"\n",
    "    for fname in os.listdir(folder_path):\n",
    "        path = os.path.join(folder_path, fname)\n",
    "        if not os.path.isfile(path):\n",
    "            continue\n",
    "        # load\n",
    "        if fname.endswith('.jsonl'):\n",
    "            df = pd.read_json(path, lines=True)\n",
    "        elif fname.endswith('.csv'):\n",
    "            df = pd.read_csv(path)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # parse and normalize\n",
    "        df['final_constraints'] = df['final_constraints'].apply(\n",
    "            lambda x: normalize_parts(ast.literal_eval(x)) if isinstance(x, str) else normalize_parts(x)\n",
    "        )\n",
    "\n",
    "        # save back\n",
    "        if fname.endswith('.jsonl'):\n",
    "            df.to_json(path, orient='records', lines=True, force_ascii=False)\n",
    "        else:\n",
    "            df.to_csv(path, index=False)\n",
    "\n",
    "        print(f\"Normalized and overwritten: {path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    folder = \"/dccstor/shanmukh/sravani_internship/benchmark_experiments/outputs/experiments\"\n",
    "    process_folder(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/metrics_outputs/test/granite-3.1-2b-instruct_results_correctness_metrics_extended.jsonl\",lines=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613584fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dataset\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d20b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigcode_bench = df[df[\"dataset\"]==\"bigcode/bigcodebench\"]\n",
    "ds_1000 = df[df[\"dataset\"]=='xlangai/DS-1000']\n",
    "share_gpt = df[df[\"dataset\"]=='ajibawa-2023/Python-Code-23k-ShareGPT']\n",
    "mceval_instruct = df[df[\"dataset\"]=='Multilingual-Multimodal-NLP/McEval-Instruct']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f257364",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3009d4",
   "metadata": {},
   "source": [
    "## Compute SSR by intsruction part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b22f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def compute_ssr_by_instruction_part(df, constraints_col, adherence_col):\n",
    "    \"\"\"\n",
    "    Return a dict mapping each instruction_part\n",
    "    (\"Extracted from instruction\" vs \"Newly Generated\")\n",
    "    to its overall SSR.\n",
    "    \"\"\"\n",
    "    part_stats = defaultdict(lambda: {\"sum\": 0, \"count\": 0})\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        constraints = (\n",
    "            ast.literal_eval(row[constraints_col])\n",
    "            if isinstance(row[constraints_col], str)\n",
    "            else row[constraints_col]\n",
    "        )\n",
    "        flags = row[adherence_col]\n",
    "        for rec, flag in zip(constraints, flags):\n",
    "            part = rec[\"instruction_part\"]\n",
    "            part_stats[part][\"sum\"] += flag\n",
    "            part_stats[part][\"count\"] += 1\n",
    "\n",
    "    return {part: v[\"sum\"] / v[\"count\"] for part, v in part_stats.items()},{part:v[\"count\"]for part, v in part_stats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_col = \"final_constraints\"\n",
    "adherence_col = \"Constraint_adherence\"\n",
    "result,counts = compute_ssr_by_instruction_part(granite_2b_dfs[\"ds_1000\"],constraints_col,adherence_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e222bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ea149",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ffac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from collections import defaultdict\n",
    "def count_correctness(df,correctness_col=\"correctness_level\"):\n",
    "    count = defaultdict(lambda: {\"count\": 0})\n",
    "    for _,row in df.iterrows():\n",
    "        correctness = row[correctness_col]\n",
    "        value = f\"{correctness.replace(' ', '_')}\"\n",
    "        # print(value)\n",
    "        count[value][\"count\"]+=1\n",
    "        \n",
    "    return {part: v[\"count\"] for part, v in count.items()},{part:v[\"count\"]/len(df) for part, v in count.items()}\n",
    "    return count\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc511dc",
   "metadata": {},
   "source": [
    "# Filtering datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb31bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_wise_filter(df):\n",
    "    dfs = {}\n",
    "    dfs[\"bigcode_bench\"] = df[df[\"dataset\"]==\"bigcode/bigcodebench\"]\n",
    "    dfs[\"ds_1000\"] = df[df[\"dataset\"]=='xlangai/DS-1000']\n",
    "    dfs[\"share_gpt\"] = df[df[\"dataset\"]=='ajibawa-2023/Python-Code-23k-ShareGPT']\n",
    "    dfs[\"mceval_instruct\"] = df[df[\"dataset\"]=='Multilingual-Multimodal-NLP/McEval-Instruct']\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03af744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 27 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      " 19  response                         1489 non-null   object \n",
      " 20  constraint_adherence_responses   1489 non-null   object \n",
      " 21  Constraint_adherence             1489 non-null   object \n",
      " 22  correctness_level                1489 non-null   object \n",
      " 23  correctness_reason               1489 non-null   object \n",
      " 24  code_correctness_response        1489 non-null   object \n",
      " 25  CSR_per_row                      1489 non-null   int64  \n",
      " 26  SSR_per_row                      1489 non-null   float64\n",
      "dtypes: float64(5), int64(2), object(20)\n",
      "memory usage: 314.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "granite_2b = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/metrics_outputs/test/granite-3.1-2b-instruct_results_correctness_metrics_extended.jsonl\",lines=True)\n",
    "granite_2b.info()\n",
    "granite_2b_dfs = dataset_wise_filter(granite_2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a64a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in granite_2b_dfs.items():\n",
    "    print(key)\n",
    "    count = count_correctness(value)\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3243a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "granite_2b_dfs[\"bigcode_bench\"].to_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/response_outputs/testing_responses/bigcode_granite-3.1-2b-instruct_results_correctness_metrics.jsonl\",orient=\"records\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "granite_2b_dfs[\"ds_1000\"].to_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/response_outputs/testing_responses/ds_1000_granite-3.1-2b-instruct_results_correctness_metrics.jsonl\",orient=\"records\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "granite_2b = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/LLMjudge_outputs/code_correctness/Mistral-Small-3.1-24B-Instruct-2503_results_correctness.jsonl\",lines=True)\n",
    "granite_2b.info()\n",
    "granite_2b_dfs = dataset_wise_filter(granite_2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5979b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in granite_2b_dfs.items():\n",
    "    print(key)\n",
    "    value = value.reset_index()\n",
    "    count = count_correctness(value)\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea6f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "granite_2b = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/LLMjudge_outputs/code_correctness/granite-3.1-8b-instruct_results_correctness.jsonl\",lines=True)\n",
    "granite_2b.info()\n",
    "granite_2b_dfs = dataset_wise_filter(granite_2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in granite_2b_dfs.items():\n",
    "    print(key)\n",
    "    count = count_correctness(value)\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7caad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ds_1000\")\n",
    "count = count_correctness(ds_1000)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1bf118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"share_gpt\")\n",
    "count = count_correctness(share_gpt)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mceval_instruct\")\n",
    "count = count_correctness(mceval_instruct)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c914420",
   "metadata": {},
   "source": [
    "## Analysing DS-1000 datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, alignments = evaluate_row_constraints_strict(4, granite_2b_dfs[\"ds_1000\"].reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1eb44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, alignments = evaluate_row_constraints_strict(7, granite_2b_dfs[\"ds_1000\"].reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42518383",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, alignments = evaluate_row_constraints_strict(12, granite_2b_dfs[\"ds_1000\"].reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825afc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, alignments = evaluate_row_constraints_strict(26, granite_2b_dfs[\"ds_1000\"].reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b760c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, alignments = evaluate_row_constraints_strict(102, granite_2b_dfs[\"ds_1000\"].reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184aee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, alignments = evaluate_row_constraints_strict(7, granite_2b_dfs[\"bigcode_bench\"].reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a2d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, alignments = evaluate_row_constraints_strict(25, granite_2b_dfs[\"bigcode_bench\"].reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Pre-compile a regex that looks for:\n",
    "#   \"Conditions\"  :  [  { ... } ( , { ... } )*  ]\n",
    "# with arbitrary content inside the dicts.\n",
    "_COND_PATTERN = re.compile(\n",
    "    r'\"Conditions\"\\s*:\\s*\\[\\s*'         # opening\n",
    "    r'\\{'                                # first dict\n",
    "      r'(?:[^{}]|\\{[^{}]*\\})*'           # (not perfect JSON parsing, but eats nested braces)\n",
    "    r'\\}'                                # close first dict\n",
    "    r'(?:\\s*,\\s*\\{(?:[^{}]|\\{[^{}]*\\})*\\})*'  # optionally more dicts\n",
    "    r'\\s*\\]',                            # closing ]\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "def count_condition_structures(df: pd.DataFrame, column: str) -> int:\n",
    "    \"\"\"\n",
    "    Scan `df[column]`, treat each entry as text, and count how many\n",
    "    contain a `\"Conditions\": [ {...} ]` block.\n",
    "\n",
    "    Args:\n",
    "      df      : your pandas DataFrame\n",
    "      column  : the name of the column (dtype object/string)\n",
    "\n",
    "    Returns:\n",
    "      int — the number of rows where that pattern was found\n",
    "    \"\"\"\n",
    "    # .astype(str) in case there are NaNs or non-strings\n",
    "    return df[df[column].astype(str).str.contains(_COND_PATTERN)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "granite_2b_dfs[\"ds_1000\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = count_condition_structures(granite_2b_dfs[\"ds_1000\"],\"combined_instruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0607ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e97d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = filter_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59901bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filter_df[\"combined_instruction\"][13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Compile once at module load\n",
    "_COND_PATTERN = re.compile(\n",
    "    r'\"Conditions\"\\s*:\\s*\\[\\s*'                    # key + opening bracket\n",
    "      r'\\{'                                        # first dict\n",
    "        r'(?:[^{}]|\\{[^{}]*\\})*'                   # contents (naïve brace balancing)\n",
    "      r'\\}'                                        # close first dict\n",
    "      r'(?:\\s*,\\s*\\{(?:[^{}]|\\{[^{}]*\\})*\\})*'     # optional additional dicts\n",
    "    r'\\s*\\]',                                      # closing bracket\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "def remove_conditions_if_present(text: str) -> str:\n",
    "    \"\"\"\n",
    "    If `text` contains a `\"Conditions\": [ ... ]` block, remove that entire block;\n",
    "    otherwise return it unchanged.\n",
    "    \"\"\"\n",
    "    if _COND_PATTERN.search(text):\n",
    "        # only perform the replace when the pattern is found\n",
    "        return _COND_PATTERN.sub('', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# → \n",
    "# 'No block here.'\n",
    "# 'Keep this but strip  now.'\n",
    "a = granite_2b_dfs[\"ds_1000\"][\"combined_instruction\"].astype(str).apply(remove_conditions_if_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e7e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.iloc[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "923bfd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "I have the following DataFrame:\n",
      "    Col1  Col2  Col3  Type\n",
      "0      1     2     3     1\n",
      "1      4     5     6     1\n",
      "2      7     8     9     2\n",
      "3    10    11    12     2\n",
      "4    13    14    15     3\n",
      "5    16    17    18     3\n",
      "\n",
      "The DataFrame is read from a CSV file. All rows which have Type 1 are on top, followed by the rows with Type 2, followed by the rows with Type 3, etc. I would like to shuffle the order of the DataFrame's rows according to a list. \n",
      "\n",
      "To ensure modularity and reusability, the function must be defined to accept a DataFrame and a list as parameters. For example, give a list [2, 4, 0, 3, 1, 5] and desired result should be:\n",
      "    Col1  Col2  Col3  Type\n",
      "2      7     8     9     2\n",
      "4     13    14    15     3\n",
      "0     1     2     3     1\n",
      "3    10    11    12     2\n",
      "1     4     5     6     1\n",
      "5    16    17    18     3\n",
      "...\n",
      "\n",
      "The function should validate that the input list contains valid indices for the DataFrame, raising an error if any index is out of bounds. How can I achieve this?\n",
      "\n",
      "A:\n",
      "<code>\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "df = pd.DataFrame({'Col1': [1, 4, 7, 10, 13, 16],\n",
      "                   'Col2': [2, 5, 8, 11, 14, 17],\n",
      "                   'Col3': [3, 6, 9, 12, 15, 18],\n",
      "                   'Type': [1, 1, 2, 2, 3, 3]})\n",
      "List = np.random.permutation(len(df))\n",
      "</code>\n",
      "result = ... # put solution in this variable\n",
      "BEGIN SOLUTION\n",
      "<code>\n",
      "    \"Conditions\": [{'type': 'Code Structure and Modularity', 'constraint': 'The function must be defined to accept a DataFrame and a list as parameters, ensuring modularity and reusability.', 'instruction_part': \"Original source: 'Extracted from instruction'\"}, {'type': 'Input and Output Handling', 'constraint': 'The function should validate that the input list contains valid indices for the DataFrame, raising an error if any index is out of bounds.', 'instruction_part': 'Newly Generated'}, {'type': 'Data Processing and Transformation', 'constraint': 'The function must return a new DataFrame that reflects the order specified by the input list without modifying the original DataFrame.', 'instruction_part': \"Original source: 'Extracted from instruction'\"}, {'type': 'Library and API Usage', 'constraint': \"Utilize the pandas library's iloc method to reorder the DataFrame based on the provided list of indices.\", 'instruction_part': 'Newly Generated'}] }\n",
      "</code>\n"
     ]
    }
   ],
   "source": [
    "print(granite_2b_dfs[\"ds_1000\"]['combined_instruction'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75306aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(granite_2b_dfs[\"ds_1000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d56ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "I have been struggling with removing the time zone info from a column in a pandas dataframe. I have checked the following question, but it does not work for me:\n",
      "\n",
      "Can I export pandas DataFrame to Excel stripping tzinfo?\n",
      "\n",
      "I used tz_localize to assign a timezone to a datetime object, because I need to convert to another timezone using tz_convert. This adds an UTC offset, in the way \"-06:00\". I need to get rid of this offset, because it results in an error when I try to export the dataframe to Excel. \n",
      "\n",
      "To ensure that the output does not include timezone information when exporting to Excel, I need to remove the UTC offset from the datetime column in the pandas dataframe.\n",
      "\n",
      "Actual output\n",
      "\n",
      "2015-12-01 00:00:00-06:00\n",
      "\n",
      "Desired output\n",
      "2015-12-01 00:00:00\n",
      "\n",
      "I have tried to get the characters I want using the str() method, but it seems the result of tz_localize is not a string. My solution so far is to export the dataframe to csv, read the file, and to use the str() method to get the characters I want. However, I would like to avoid using intermediate file formats (like CSV) for data transformation when a direct method is available. Is there an easier solution?\n",
      "\n",
      "A:\n",
      "<code>\n",
      "import pandas as pd\n",
      "\n",
      "example_df = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\n",
      "example_df['datetime'] = pd.to_datetime(example_df['datetime'])\n",
      "def f(df=example_df):\n",
      "    # return the solution in this function\n",
      "    # result = f(df)\n",
      "    ### BEGIN SOLUTION\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\"Conditions\"\\s*:\\s*\\[\\s*\\{.*?\\}(?:\\s*,\\s*\\{.*?\\})*\\s*\\]\\s}'\n",
    "print(granite_2b_dfs[\"ds_1000\"].combined_instruction.str.replace(pattern, '', regex=True).str.strip().iloc[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c289df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>instruction</th>\n",
       "      <th>code</th>\n",
       "      <th>test</th>\n",
       "      <th>relevant_categories</th>\n",
       "      <th>simplified_instruction</th>\n",
       "      <th>extracted_constraints</th>\n",
       "      <th>final_comprehensive_constraints</th>\n",
       "      <th>filtered_relevant_constraints</th>\n",
       "      <th>...</th>\n",
       "      <th>constraint_presence_response</th>\n",
       "      <th>final_constraints</th>\n",
       "      <th>response</th>\n",
       "      <th>constraint_adherence_responses</th>\n",
       "      <th>Constraint_adherence</th>\n",
       "      <th>correctness_level</th>\n",
       "      <th>correctness_reason</th>\n",
       "      <th>code_correctness_response</th>\n",
       "      <th>CSR_per_row</th>\n",
       "      <th>SSR_per_row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>375</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have the following DataFrame:\\n   ...</td>\n",
       "      <td>def g(df, List):\\n    return df.iloc[List]\\n\\n...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have the following DataFrame:\\n   ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>Here's a function that meets your requirements...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code is free of syntax errors, logically m...</td>\n",
       "      <td>{\"reason\": \"The code is free of syntax errors,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>376</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have the following DataFrame:\\n   ...</td>\n",
       "      <td>def g(df, List):\\n    df2 = df.iloc[List].rein...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have the following DataFrame:\\n   ...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's how you can achieve this using pandas:\\...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, False, False, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors and does not correc...</td>\n",
       "      <td>{\"reason\": \"The code has syntax errors and doe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>377</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have following pandas dataframe :\\...</td>\n",
       "      <td>def g(df):\\n    return df.where(df.apply(lambd...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Data Processing and Transformation', 'Librar...</td>\n",
       "      <td>Problem:\\nI have following pandas dataframe :\\...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's how you can achieve this using pandas:\\...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[False, False, False, True, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code contains a syntax error due to incorr...</td>\n",
       "      <td>{\"reason\": \"The code contains a syntax error d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>378</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have following pandas dataframe :\\...</td>\n",
       "      <td>def g(df):\\n    return df.where(df.apply(lambd...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Data Processing and Transformation', 'Librar...</td>\n",
       "      <td>Problem:\\nI have following pandas dataframe :\\...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's the solution to your problem:\\n\\n```pyt...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[False, True, True, True, False, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code does not correctly transform the Data...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code does not cor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>379</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have following pandas dataframe :\\...</td>\n",
       "      <td>result = df.where(df.apply(lambda x: x.map(x.v...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have following pandas dataframe :\\...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's the solution to your problem:\\n\\n```pyt...</td>\n",
       "      <td>{\"Evaluation\": [ {\"Constraint\": \"Change values...</td>\n",
       "      <td>[False, True, True, True, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors due to incorrect fo...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code has syntax e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>594</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI'm using groupby on a pandas datafr...</td>\n",
       "      <td>def g(df):\\n    return df.loc[df.groupby(\"item...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem: I'm using groupby on a pandas datafra...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>result = df.groupby(\"item\", as_index=False)[\"d...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, False, False, True]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code correctly uses groupby to find the mi...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code correctly us...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>595</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have the following kind of strings...</td>\n",
       "      <td>def g(df):\\n    df['SOURCE_NAME'] = df['SOURCE...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem: I have the following kind of strings ...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>df['SOURCE_NAME'] = df['SOURCE_NAME'].str.rspl...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, False, True, True, True, True, True]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code correctly uses the rsplit method to s...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code correctly us...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>597</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have the following kind of strings...</td>\n",
       "      <td>df['SOURCE_NAME'] = df['SOURCE_NAME'].str.rspl...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have the following kind of strings...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's a solution using the `str.rsplit()` met...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has a semantic error because it incor...</td>\n",
       "      <td>{\"reason\": \"The code has a semantic error beca...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>598</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a column ( lets call it Colum...</td>\n",
       "      <td>def g(df):\\n    idx = df['Column_x'].index[df[...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem: I have a column (lets call it Column ...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's how you can achieve the desired result ...</td>\n",
       "      <td>{\"Evaluation\": [{\"Constraint\": \"Fill the first...</td>\n",
       "      <td>[False, False, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors and does not correc...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code has syntax e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>599</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a column ( lets call it Colum...</td>\n",
       "      <td>def g(df):\\n    idx = df['Column_x'].index[df[...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Data Processing and Transformation', 'Input ...</td>\n",
       "      <td>Problem: I have a column (lets call it Column ...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's the modified DataFrame based on your re...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[False, False, False, True, False, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors and does not logica...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code has syntax e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id          dataset                                        instruction  \\\n",
       "375  375  xlangai/DS-1000  Problem:\\nI have the following DataFrame:\\n   ...   \n",
       "376  376  xlangai/DS-1000  Problem:\\nI have the following DataFrame:\\n   ...   \n",
       "377  377  xlangai/DS-1000  Problem:\\nI have following pandas dataframe :\\...   \n",
       "378  378  xlangai/DS-1000  Problem:\\nI have following pandas dataframe :\\...   \n",
       "379  379  xlangai/DS-1000  Problem:\\nI have following pandas dataframe :\\...   \n",
       "..   ...              ...                                                ...   \n",
       "584  594  xlangai/DS-1000  Problem:\\nI'm using groupby on a pandas datafr...   \n",
       "585  595  xlangai/DS-1000  Problem:\\nI have the following kind of strings...   \n",
       "587  597  xlangai/DS-1000  Problem:\\nI have the following kind of strings...   \n",
       "588  598  xlangai/DS-1000  Problem:\\nI have a column ( lets call it Colum...   \n",
       "589  599  xlangai/DS-1000  Problem:\\nI have a column ( lets call it Colum...   \n",
       "\n",
       "                                                  code  \\\n",
       "375  def g(df, List):\\n    return df.iloc[List]\\n\\n...   \n",
       "376  def g(df, List):\\n    df2 = df.iloc[List].rein...   \n",
       "377  def g(df):\\n    return df.where(df.apply(lambd...   \n",
       "378  def g(df):\\n    return df.where(df.apply(lambd...   \n",
       "379  result = df.where(df.apply(lambda x: x.map(x.v...   \n",
       "..                                                 ...   \n",
       "584  def g(df):\\n    return df.loc[df.groupby(\"item...   \n",
       "585  def g(df):\\n    df['SOURCE_NAME'] = df['SOURCE...   \n",
       "587  df['SOURCE_NAME'] = df['SOURCE_NAME'].str.rspl...   \n",
       "588  def g(df):\\n    idx = df['Column_x'].index[df[...   \n",
       "589  def g(df):\\n    idx = df['Column_x'].index[df[...   \n",
       "\n",
       "                                                  test  \\\n",
       "375  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "376  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "377  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "378  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "379  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "..                                                 ...   \n",
       "584  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "585  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "587  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "588  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "589  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "\n",
       "                                   relevant_categories  \\\n",
       "375  ['Code Structure and Modularity', 'Input and O...   \n",
       "376  ['Code Structure and Modularity', 'Input and O...   \n",
       "377  ['Data Processing and Transformation', 'Librar...   \n",
       "378  ['Data Processing and Transformation', 'Librar...   \n",
       "379  ['Code Structure and Modularity', 'Input and O...   \n",
       "..                                                 ...   \n",
       "584  ['Code Structure and Modularity', 'Input and O...   \n",
       "585  ['Code Structure and Modularity', 'Input and O...   \n",
       "587  ['Code Structure and Modularity', 'Input and O...   \n",
       "588  ['Code Structure and Modularity', 'Input and O...   \n",
       "589  ['Data Processing and Transformation', 'Input ...   \n",
       "\n",
       "                                simplified_instruction  \\\n",
       "375  Problem:\\nI have the following DataFrame:\\n   ...   \n",
       "376  Problem:\\nI have the following DataFrame:\\n   ...   \n",
       "377  Problem:\\nI have following pandas dataframe :\\...   \n",
       "378  Problem:\\nI have following pandas dataframe :\\...   \n",
       "379  Problem:\\nI have following pandas dataframe :\\...   \n",
       "..                                                 ...   \n",
       "584  Problem: I'm using groupby on a pandas datafra...   \n",
       "585  Problem: I have the following kind of strings ...   \n",
       "587  Problem:\\nI have the following kind of strings...   \n",
       "588  Problem: I have a column (lets call it Column ...   \n",
       "589  Problem: I have a column (lets call it Column ...   \n",
       "\n",
       "                                 extracted_constraints  \\\n",
       "375                                                 []   \n",
       "376  [{'type': 'Data Processing and Transformation'...   \n",
       "377  [{'type': 'Data Processing and Transformation'...   \n",
       "378  [{'type': 'Data Processing and Transformation'...   \n",
       "379  [{'type': 'Data Processing and Transformation'...   \n",
       "..                                                 ...   \n",
       "584                                                 []   \n",
       "585  [{'type': 'Data Processing and Transformation'...   \n",
       "587  [{'type': 'Data Processing and Transformation'...   \n",
       "588  [{'type': 'Data Processing and Transformation'...   \n",
       "589  [{'type': 'Data Processing and Transformation'...   \n",
       "\n",
       "                       final_comprehensive_constraints  \\\n",
       "375  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "376  [{'type': 'Data Processing and Transformation'...   \n",
       "377  [{'type': 'Data Processing and Transformation'...   \n",
       "378  [{'type': 'Data Processing and Transformation'...   \n",
       "379  [{'type': 'Data Processing and Transformation'...   \n",
       "..                                                 ...   \n",
       "584  [{'type': 'Data Processing and Transformation'...   \n",
       "585  [{'type': 'Data Processing and Transformation'...   \n",
       "587  [{'type': 'Data Processing and Transformation'...   \n",
       "588  [{'type': 'Data Processing and Transformation'...   \n",
       "589  [{'type': 'Data Processing and Transformation'...   \n",
       "\n",
       "                         filtered_relevant_constraints  ...  \\\n",
       "375  [{'type': 'Code Structure and Modularity', 'co...  ...   \n",
       "376  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "377  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "378  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "379  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "..                                                 ...  ...   \n",
       "584  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "585  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "587  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "588  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "589  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "\n",
       "                          constraint_presence_response  \\\n",
       "375  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "376  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "377  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "378  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "379  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "..                                                 ...   \n",
       "584  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "585  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "587  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "588  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "589  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "\n",
       "                                     final_constraints  \\\n",
       "375  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "376  [{'type': 'Data Processing and Transformation'...   \n",
       "377  [{'type': 'Data Processing and Transformation'...   \n",
       "378  [{'type': 'Data Processing and Transformation'...   \n",
       "379  [{'type': 'Data Processing and Transformation'...   \n",
       "..                                                 ...   \n",
       "584  [{'type': 'Data Processing and Transformation'...   \n",
       "585  [{'type': 'Data Processing and Transformation'...   \n",
       "587  [{'type': 'Data Processing and Transformation'...   \n",
       "588  [{'type': 'Data Processing and Transformation'...   \n",
       "589  [{'type': 'Data Processing and Transformation'...   \n",
       "\n",
       "                                              response  \\\n",
       "375  Here's a function that meets your requirements...   \n",
       "376  Here's how you can achieve this using pandas:\\...   \n",
       "377  Here's how you can achieve this using pandas:\\...   \n",
       "378  Here's the solution to your problem:\\n\\n```pyt...   \n",
       "379  Here's the solution to your problem:\\n\\n```pyt...   \n",
       "..                                                 ...   \n",
       "584  result = df.groupby(\"item\", as_index=False)[\"d...   \n",
       "585  df['SOURCE_NAME'] = df['SOURCE_NAME'].str.rspl...   \n",
       "587  Here's a solution using the `str.rsplit()` met...   \n",
       "588  Here's how you can achieve the desired result ...   \n",
       "589  Here's the modified DataFrame based on your re...   \n",
       "\n",
       "                        constraint_adherence_responses  \\\n",
       "375  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "376  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "377  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "378  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "379  {\"Evaluation\": [ {\"Constraint\": \"Change values...   \n",
       "..                                                 ...   \n",
       "584  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "585  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "587  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "588  {\"Evaluation\": [{\"Constraint\": \"Fill the first...   \n",
       "589  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "\n",
       "                            Constraint_adherence   correctness_level  \\\n",
       "375                     [True, True, True, True]  Completely Correct   \n",
       "376                   [True, False, False, True]               Wrong   \n",
       "377           [False, False, False, True, False]               Wrong   \n",
       "378       [False, True, True, True, False, True]               Wrong   \n",
       "379             [False, True, True, True, False]               Wrong   \n",
       "..                                           ...                 ...   \n",
       "584             [True, True, False, False, True]  Completely Correct   \n",
       "585  [True, False, True, True, True, True, True]  Completely Correct   \n",
       "587                     [True, True, True, True]               Wrong   \n",
       "588                   [False, False, True, True]               Wrong   \n",
       "589     [False, False, False, True, False, True]               Wrong   \n",
       "\n",
       "                                    correctness_reason  \\\n",
       "375  The code is free of syntax errors, logically m...   \n",
       "376  The code has syntax errors and does not correc...   \n",
       "377  The code contains a syntax error due to incorr...   \n",
       "378  The code does not correctly transform the Data...   \n",
       "379  The code has syntax errors due to incorrect fo...   \n",
       "..                                                 ...   \n",
       "584  The code correctly uses groupby to find the mi...   \n",
       "585  The code correctly uses the rsplit method to s...   \n",
       "587  The code has a semantic error because it incor...   \n",
       "588  The code has syntax errors and does not correc...   \n",
       "589  The code has syntax errors and does not logica...   \n",
       "\n",
       "                             code_correctness_response CSR_per_row SSR_per_row  \n",
       "375  {\"reason\": \"The code is free of syntax errors,...           1    1.000000  \n",
       "376  {\"reason\": \"The code has syntax errors and doe...           0    0.500000  \n",
       "377  {\"reason\": \"The code contains a syntax error d...           0    0.200000  \n",
       "378  ```json\\n{\\n  \"reason\": \"The code does not cor...           0    0.666667  \n",
       "379  ```json\\n{\\n  \"reason\": \"The code has syntax e...           0    0.600000  \n",
       "..                                                 ...         ...         ...  \n",
       "584  ```json\\n{\\n  \"reason\": \"The code correctly us...           0    0.600000  \n",
       "585  ```json\\n{\\n  \"reason\": \"The code correctly us...           0    0.857143  \n",
       "587  {\"reason\": \"The code has a semantic error beca...           1    1.000000  \n",
       "588  ```json\\n{\\n  \"reason\": \"The code has syntax e...           0    0.500000  \n",
       "589  ```json\\n{\\n  \"reason\": \"The code has syntax e...           0    0.333333  \n",
       "\n",
       "[114 rows x 27 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "granite_2b_dfs[\"ds_1000\"][granite_2b_dfs[\"ds_1000\"].combined_instruction.str.contains(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8531eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "I have been struggling with removing the time zone info from a column in a pandas dataframe. I have checked the following question, but it does not work for me:\n",
      "\n",
      "Can I export pandas DataFrame to Excel stripping tzinfo?\n",
      "\n",
      "I used tz_localize to assign a timezone to a datetime object, because I need to convert to another timezone using tz_convert. This adds an UTC offset, in the way \"-06:00\". I need to get rid of this offset, because it results in an error when I try to export the dataframe to Excel. \n",
      "\n",
      "To ensure that the output does not include timezone information when exporting to Excel, I need to remove the UTC offset from the datetime column in the pandas dataframe.\n",
      "\n",
      "Actual output\n",
      "\n",
      "2015-12-01 00:00:00-06:00\n",
      "\n",
      "Desired output\n",
      "2015-12-01 00:00:00\n",
      "\n",
      "I have tried to get the characters I want using the str() method, but it seems the result of tz_localize is not a string. My solution so far is to export the dataframe to csv, read the file, and to use the str() method to get the characters I want. However, I would like to avoid using intermediate file formats (like CSV) for data transformation when a direct method is available. Is there an easier solution?\n",
      "\n",
      "A:\n",
      "<code>\n",
      "import pandas as pd\n",
      "\n",
      "example_df = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\n",
      "example_df['datetime'] = pd.to_datetime(example_df['datetime'])\n",
      "def f(df=example_df):\n",
      "    # return the solution in this function\n",
      "    # result = f(df)\n",
      "    ### BEGIN SOLUTION\n",
      "    \"Conditions\": [{'type': 'Data Processing and Transformation', 'constraint': 'Remove the UTC offset from the datetime column in the pandas dataframe.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Error Handling and Robustness', 'constraint': 'Ensure that the output does not include timezone information when exporting to Excel.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Library and API Usage', 'constraint': 'Use pandas methods such as tz_localize and tz_convert appropriately.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Input and Output Handling', 'constraint': 'The output should match the desired format without timezone information.', 'instruction_part': 'Extracted from instruction'}, {'type': 'File and Data Management', 'constraint': 'Avoid using intermediate file formats (like CSV) for data transformation when a direct method is available.', 'instruction_part': 'Newly Generated'}, {'type': 'Error Handling and Robustness', 'constraint': 'Implement error handling to manage potential issues when converting datetime formats.', 'instruction_part': 'Newly Generated'}] }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(granite_2b_dfs[\"ds_1000\"].combined_instruction.iloc[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08012031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern_identification_1 = re.compile(\n",
    "    r'###\\s*BEGIN\\s+SOLUTION'      # the header\n",
    "    r'[\\s\\S]*?'                     # any text, non-greedy\n",
    "    r'\"Conditions\"\\s*:\\s*\\[\\s*\\{'   # the start of the JSON‐list\n",
    "    r'[\\s\\S]*?'                     # anything inside\n",
    "    r'\\}\\s*\\]\\s}',                     # closing brace and bracket\n",
    "    re.IGNORECASE                   # if you want case‐insensitive match\n",
    ")\n",
    "type1 = granite_2b_dfs[\"ds_1000\"][granite_2b_dfs[\"ds_1000\"].combined_instruction.str.contains(pattern_identification_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7afd7ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "I have following pandas dataframe :\n",
      "\n",
      "\n",
      "import pandas as pd \n",
      "from pandas import Series, DataFrame\n",
      "data = DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n",
      "              'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n",
      "              'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n",
      "\n",
      "\n",
      "I'd like to change values in columns Qu1, Qu2, Qu3 according to value_counts() when value count is greater than or equal to 2, specifically keeping values cheese, potato, and banana in column Qu1. For example for Qu1 column \n",
      ">>> pd.value_counts(data.Qu1) >= 2\n",
      "cheese     True\n",
      "potato     True\n",
      "banana     True\n",
      "apple     False\n",
      "egg       False\n",
      "\n",
      "From values apple and egg, I would like to create a value 'others' in column Qu1. For column Qu2, I want no changes:\n",
      ">>> pd.value_counts(data.Qu2) >= 2\n",
      "banana     True\n",
      "apple      True\n",
      "sausage    True\n",
      "\n",
      "The final result should match the provided test_data structure as in attached test_data.\n",
      "test_data = DataFrame({'Qu1': ['other', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'other'],\n",
      "                  'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n",
      "                  'Qu3': ['other', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']})\n",
      "\n",
      "\n",
      "Thanks !\n",
      "\n",
      "A:\n",
      "<code>\n",
      "import pandas as pd\n",
      "\n",
      "example_df = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n",
      "                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n",
      "                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n",
      "def f(df=example_df):\n",
      "    # return the solution in this function\n",
      "    # result = f(df)\n",
      "    ### BEGIN SOLUTION\n",
      "    \n",
      "    ## response format\n",
      "    {\"new_instruction\": \"The new instruction with the conditions merged in a natural way.\"}\n"
     ]
    }
   ],
   "source": [
    "pattern_replace_1 = r'\"Conditions\"\\s*:\\s*\\[\\s*\\{.*?\\}(?:\\s*,\\s*\\{.*?\\})*\\s*\\]\\s}'\n",
    "print(type1.combined_instruction.str.replace(pattern_replace_1, '', regex=True).str.strip().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "543b47f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>instruction</th>\n",
       "      <th>code</th>\n",
       "      <th>test</th>\n",
       "      <th>relevant_categories</th>\n",
       "      <th>simplified_instruction</th>\n",
       "      <th>extracted_constraints</th>\n",
       "      <th>final_comprehensive_constraints</th>\n",
       "      <th>filtered_relevant_constraints</th>\n",
       "      <th>...</th>\n",
       "      <th>constraint_presence_response</th>\n",
       "      <th>final_constraints</th>\n",
       "      <th>response</th>\n",
       "      <th>constraint_adherence_responses</th>\n",
       "      <th>Constraint_adherence</th>\n",
       "      <th>correctness_level</th>\n",
       "      <th>correctness_reason</th>\n",
       "      <th>code_correctness_response</th>\n",
       "      <th>CSR_per_row</th>\n",
       "      <th>SSR_per_row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>375</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have the following DataFrame:\\n   ...</td>\n",
       "      <td>def g(df, List):\\n    return df.iloc[List]\\n\\n...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have the following DataFrame:\\n   ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>Here's a function that meets your requirements...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code is free of syntax errors, logically m...</td>\n",
       "      <td>{\"reason\": \"The code is free of syntax errors,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>376</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have the following DataFrame:\\n   ...</td>\n",
       "      <td>def g(df, List):\\n    df2 = df.iloc[List].rein...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have the following DataFrame:\\n   ...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's how you can achieve this using pandas:\\...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, False, False, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors and does not correc...</td>\n",
       "      <td>{\"reason\": \"The code has syntax errors and doe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>377</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have following pandas dataframe :\\...</td>\n",
       "      <td>def g(df):\\n    return df.where(df.apply(lambd...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Data Processing and Transformation', 'Librar...</td>\n",
       "      <td>Problem:\\nI have following pandas dataframe :\\...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's how you can achieve this using pandas:\\...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[False, False, False, True, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code contains a syntax error due to incorr...</td>\n",
       "      <td>{\"reason\": \"The code contains a syntax error d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>382</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a dataset :\\nid    url     ke...</td>\n",
       "      <td>def g(df):\\n    return df.loc[(df['keep_if_dup...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Data Processing and Transformation', 'Input ...</td>\n",
       "      <td>Problem:\\nI have a dataset :\\nid    url     ke...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>result = df.loc[df.duplicated(subset='url', ke...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, False, False, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code does not correctly implement the logi...</td>\n",
       "      <td>{\"reason\": \"The code does not correctly implem...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>383</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a dataset :\\nid    url     dr...</td>\n",
       "      <td>def g(df):\\n    return df.loc[(df['drop_if_dup...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Data Processing and Transformation', 'Input ...</td>\n",
       "      <td>Problem:\\nI have a dataset :\\nid    url     dr...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>result = df.loc[df.duplicated(subset='url', ke...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[False, True, False, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code does not correctly implement the logi...</td>\n",
       "      <td>{\"reason\": \"The code does not correctly implem...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>388</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have been struggling with removing...</td>\n",
       "      <td>df['datetime'] = df['datetime'].dt.tz_localize...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem: I have been struggling with removing ...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>To achieve the desired output without manually...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, True, True, True, True, True, True]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code correctly converts the datetime colum...</td>\n",
       "      <td>{\"reason\": \"The code correctly converts the da...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>389</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have been struggling with removing...</td>\n",
       "      <td>def g(df):\\n    df['datetime'] = df['datetime'...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem: I have been struggling with removing ...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>To remove the time zone info from the 'datetim...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has no syntax errors, but it incorrec...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code has no synta...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>400</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have the following DF\\n\\tDate\\n0  ...</td>\n",
       "      <td>df = df[df['Date'] &gt;= List[0]]\\ndf = df[df['Da...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have the following DF\\n\\tDate\\n0  ...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>To solve this problem, we'll first filter the ...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[False, True, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors and does not satisf...</td>\n",
       "      <td>{\"reason\": \"The code has syntax errors and doe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>404</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nSo I have a dataframe that looks lik...</td>\n",
       "      <td>import numpy as np\\ndef g(df):\\n    sh = 0\\n  ...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nSo I have a dataframe that looks lik...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>To achieve the desired outcome, you can follow...</td>\n",
       "      <td>{\"Evaluation\": [{\"Constraint\": \"Shift the firs...</td>\n",
       "      <td>[False, False, False, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code provided does not correctly implement...</td>\n",
       "      <td>{\"reason\": \"The code provided does not correct...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>405</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nConsidering a simple df:\\nHeaderA | ...</td>\n",
       "      <td>def g(df):\\n    return df.add_suffix('X')\\n\\nd...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Data Proces...</td>\n",
       "      <td>Problem:\\nConsidering a simple df:\\nHeaderA | ...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>Here's a reusable function that efficiently ha...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, True, True, True, True, True, True, False]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code correctly renames all columns by appe...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code correctly re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>407</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nConsidering a simple df:\\nHeaderA | ...</td>\n",
       "      <td>def g(df):\\n    for col in df.columns:\\n      ...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nConsidering a simple df:\\nHeaderA | ...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>Here's a solution using pandas' `rename` funct...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code correctly renames columns that do not...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code correctly re...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>409</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a script that generates a pan...</td>\n",
       "      <td>def g(df):\\n    return df.groupby('group').agg...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have a script that generates a pan...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's a function that meets your requirements...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code correctly identifies columns starting...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code correctly id...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>410</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a script that generates a pan...</td>\n",
       "      <td>def g(df):\\n    return df.groupby('group').agg...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem: I have a script that generates a pand...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>To achieve the dynamic solution that works wit...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors because the aggrega...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code has syntax e...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>413</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have pandas df with say, 100 rows,...</td>\n",
       "      <td>def g(df, row_list, column_list):\\n    result ...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have pandas df with say, 100 rows,...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>To achieve the desired output, you can use the...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[False, True, False, False, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code contains syntax errors and does not l...</td>\n",
       "      <td>{\"reason\": \"The code contains syntax errors an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>414</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a dataframe with numerous col...</td>\n",
       "      <td>def g(df):\\n    return df.apply(lambda x: x.va...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Data Processing and Transformation', 'Input ...</td>\n",
       "      <td>Problem:\\nI have a dataframe with numerous col...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's a function that meets your requirements...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[False, False, True, False, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors and does not produc...</td>\n",
       "      <td>{\"reason\": \"The code has syntax errors and doe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>416</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a dataframe with numerous col...</td>\n",
       "      <td>def g(df):\\n    s = ''\\n    for c in df.column...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have a dataframe with numerous col...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>Here's the Python code to achieve the desired ...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, False, True, True, False, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code contains syntax errors and does not c...</td>\n",
       "      <td>{\"reason\": \"The code contains syntax errors an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>431</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI've a data frame that looks like th...</td>\n",
       "      <td>def g(df):\\n    df.dt = pd.to_datetime(df.dt)\\...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Data Processing and Transformation', 'Librar...</td>\n",
       "      <td>Problem: I've a data frame that looks like the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>To achieve the desired output, you can follow ...</td>\n",
       "      <td>{\"Evaluation\": [{\"Constraint\": \"The solution m...</td>\n",
       "      <td>[True, False, True, False, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors and does not correc...</td>\n",
       "      <td>{\"reason\": \"The code has syntax errors and doe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>433</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI've a data frame that looks like th...</td>\n",
       "      <td>def g(df):\\n    df.dt = pd.to_datetime(df.dt)\\...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem: I've a data frame that looks like the...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>To achieve the desired output, you can follow ...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, True, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors and does not produc...</td>\n",
       "      <td>{\"reason\": \"The code has syntax errors and doe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>434</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI've a data frame that looks like th...</td>\n",
       "      <td>def g(df):\\n    df.dt = pd.to_datetime(df.dt)\\...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem: I've a data frame that looks like the...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's a function that accomplishes the desire...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, True, True, True, False, False, True, T...</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has a syntax error due to incorrect i...</td>\n",
       "      <td>{\"reason\": \"The code has a syntax error due to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>435</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI've a data frame that looks like th...</td>\n",
       "      <td>def g(df):\\n    df.dt = pd.to_datetime(df.dt)\\...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem: I've a data frame that looks like the...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's the solution to your problem:\\n\\n```pyt...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, False, True, True, False, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors and does not correc...</td>\n",
       "      <td>{\"reason\": \"The code has syntax errors and doe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>444</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI'm wondering if there is a simpler,...</td>\n",
       "      <td>result = df.loc[df['c']&gt;0.45,columns]</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>I'm wondering if there is a simpler, memory ef...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>To achieve the desired result in a memory-effi...</td>\n",
       "      <td>{\"Evaluation\": [{\"Constraint\": \"Select only th...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code is free of syntax errors, logically s...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code is free of s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>457</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have the following dataframe:\\nind...</td>\n",
       "      <td>def g(df):\\n    df['A'].replace(to_replace=0, ...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have the following dataframe:\\nind...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's a function that fulfills your requireme...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[False, False, True, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors due to incorrect fo...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code has syntax e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>458</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have the following dataframe:\\nind...</td>\n",
       "      <td>def g(df):\\n    df['A'].replace(to_replace=0, ...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have the following dataframe:\\nind...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>To fill the zeros with the posterior non-zero ...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[False, False, True, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code uses ffill() which fills with the pre...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code uses ffill()...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>465</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI am aware there are many questions ...</td>\n",
       "      <td>def g(df1, df2, columns_check_list):\\n    mask...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI am aware there are many questions ...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's the solution with the requested feature...</td>\n",
       "      <td>{\"Evaluation\": [{\"Constraint\": \"Check that all...</td>\n",
       "      <td>[True, True, False, True, True, True, True, Tr...</td>\n",
       "      <td>Partially Correct</td>\n",
       "      <td>The code has no syntax errors, but it does not...</td>\n",
       "      <td>{\"reason\": \"The code has no syntax errors, but...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>467</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have multi-index df as follows\\n\\n...</td>\n",
       "      <td>def g(df):\\n    df.index = df.index.set_levels...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have multi-index df as follows\\n\\n...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's the corrected code to parse the datetim...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, True, True, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code attempts to parse the datetime index ...</td>\n",
       "      <td>{\"reason\": \"The code attempts to parse the dat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>472</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a data frame like below \\n   ...</td>\n",
       "      <td>def g(df):\\n    mask = (df.filter(like='Value'...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have a data frame like below \\n   ...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>To solve this problem, we'll first identify th...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code correctly identifies columns prefixed...</td>\n",
       "      <td>{\"reason\": \"The code correctly identifies colu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>473</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a data frame like below \\n   ...</td>\n",
       "      <td>def g(df):\\n    mask = (df.filter(like='Value'...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have a data frame like below \\n   ...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's how you can achieve this using pandas:\\...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has a syntax error due to the incorre...</td>\n",
       "      <td>{\"reason\": \"The code has a syntax error due to...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>483</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nSay I have two dataframes:\\ndf1:    ...</td>\n",
       "      <td>def g(df1, df2):\\n    return pd.merge_asof(df2...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nSay I have two dataframes:\\ndf1:    ...</td>\n",
       "      <td>[{'type': 'Performance and Optimization', 'con...</td>\n",
       "      <td>[{'type': 'Performance and Optimization', 'con...</td>\n",
       "      <td>[{'type': 'Performance and Optimization', 'con...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Performance and Optimization', 'con...</td>\n",
       "      <td>result = df2.merge_asof(df1, on='Timestamp', d...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, True, True, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code contains a syntax error due to incorr...</td>\n",
       "      <td>{\"reason\": \"The code contains a syntax error d...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>484</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nSay I have two dataframes:\\ndf1:    ...</td>\n",
       "      <td>def g(df1, df2):\\n    return pd.merge_asof(df1...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nSay I have two dataframes:\\ndf1:    ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'type': 'Performance and Optimization', 'con...</td>\n",
       "      <td>[{'type': 'Performance and Optimization', 'con...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Performance and Optimization', 'con...</td>\n",
       "      <td>result = pd.merge_asof(df1, df2, on='Timestamp...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, False, True, True, True, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code contains syntax errors due to incorre...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code contains syn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>492</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI am trying to extract rows from a P...</td>\n",
       "      <td>def g(df, test):\\n    return df.loc[test]\\n\\nr...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem: I am trying to extract rows from a Pa...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>To extract rows from a Pandas dataframe using ...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, True, True, True, True, False]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code correctly sets the index of the dataf...</td>\n",
       "      <td>{\"reason\": \"The code correctly sets the index ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>503</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a pandas Dataframe like below...</td>\n",
       "      <td>def g(df):\\n    l = int(0.2 * len(df))\\n    df...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have a pandas Dataframe like below...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's how you can achieve this using the prov...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors due to incorrect fo...</td>\n",
       "      <td>{\"reason\": \"The code has syntax errors due to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>508</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI am trying to find col duplicates r...</td>\n",
       "      <td>def g(df):\\n    cols = list(df.filter(like='co...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI am trying to find col duplicates r...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's the solution that meets all the require...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, False, False, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has no syntax errors, but it does not...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code has no synta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>514</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI am performing a query on a DataFra...</td>\n",
       "      <td>def g(df, filter_list):\\n    return df.query(\"...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI am performing a query on a DataFra...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>Here's a function that addresses your requirem...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, True, True, True, True, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors and does not satisf...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code has syntax e...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>515</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI am performing a query on a DataFra...</td>\n",
       "      <td>def g(df, filter_list):\\n    return df.query(\"...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI am performing a query on a DataFra...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>Here's a reusable function that filters the Da...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, True, True, True, True, True, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has a syntax error in the condition c...</td>\n",
       "      <td>{\"reason\": \"The code has a syntax error in the...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>516</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a Pandas DataFrame that looks...</td>\n",
       "      <td>def g(df):\\n    return pd.melt(df)\\n\\nresult =...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have a Pandas DataFrame that looks...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>To achieve a generalizable solution for meltin...</td>\n",
       "      <td>{\"Evaluation\": [{\"Constraint\": \"Use pd.melt to...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code correctly defines a function to gener...</td>\n",
       "      <td>{\"reason\": \"The code correctly defines a funct...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>517</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a Pandas DataFrame that looks...</td>\n",
       "      <td>def g(df):\\n    result = pd.melt(df, value_var...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have a Pandas DataFrame that looks...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>To achieve the desired generalized melt functi...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code provided does not correctly implement...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code provided doe...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>524</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nExample\\nimport pandas as pd\\nimport...</td>\n",
       "      <td>def g(df):\\n    return df.groupby('r')['v'].ap...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem\\nWhen a grouped dataframe contains a v...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>result = df.groupby('r')['v'].transform(lambda...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[False, False, False, True, False, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code contains a syntax error due to incorr...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code contains a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>525</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nExample\\nimport pandas as pd\\nimport...</td>\n",
       "      <td>def g(df):\\n    return df.groupby('l')['v'].ap...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem\\nWhen a grouped dataframe contains a v...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's the solution to your problem:\\n\\n```pyt...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, True, True, True, True, True, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors due to incorrect fo...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code has syntax e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>526</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nLet's say I have 5 columns.\\npd.Data...</td>\n",
       "      <td>def get_relation(df, col1, col2):\\n    first_m...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nfrom ...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nLet's say I have 5 columns.\\npd.Data...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>Here's a function that meets your requirements...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, False, True, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The provided code does not correctly implement...</td>\n",
       "      <td>{\"reason\": \"The provided code does not correct...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>533</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\n   Survived  SibSp  Parch\\n0        ...</td>\n",
       "      <td>import numpy as np\\ndef g(df):\\n    family = n...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Data Processing and Transformation', 'Librar...</td>\n",
       "      <td>Given the above dataframe, is there an elegant...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's how you can achieve this using the Pand...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[False, False, False, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code does not correctly implement the grou...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code does not cor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>534</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\n   Survived  SibSp  Parch\\n0        ...</td>\n",
       "      <td>def g(df):\\n    family = []\\n    for i in rang...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Data Proces...</td>\n",
       "      <td>Given the above dataframe, is there an elegant...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's how you can achieve this using pandas' ...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[False, False, False, False, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code contains syntax errors and does not c...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code contains syn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>536</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nHow do I apply sort to a pandas grou...</td>\n",
       "      <td>def g(df):\\n    return df.groupby('cokey').app...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem: How do I apply sort to a pandas group...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>result = df.groupby('cokey').apply(lambda x: x...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, False, True]</td>\n",
       "      <td>Partially Correct</td>\n",
       "      <td>The code has no syntax errors and logically so...</td>\n",
       "      <td>{\"reason\": \"The code has no syntax errors and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>538</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI get how to use pd.MultiIndex.from_...</td>\n",
       "      <td>def g(df):\\n    df.columns = pd.MultiIndex.fro...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI get how to use pd.MultiIndex.from_...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's a function `g` that takes a DataFrame a...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, False, False, True, False, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors and does not correc...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code has syntax e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>542</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nHaving a pandas data frame as follow...</td>\n",
       "      <td>import numpy as np\\ndef g(df):\\n    return df....</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nHaving a pandas data frame as follow...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here is the corrected code to calculate the me...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[False, True, False, False, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code contains syntax errors and does not p...</td>\n",
       "      <td>{\"reason\": \"The code contains syntax errors an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>545</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a dataFrame with rows and col...</td>\n",
       "      <td>def g(df):\\n    return df.loc[(df.sum(axis=1) ...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Data Processing and Transformation', 'Input ...</td>\n",
       "      <td>Problem:\\nI have a dataFrame with rows and col...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>result = df.loc[df.ne(0).all(axis=1)].loc[:, d...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, False, True, True, True]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code correctly filters out rows and column...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code correctly fi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>560</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI am trying to groupby counts of dat...</td>\n",
       "      <td>def g(df):\\n    df['Date'] = pd.to_datetime(df...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI am trying to groupby counts of dat...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's the corrected code to achieve the desir...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[False, False, True, True, True, False, False,...</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code provided does not correctly implement...</td>\n",
       "      <td>{\"reason\": \"The code provided does not correct...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>562</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI am trying to groupby counts of dat...</td>\n",
       "      <td>def g(df):\\n    df['Date'] = pd.to_datetime(df...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI am trying to groupby counts of dat...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's the solution to achieve the desired out...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[False, False, True, True, False, False, False...</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code contains syntax errors and does not c...</td>\n",
       "      <td>{\"reason\": \"The code contains syntax errors an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>564</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a dataframe, e.g:\\nDate      ...</td>\n",
       "      <td>def g(df):\\n    df1 = df.groupby('Date').agg(l...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have a dataframe, e.g:\\nDate      ...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's how you can achieve this using the pand...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, True, False, True, False, False, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors due to incorrect us...</td>\n",
       "      <td>{\"reason\": \"The code has syntax errors due to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>565</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nWas trying to generate a pivot table...</td>\n",
       "      <td>def g(df):\\n    return pd.pivot_table(df, valu...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem: Was trying to generate a pivot table ...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>To achieve the desired result, you can use the...</td>\n",
       "      <td>```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code correctly uses groupby and apply to a...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code correctly us...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>566</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a dataframe:\\n\\n\\ndf = pd.Dat...</td>\n",
       "      <td>def g(df):\\n    return pd.pivot_table(df, valu...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have a dataframe:\\n\\n\\ndf = pd.Dat...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>result = df.pivot_table(values=['D', 'E'], ind...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, True, True, True, True]</td>\n",
       "      <td>Completely Correct</td>\n",
       "      <td>The code correctly uses the pivot_table functi...</td>\n",
       "      <td>{\"reason\": \"The code correctly uses the pivot_...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>570</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nWhat is an efficient way of splittin...</td>\n",
       "      <td>def g(df):\\n    return df.join(pd.DataFrame(df...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Data Processing and Transformation', 'Perfor...</td>\n",
       "      <td>Problem:\\nWhat is an efficient way of splittin...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>To efficiently split a column into multiple ro...</td>\n",
       "      <td>{\"Evaluation\": [{\"Constraint\": \"Dask does not ...</td>\n",
       "      <td>[True, True, True, True, True, True, True, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors and does not correc...</td>\n",
       "      <td>{\"reason\": \"The code has syntax errors and doe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>577</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have a Dataframe as below.\\nName  ...</td>\n",
       "      <td>def g(df):\\n    cols = list(df)[1:]\\n    for i...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Problem:\\nI have a Dataframe as below.\\nName  ...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here is the solution to calculate the cumulati...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[False, False, True, False]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code contains syntax errors and does not c...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code contains syn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>588</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have the following dataframe:\\n  k...</td>\n",
       "      <td>def g(df):\\n    return df.groupby('key1')['key...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Data Processing and Transformation', 'Librar...</td>\n",
       "      <td>Problem:\\nI have the following dataframe:\\n  k...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>result = df.groupby('key1')['key2'].transform(...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, False, True, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has syntax errors and does not correc...</td>\n",
       "      <td>{\"reason\": \"The code has syntax errors and doe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>589</td>\n",
       "      <td>xlangai/DS-1000</td>\n",
       "      <td>Problem:\\nI have the following dataframe:\\n  k...</td>\n",
       "      <td>def g(df):\\n    return df.groupby('key1')['key...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nimpor...</td>\n",
       "      <td>['Data Processing and Transformation', 'Librar...</td>\n",
       "      <td>Problem:\\nI have the following dataframe:\\n  k...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>...</td>\n",
       "      <td>```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...</td>\n",
       "      <td>[{'type': 'Data Processing and Transformation'...</td>\n",
       "      <td>Here's how you can achieve the desired result:...</td>\n",
       "      <td>```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...</td>\n",
       "      <td>[True, True, True, True, True, True]</td>\n",
       "      <td>Wrong</td>\n",
       "      <td>The code has a syntax error due to incorrect u...</td>\n",
       "      <td>```json\\n{\\n  \"reason\": \"The code has a syntax...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id          dataset                                        instruction  \\\n",
       "375  375  xlangai/DS-1000  Problem:\\nI have the following DataFrame:\\n   ...   \n",
       "376  376  xlangai/DS-1000  Problem:\\nI have the following DataFrame:\\n   ...   \n",
       "377  377  xlangai/DS-1000  Problem:\\nI have following pandas dataframe :\\...   \n",
       "382  382  xlangai/DS-1000  Problem:\\nI have a dataset :\\nid    url     ke...   \n",
       "383  383  xlangai/DS-1000  Problem:\\nI have a dataset :\\nid    url     dr...   \n",
       "388  388  xlangai/DS-1000  Problem:\\nI have been struggling with removing...   \n",
       "389  389  xlangai/DS-1000  Problem:\\nI have been struggling with removing...   \n",
       "400  400  xlangai/DS-1000  Problem:\\nI have the following DF\\n\\tDate\\n0  ...   \n",
       "404  404  xlangai/DS-1000  Problem:\\nSo I have a dataframe that looks lik...   \n",
       "405  405  xlangai/DS-1000  Problem:\\nConsidering a simple df:\\nHeaderA | ...   \n",
       "407  407  xlangai/DS-1000  Problem:\\nConsidering a simple df:\\nHeaderA | ...   \n",
       "409  409  xlangai/DS-1000  Problem:\\nI have a script that generates a pan...   \n",
       "410  410  xlangai/DS-1000  Problem:\\nI have a script that generates a pan...   \n",
       "412  413  xlangai/DS-1000  Problem:\\nI have pandas df with say, 100 rows,...   \n",
       "413  414  xlangai/DS-1000  Problem:\\nI have a dataframe with numerous col...   \n",
       "415  416  xlangai/DS-1000  Problem:\\nI have a dataframe with numerous col...   \n",
       "430  431  xlangai/DS-1000  Problem:\\nI've a data frame that looks like th...   \n",
       "432  433  xlangai/DS-1000  Problem:\\nI've a data frame that looks like th...   \n",
       "433  434  xlangai/DS-1000  Problem:\\nI've a data frame that looks like th...   \n",
       "434  435  xlangai/DS-1000  Problem:\\nI've a data frame that looks like th...   \n",
       "441  444  xlangai/DS-1000  Problem:\\nI'm wondering if there is a simpler,...   \n",
       "453  457  xlangai/DS-1000  Problem:\\nI have the following dataframe:\\nind...   \n",
       "454  458  xlangai/DS-1000  Problem:\\nI have the following dataframe:\\nind...   \n",
       "461  465  xlangai/DS-1000  Problem:\\nI am aware there are many questions ...   \n",
       "463  467  xlangai/DS-1000  Problem:\\nI have multi-index df as follows\\n\\n...   \n",
       "467  472  xlangai/DS-1000  Problem:\\nI have a data frame like below \\n   ...   \n",
       "468  473  xlangai/DS-1000  Problem:\\nI have a data frame like below \\n   ...   \n",
       "478  483  xlangai/DS-1000  Problem:\\nSay I have two dataframes:\\ndf1:    ...   \n",
       "479  484  xlangai/DS-1000  Problem:\\nSay I have two dataframes:\\ndf1:    ...   \n",
       "486  492  xlangai/DS-1000  Problem:\\nI am trying to extract rows from a P...   \n",
       "495  503  xlangai/DS-1000  Problem:\\nI have a pandas Dataframe like below...   \n",
       "500  508  xlangai/DS-1000  Problem:\\nI am trying to find col duplicates r...   \n",
       "506  514  xlangai/DS-1000  Problem:\\nI am performing a query on a DataFra...   \n",
       "507  515  xlangai/DS-1000  Problem:\\nI am performing a query on a DataFra...   \n",
       "508  516  xlangai/DS-1000  Problem:\\nI have a Pandas DataFrame that looks...   \n",
       "509  517  xlangai/DS-1000  Problem:\\nI have a Pandas DataFrame that looks...   \n",
       "516  524  xlangai/DS-1000  Problem:\\nExample\\nimport pandas as pd\\nimport...   \n",
       "517  525  xlangai/DS-1000  Problem:\\nExample\\nimport pandas as pd\\nimport...   \n",
       "518  526  xlangai/DS-1000  Problem:\\nLet's say I have 5 columns.\\npd.Data...   \n",
       "525  533  xlangai/DS-1000  Problem:\\n   Survived  SibSp  Parch\\n0        ...   \n",
       "526  534  xlangai/DS-1000  Problem:\\n   Survived  SibSp  Parch\\n0        ...   \n",
       "528  536  xlangai/DS-1000  Problem:\\nHow do I apply sort to a pandas grou...   \n",
       "530  538  xlangai/DS-1000  Problem:\\nI get how to use pd.MultiIndex.from_...   \n",
       "533  542  xlangai/DS-1000  Problem:\\nHaving a pandas data frame as follow...   \n",
       "536  545  xlangai/DS-1000  Problem:\\nI have a dataFrame with rows and col...   \n",
       "550  560  xlangai/DS-1000  Problem:\\nI am trying to groupby counts of dat...   \n",
       "552  562  xlangai/DS-1000  Problem:\\nI am trying to groupby counts of dat...   \n",
       "554  564  xlangai/DS-1000  Problem:\\nI have a dataframe, e.g:\\nDate      ...   \n",
       "555  565  xlangai/DS-1000  Problem:\\nWas trying to generate a pivot table...   \n",
       "556  566  xlangai/DS-1000  Problem:\\nI have a dataframe:\\n\\n\\ndf = pd.Dat...   \n",
       "560  570  xlangai/DS-1000  Problem:\\nWhat is an efficient way of splittin...   \n",
       "567  577  xlangai/DS-1000  Problem:\\nI have a Dataframe as below.\\nName  ...   \n",
       "578  588  xlangai/DS-1000  Problem:\\nI have the following dataframe:\\n  k...   \n",
       "579  589  xlangai/DS-1000  Problem:\\nI have the following dataframe:\\n  k...   \n",
       "\n",
       "                                                  code  \\\n",
       "375  def g(df, List):\\n    return df.iloc[List]\\n\\n...   \n",
       "376  def g(df, List):\\n    df2 = df.iloc[List].rein...   \n",
       "377  def g(df):\\n    return df.where(df.apply(lambd...   \n",
       "382  def g(df):\\n    return df.loc[(df['keep_if_dup...   \n",
       "383  def g(df):\\n    return df.loc[(df['drop_if_dup...   \n",
       "388  df['datetime'] = df['datetime'].dt.tz_localize...   \n",
       "389  def g(df):\\n    df['datetime'] = df['datetime'...   \n",
       "400  df = df[df['Date'] >= List[0]]\\ndf = df[df['Da...   \n",
       "404  import numpy as np\\ndef g(df):\\n    sh = 0\\n  ...   \n",
       "405  def g(df):\\n    return df.add_suffix('X')\\n\\nd...   \n",
       "407  def g(df):\\n    for col in df.columns:\\n      ...   \n",
       "409  def g(df):\\n    return df.groupby('group').agg...   \n",
       "410  def g(df):\\n    return df.groupby('group').agg...   \n",
       "412  def g(df, row_list, column_list):\\n    result ...   \n",
       "413  def g(df):\\n    return df.apply(lambda x: x.va...   \n",
       "415  def g(df):\\n    s = ''\\n    for c in df.column...   \n",
       "430  def g(df):\\n    df.dt = pd.to_datetime(df.dt)\\...   \n",
       "432  def g(df):\\n    df.dt = pd.to_datetime(df.dt)\\...   \n",
       "433  def g(df):\\n    df.dt = pd.to_datetime(df.dt)\\...   \n",
       "434  def g(df):\\n    df.dt = pd.to_datetime(df.dt)\\...   \n",
       "441              result = df.loc[df['c']>0.45,columns]   \n",
       "453  def g(df):\\n    df['A'].replace(to_replace=0, ...   \n",
       "454  def g(df):\\n    df['A'].replace(to_replace=0, ...   \n",
       "461  def g(df1, df2, columns_check_list):\\n    mask...   \n",
       "463  def g(df):\\n    df.index = df.index.set_levels...   \n",
       "467  def g(df):\\n    mask = (df.filter(like='Value'...   \n",
       "468  def g(df):\\n    mask = (df.filter(like='Value'...   \n",
       "478  def g(df1, df2):\\n    return pd.merge_asof(df2...   \n",
       "479  def g(df1, df2):\\n    return pd.merge_asof(df1...   \n",
       "486  def g(df, test):\\n    return df.loc[test]\\n\\nr...   \n",
       "495  def g(df):\\n    l = int(0.2 * len(df))\\n    df...   \n",
       "500  def g(df):\\n    cols = list(df.filter(like='co...   \n",
       "506  def g(df, filter_list):\\n    return df.query(\"...   \n",
       "507  def g(df, filter_list):\\n    return df.query(\"...   \n",
       "508  def g(df):\\n    return pd.melt(df)\\n\\nresult =...   \n",
       "509  def g(df):\\n    result = pd.melt(df, value_var...   \n",
       "516  def g(df):\\n    return df.groupby('r')['v'].ap...   \n",
       "517  def g(df):\\n    return df.groupby('l')['v'].ap...   \n",
       "518  def get_relation(df, col1, col2):\\n    first_m...   \n",
       "525  import numpy as np\\ndef g(df):\\n    family = n...   \n",
       "526  def g(df):\\n    family = []\\n    for i in rang...   \n",
       "528  def g(df):\\n    return df.groupby('cokey').app...   \n",
       "530  def g(df):\\n    df.columns = pd.MultiIndex.fro...   \n",
       "533  import numpy as np\\ndef g(df):\\n    return df....   \n",
       "536  def g(df):\\n    return df.loc[(df.sum(axis=1) ...   \n",
       "550  def g(df):\\n    df['Date'] = pd.to_datetime(df...   \n",
       "552  def g(df):\\n    df['Date'] = pd.to_datetime(df...   \n",
       "554  def g(df):\\n    df1 = df.groupby('Date').agg(l...   \n",
       "555  def g(df):\\n    return pd.pivot_table(df, valu...   \n",
       "556  def g(df):\\n    return pd.pivot_table(df, valu...   \n",
       "560  def g(df):\\n    return df.join(pd.DataFrame(df...   \n",
       "567  def g(df):\\n    cols = list(df)[1:]\\n    for i...   \n",
       "578  def g(df):\\n    return df.groupby('key1')['key...   \n",
       "579  def g(df):\\n    return df.groupby('key1')['key...   \n",
       "\n",
       "                                                  test  \\\n",
       "375  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "376  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "377  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "382  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "383  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "388  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "389  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "400  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "404  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "405  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "407  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "409  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "410  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "412  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "413  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "415  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "430  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "432  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "433  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "434  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "441  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "453  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "454  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "461  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "463  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "467  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "468  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "478  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "479  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "486  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "495  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "500  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "506  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "507  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "508  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "509  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "516  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "517  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "518  import pandas as pd\\nimport numpy as np\\nfrom ...   \n",
       "525  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "526  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "528  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "530  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "533  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "536  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "550  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "552  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "554  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "555  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "556  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "560  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "567  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "578  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "579  import pandas as pd\\nimport numpy as np\\nimpor...   \n",
       "\n",
       "                                   relevant_categories  \\\n",
       "375  ['Code Structure and Modularity', 'Input and O...   \n",
       "376  ['Code Structure and Modularity', 'Input and O...   \n",
       "377  ['Data Processing and Transformation', 'Librar...   \n",
       "382  ['Data Processing and Transformation', 'Input ...   \n",
       "383  ['Data Processing and Transformation', 'Input ...   \n",
       "388  ['Code Structure and Modularity', 'Input and O...   \n",
       "389  ['Code Structure and Modularity', 'Input and O...   \n",
       "400  ['Code Structure and Modularity', 'Input and O...   \n",
       "404  ['Code Structure and Modularity', 'Input and O...   \n",
       "405  ['Code Structure and Modularity', 'Data Proces...   \n",
       "407  ['Code Structure and Modularity', 'Input and O...   \n",
       "409  ['Code Structure and Modularity', 'Input and O...   \n",
       "410  ['Code Structure and Modularity', 'Input and O...   \n",
       "412  ['Code Structure and Modularity', 'Input and O...   \n",
       "413  ['Data Processing and Transformation', 'Input ...   \n",
       "415  ['Code Structure and Modularity', 'Input and O...   \n",
       "430  ['Data Processing and Transformation', 'Librar...   \n",
       "432  ['Code Structure and Modularity', 'Input and O...   \n",
       "433  ['Code Structure and Modularity', 'Input and O...   \n",
       "434  ['Code Structure and Modularity', 'Input and O...   \n",
       "441  ['Code Structure and Modularity', 'Input and O...   \n",
       "453  ['Code Structure and Modularity', 'Input and O...   \n",
       "454  ['Code Structure and Modularity', 'Input and O...   \n",
       "461  ['Code Structure and Modularity', 'Input and O...   \n",
       "463  ['Code Structure and Modularity', 'Input and O...   \n",
       "467  ['Code Structure and Modularity', 'Input and O...   \n",
       "468  ['Code Structure and Modularity', 'Input and O...   \n",
       "478  ['Code Structure and Modularity', 'Input and O...   \n",
       "479  ['Code Structure and Modularity', 'Input and O...   \n",
       "486  ['Code Structure and Modularity', 'Input and O...   \n",
       "495  ['Code Structure and Modularity', 'Input and O...   \n",
       "500  ['Code Structure and Modularity', 'Input and O...   \n",
       "506  ['Code Structure and Modularity', 'Input and O...   \n",
       "507  ['Code Structure and Modularity', 'Input and O...   \n",
       "508  ['Code Structure and Modularity', 'Input and O...   \n",
       "509  ['Code Structure and Modularity', 'Input and O...   \n",
       "516  ['Code Structure and Modularity', 'Input and O...   \n",
       "517  ['Code Structure and Modularity', 'Input and O...   \n",
       "518  ['Code Structure and Modularity', 'Input and O...   \n",
       "525  ['Data Processing and Transformation', 'Librar...   \n",
       "526  ['Code Structure and Modularity', 'Data Proces...   \n",
       "528  ['Code Structure and Modularity', 'Input and O...   \n",
       "530  ['Code Structure and Modularity', 'Input and O...   \n",
       "533  ['Code Structure and Modularity', 'Input and O...   \n",
       "536  ['Data Processing and Transformation', 'Input ...   \n",
       "550  ['Code Structure and Modularity', 'Input and O...   \n",
       "552  ['Code Structure and Modularity', 'Input and O...   \n",
       "554  ['Code Structure and Modularity', 'Input and O...   \n",
       "555  ['Code Structure and Modularity', 'Input and O...   \n",
       "556  ['Code Structure and Modularity', 'Input and O...   \n",
       "560  ['Data Processing and Transformation', 'Perfor...   \n",
       "567  ['Code Structure and Modularity', 'Input and O...   \n",
       "578  ['Data Processing and Transformation', 'Librar...   \n",
       "579  ['Data Processing and Transformation', 'Librar...   \n",
       "\n",
       "                                simplified_instruction  \\\n",
       "375  Problem:\\nI have the following DataFrame:\\n   ...   \n",
       "376  Problem:\\nI have the following DataFrame:\\n   ...   \n",
       "377  Problem:\\nI have following pandas dataframe :\\...   \n",
       "382  Problem:\\nI have a dataset :\\nid    url     ke...   \n",
       "383  Problem:\\nI have a dataset :\\nid    url     dr...   \n",
       "388  Problem: I have been struggling with removing ...   \n",
       "389  Problem: I have been struggling with removing ...   \n",
       "400  Problem:\\nI have the following DF\\n\\tDate\\n0  ...   \n",
       "404  Problem:\\nSo I have a dataframe that looks lik...   \n",
       "405  Problem:\\nConsidering a simple df:\\nHeaderA | ...   \n",
       "407  Problem:\\nConsidering a simple df:\\nHeaderA | ...   \n",
       "409  Problem:\\nI have a script that generates a pan...   \n",
       "410  Problem: I have a script that generates a pand...   \n",
       "412  Problem:\\nI have pandas df with say, 100 rows,...   \n",
       "413  Problem:\\nI have a dataframe with numerous col...   \n",
       "415  Problem:\\nI have a dataframe with numerous col...   \n",
       "430  Problem: I've a data frame that looks like the...   \n",
       "432  Problem: I've a data frame that looks like the...   \n",
       "433  Problem: I've a data frame that looks like the...   \n",
       "434  Problem: I've a data frame that looks like the...   \n",
       "441  I'm wondering if there is a simpler, memory ef...   \n",
       "453  Problem:\\nI have the following dataframe:\\nind...   \n",
       "454  Problem:\\nI have the following dataframe:\\nind...   \n",
       "461  Problem:\\nI am aware there are many questions ...   \n",
       "463  Problem:\\nI have multi-index df as follows\\n\\n...   \n",
       "467  Problem:\\nI have a data frame like below \\n   ...   \n",
       "468  Problem:\\nI have a data frame like below \\n   ...   \n",
       "478  Problem:\\nSay I have two dataframes:\\ndf1:    ...   \n",
       "479  Problem:\\nSay I have two dataframes:\\ndf1:    ...   \n",
       "486  Problem: I am trying to extract rows from a Pa...   \n",
       "495  Problem:\\nI have a pandas Dataframe like below...   \n",
       "500  Problem:\\nI am trying to find col duplicates r...   \n",
       "506  Problem:\\nI am performing a query on a DataFra...   \n",
       "507  Problem:\\nI am performing a query on a DataFra...   \n",
       "508  Problem:\\nI have a Pandas DataFrame that looks...   \n",
       "509  Problem:\\nI have a Pandas DataFrame that looks...   \n",
       "516  Problem\\nWhen a grouped dataframe contains a v...   \n",
       "517  Problem\\nWhen a grouped dataframe contains a v...   \n",
       "518  Problem:\\nLet's say I have 5 columns.\\npd.Data...   \n",
       "525  Given the above dataframe, is there an elegant...   \n",
       "526  Given the above dataframe, is there an elegant...   \n",
       "528  Problem: How do I apply sort to a pandas group...   \n",
       "530  Problem:\\nI get how to use pd.MultiIndex.from_...   \n",
       "533  Problem:\\nHaving a pandas data frame as follow...   \n",
       "536  Problem:\\nI have a dataFrame with rows and col...   \n",
       "550  Problem:\\nI am trying to groupby counts of dat...   \n",
       "552  Problem:\\nI am trying to groupby counts of dat...   \n",
       "554  Problem:\\nI have a dataframe, e.g:\\nDate      ...   \n",
       "555  Problem: Was trying to generate a pivot table ...   \n",
       "556  Problem:\\nI have a dataframe:\\n\\n\\ndf = pd.Dat...   \n",
       "560  Problem:\\nWhat is an efficient way of splittin...   \n",
       "567  Problem:\\nI have a Dataframe as below.\\nName  ...   \n",
       "578  Problem:\\nI have the following dataframe:\\n  k...   \n",
       "579  Problem:\\nI have the following dataframe:\\n  k...   \n",
       "\n",
       "                                 extracted_constraints  \\\n",
       "375                                                 []   \n",
       "376  [{'type': 'Data Processing and Transformation'...   \n",
       "377  [{'type': 'Data Processing and Transformation'...   \n",
       "382  [{'type': 'Data Processing and Transformation'...   \n",
       "383  [{'type': 'Data Processing and Transformation'...   \n",
       "388  [{'type': 'Data Processing and Transformation'...   \n",
       "389  [{'type': 'Data Processing and Transformation'...   \n",
       "400  [{'type': 'Data Processing and Transformation'...   \n",
       "404  [{'type': 'Data Processing and Transformation'...   \n",
       "405  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "407  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "409  [{'type': 'Data Processing and Transformation'...   \n",
       "410  [{'type': 'Data Processing and Transformation'...   \n",
       "412  [{'type': 'Data Processing and Transformation'...   \n",
       "413                                                 []   \n",
       "415  [{'type': 'Input and Output Handling', 'constr...   \n",
       "430                                                 []   \n",
       "432  [{'type': 'Data Processing and Transformation'...   \n",
       "433  [{'type': 'Data Processing and Transformation'...   \n",
       "434  [{'type': 'Data Processing and Transformation'...   \n",
       "441  [{'type': 'Data Processing and Transformation'...   \n",
       "453  [{'type': 'Data Processing and Transformation'...   \n",
       "454  [{'type': 'Data Processing and Transformation'...   \n",
       "461  [{'type': 'Data Processing and Transformation'...   \n",
       "463  [{'type': 'Data Processing and Transformation'...   \n",
       "467  [{'type': 'Data Processing and Transformation'...   \n",
       "468  [{'type': 'Data Processing and Transformation'...   \n",
       "478  [{'type': 'Performance and Optimization', 'con...   \n",
       "479                                                 []   \n",
       "486  [{'type': 'Library and API Usage', 'constraint...   \n",
       "495  [{'type': 'Data Processing and Transformation'...   \n",
       "500  [{'type': 'Data Processing and Transformation'...   \n",
       "506  [{'type': 'Error Handling and Robustness', 'co...   \n",
       "507  [{'type': 'Error Handling and Robustness', 'co...   \n",
       "508  [{'type': 'Library and API Usage', 'constraint...   \n",
       "509  [{'type': 'Data Processing and Transformation'...   \n",
       "516  [{'type': 'Data Processing and Transformation'...   \n",
       "517  [{'type': 'Data Processing and Transformation'...   \n",
       "518                                                 []   \n",
       "525  [{'type': 'Data Processing and Transformation'...   \n",
       "526  [{'type': 'Data Processing and Transformation'...   \n",
       "528  [{'type': 'Library and API Usage', 'constraint...   \n",
       "530                                                 []   \n",
       "533  [{'type': 'Data Processing and Transformation'...   \n",
       "536  [{'type': 'Data Processing and Transformation'...   \n",
       "550  [{'type': 'Data Processing and Transformation'...   \n",
       "552  [{'type': 'Data Processing and Transformation'...   \n",
       "554  [{'type': 'Data Processing and Transformation'...   \n",
       "555  [{'type': 'Library and API Usage', 'constraint...   \n",
       "556  [{'type': 'Data Processing and Transformation'...   \n",
       "560  [{'type': 'Library and API Usage', 'constraint...   \n",
       "567  [{'type': 'Data Processing and Transformation'...   \n",
       "578  [{'type': 'Data Processing and Transformation'...   \n",
       "579  [{'type': 'Data Processing and Transformation'...   \n",
       "\n",
       "                       final_comprehensive_constraints  \\\n",
       "375  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "376  [{'type': 'Data Processing and Transformation'...   \n",
       "377  [{'type': 'Data Processing and Transformation'...   \n",
       "382  [{'type': 'Data Processing and Transformation'...   \n",
       "383  [{'type': 'Data Processing and Transformation'...   \n",
       "388  [{'type': 'Data Processing and Transformation'...   \n",
       "389  [{'type': 'Data Processing and Transformation'...   \n",
       "400  [{'type': 'Data Processing and Transformation'...   \n",
       "404  [{'type': 'Data Processing and Transformation'...   \n",
       "405  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "407  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "409  [{'type': 'Data Processing and Transformation'...   \n",
       "410  [{'type': 'Data Processing and Transformation'...   \n",
       "412  [{'type': 'Data Processing and Transformation'...   \n",
       "413  [{'type': 'Data Processing and Transformation'...   \n",
       "415  [{'type': 'Input and Output Handling', 'constr...   \n",
       "430  [{'type': 'Data Processing and Transformation'...   \n",
       "432  [{'type': 'Data Processing and Transformation'...   \n",
       "433  [{'type': 'Data Processing and Transformation'...   \n",
       "434  [{'type': 'Data Processing and Transformation'...   \n",
       "441  [{'type': 'Data Processing and Transformation'...   \n",
       "453  [{'type': 'Data Processing and Transformation'...   \n",
       "454  [{'type': 'Data Processing and Transformation'...   \n",
       "461  [{'type': 'Data Processing and Transformation'...   \n",
       "463  [{'type': 'Data Processing and Transformation'...   \n",
       "467  [{'type': 'Data Processing and Transformation'...   \n",
       "468  [{'type': 'Data Processing and Transformation'...   \n",
       "478  [{'type': 'Performance and Optimization', 'con...   \n",
       "479  [{'type': 'Performance and Optimization', 'con...   \n",
       "486  [{'type': 'Library and API Usage', 'constraint...   \n",
       "495  [{'type': 'Data Processing and Transformation'...   \n",
       "500  [{'type': 'Data Processing and Transformation'...   \n",
       "506  [{'type': 'Error Handling and Robustness', 'co...   \n",
       "507  [{'type': 'Error Handling and Robustness', 'co...   \n",
       "508  [{'type': 'Library and API Usage', 'constraint...   \n",
       "509  [{'type': 'Data Processing and Transformation'...   \n",
       "516  [{'type': 'Data Processing and Transformation'...   \n",
       "517  [{'type': 'Data Processing and Transformation'...   \n",
       "518  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "525  [{'type': 'Data Processing and Transformation'...   \n",
       "526  [{'type': 'Data Processing and Transformation'...   \n",
       "528  [{'type': 'Library and API Usage', 'constraint...   \n",
       "530  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "533  [{'type': 'Data Processing and Transformation'...   \n",
       "536  [{'type': 'Data Processing and Transformation'...   \n",
       "550  [{'type': 'Data Processing and Transformation'...   \n",
       "552  [{'type': 'Data Processing and Transformation'...   \n",
       "554  [{'type': 'Data Processing and Transformation'...   \n",
       "555  [{'type': 'Library and API Usage', 'constraint...   \n",
       "556  [{'type': 'Data Processing and Transformation'...   \n",
       "560  [{'type': 'Library and API Usage', 'constraint...   \n",
       "567  [{'type': 'Data Processing and Transformation'...   \n",
       "578  [{'type': 'Data Processing and Transformation'...   \n",
       "579  [{'type': 'Data Processing and Transformation'...   \n",
       "\n",
       "                         filtered_relevant_constraints  ...  \\\n",
       "375  [{'type': 'Code Structure and Modularity', 'co...  ...   \n",
       "376  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "377  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "382  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "383  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "388  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "389  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "400  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "404  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "405  [{'type': 'Code Structure and Modularity', 'co...  ...   \n",
       "407  [{'type': 'Code Structure and Modularity', 'co...  ...   \n",
       "409  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "410  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "412  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "413  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "415  [{'type': 'Input and Output Handling', 'constr...  ...   \n",
       "430  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "432  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "433  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "434  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "441  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "453  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "454  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "461  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "463  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "467  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "468  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "478  [{'type': 'Performance and Optimization', 'con...  ...   \n",
       "479  [{'type': 'Performance and Optimization', 'con...  ...   \n",
       "486  [{'type': 'Library and API Usage', 'constraint...  ...   \n",
       "495  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "500  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "506  [{'type': 'Error Handling and Robustness', 'co...  ...   \n",
       "507  [{'type': 'Error Handling and Robustness', 'co...  ...   \n",
       "508  [{'type': 'Library and API Usage', 'constraint...  ...   \n",
       "509  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "516  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "517  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "518  [{'type': 'Input and Output Handling', 'constr...  ...   \n",
       "525  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "526  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "528  [{'type': 'Library and API Usage', 'constraint...  ...   \n",
       "530  [{'type': 'Code Structure and Modularity', 'co...  ...   \n",
       "533  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "536  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "550  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "552  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "554  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "555  [{'type': 'Library and API Usage', 'constraint...  ...   \n",
       "556  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "560  [{'type': 'Library and API Usage', 'constraint...  ...   \n",
       "567  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "578  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "579  [{'type': 'Data Processing and Transformation'...  ...   \n",
       "\n",
       "                          constraint_presence_response  \\\n",
       "375  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "376  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "377  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "382  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "383  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "388  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "389  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "400  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "404  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "405  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "407  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "409  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "410  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "412  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "413  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "415  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "430  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "432  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "433  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "434  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "441  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "453  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "454  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "461  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "463  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "467  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "468  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "478  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "479  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "486  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "495  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "500  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "506  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "507  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "508  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "509  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "516  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "517  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "518  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "525  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "526  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "528  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "530  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "533  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "536  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "550  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "552  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "554  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "555  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "556  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "560  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "567  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "578  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "579  ```json\\n{\\n  \"Evaluation\": [\\n    {\\n      \"C...   \n",
       "\n",
       "                                     final_constraints  \\\n",
       "375  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "376  [{'type': 'Data Processing and Transformation'...   \n",
       "377  [{'type': 'Data Processing and Transformation'...   \n",
       "382  [{'type': 'Data Processing and Transformation'...   \n",
       "383  [{'type': 'Data Processing and Transformation'...   \n",
       "388  [{'type': 'Data Processing and Transformation'...   \n",
       "389  [{'type': 'Data Processing and Transformation'...   \n",
       "400  [{'type': 'Data Processing and Transformation'...   \n",
       "404  [{'type': 'Data Processing and Transformation'...   \n",
       "405  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "407  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "409  [{'type': 'Data Processing and Transformation'...   \n",
       "410  [{'type': 'Data Processing and Transformation'...   \n",
       "412  [{'type': 'Data Processing and Transformation'...   \n",
       "413  [{'type': 'Data Processing and Transformation'...   \n",
       "415  [{'type': 'Input and Output Handling', 'constr...   \n",
       "430  [{'type': 'Data Processing and Transformation'...   \n",
       "432  [{'type': 'Data Processing and Transformation'...   \n",
       "433  [{'type': 'Data Processing and Transformation'...   \n",
       "434  [{'type': 'Data Processing and Transformation'...   \n",
       "441  [{'type': 'Data Processing and Transformation'...   \n",
       "453  [{'type': 'Data Processing and Transformation'...   \n",
       "454  [{'type': 'Data Processing and Transformation'...   \n",
       "461  [{'type': 'Data Processing and Transformation'...   \n",
       "463  [{'type': 'Data Processing and Transformation'...   \n",
       "467  [{'type': 'Data Processing and Transformation'...   \n",
       "468  [{'type': 'Data Processing and Transformation'...   \n",
       "478  [{'type': 'Performance and Optimization', 'con...   \n",
       "479  [{'type': 'Performance and Optimization', 'con...   \n",
       "486  [{'type': 'Library and API Usage', 'constraint...   \n",
       "495  [{'type': 'Data Processing and Transformation'...   \n",
       "500  [{'type': 'Data Processing and Transformation'...   \n",
       "506  [{'type': 'Error Handling and Robustness', 'co...   \n",
       "507  [{'type': 'Error Handling and Robustness', 'co...   \n",
       "508  [{'type': 'Library and API Usage', 'constraint...   \n",
       "509  [{'type': 'Data Processing and Transformation'...   \n",
       "516  [{'type': 'Data Processing and Transformation'...   \n",
       "517  [{'type': 'Data Processing and Transformation'...   \n",
       "518  [{'type': 'Input and Output Handling', 'constr...   \n",
       "525  [{'type': 'Data Processing and Transformation'...   \n",
       "526  [{'type': 'Data Processing and Transformation'...   \n",
       "528  [{'type': 'Library and API Usage', 'constraint...   \n",
       "530  [{'type': 'Data Processing and Transformation'...   \n",
       "533  [{'type': 'Data Processing and Transformation'...   \n",
       "536  [{'type': 'Data Processing and Transformation'...   \n",
       "550  [{'type': 'Data Processing and Transformation'...   \n",
       "552  [{'type': 'Data Processing and Transformation'...   \n",
       "554  [{'type': 'Data Processing and Transformation'...   \n",
       "555  [{'type': 'Library and API Usage', 'constraint...   \n",
       "556  [{'type': 'Data Processing and Transformation'...   \n",
       "560  [{'type': 'Library and API Usage', 'constraint...   \n",
       "567  [{'type': 'Data Processing and Transformation'...   \n",
       "578  [{'type': 'Data Processing and Transformation'...   \n",
       "579  [{'type': 'Data Processing and Transformation'...   \n",
       "\n",
       "                                              response  \\\n",
       "375  Here's a function that meets your requirements...   \n",
       "376  Here's how you can achieve this using pandas:\\...   \n",
       "377  Here's how you can achieve this using pandas:\\...   \n",
       "382  result = df.loc[df.duplicated(subset='url', ke...   \n",
       "383  result = df.loc[df.duplicated(subset='url', ke...   \n",
       "388  To achieve the desired output without manually...   \n",
       "389  To remove the time zone info from the 'datetim...   \n",
       "400  To solve this problem, we'll first filter the ...   \n",
       "404  To achieve the desired outcome, you can follow...   \n",
       "405  Here's a reusable function that efficiently ha...   \n",
       "407  Here's a solution using pandas' `rename` funct...   \n",
       "409  Here's a function that meets your requirements...   \n",
       "410  To achieve the dynamic solution that works wit...   \n",
       "412  To achieve the desired output, you can use the...   \n",
       "413  Here's a function that meets your requirements...   \n",
       "415  Here's the Python code to achieve the desired ...   \n",
       "430  To achieve the desired output, you can follow ...   \n",
       "432  To achieve the desired output, you can follow ...   \n",
       "433  Here's a function that accomplishes the desire...   \n",
       "434  Here's the solution to your problem:\\n\\n```pyt...   \n",
       "441  To achieve the desired result in a memory-effi...   \n",
       "453  Here's a function that fulfills your requireme...   \n",
       "454  To fill the zeros with the posterior non-zero ...   \n",
       "461  Here's the solution with the requested feature...   \n",
       "463  Here's the corrected code to parse the datetim...   \n",
       "467  To solve this problem, we'll first identify th...   \n",
       "468  Here's how you can achieve this using pandas:\\...   \n",
       "478  result = df2.merge_asof(df1, on='Timestamp', d...   \n",
       "479  result = pd.merge_asof(df1, df2, on='Timestamp...   \n",
       "486  To extract rows from a Pandas dataframe using ...   \n",
       "495  Here's how you can achieve this using the prov...   \n",
       "500  Here's the solution that meets all the require...   \n",
       "506  Here's a function that addresses your requirem...   \n",
       "507  Here's a reusable function that filters the Da...   \n",
       "508  To achieve a generalizable solution for meltin...   \n",
       "509  To achieve the desired generalized melt functi...   \n",
       "516  result = df.groupby('r')['v'].transform(lambda...   \n",
       "517  Here's the solution to your problem:\\n\\n```pyt...   \n",
       "518  Here's a function that meets your requirements...   \n",
       "525  Here's how you can achieve this using the Pand...   \n",
       "526  Here's how you can achieve this using pandas' ...   \n",
       "528  result = df.groupby('cokey').apply(lambda x: x...   \n",
       "530  Here's a function `g` that takes a DataFrame a...   \n",
       "533  Here is the corrected code to calculate the me...   \n",
       "536  result = df.loc[df.ne(0).all(axis=1)].loc[:, d...   \n",
       "550  Here's the corrected code to achieve the desir...   \n",
       "552  Here's the solution to achieve the desired out...   \n",
       "554  Here's how you can achieve this using the pand...   \n",
       "555  To achieve the desired result, you can use the...   \n",
       "556  result = df.pivot_table(values=['D', 'E'], ind...   \n",
       "560  To efficiently split a column into multiple ro...   \n",
       "567  Here is the solution to calculate the cumulati...   \n",
       "578  result = df.groupby('key1')['key2'].transform(...   \n",
       "579  Here's how you can achieve the desired result:...   \n",
       "\n",
       "                        constraint_adherence_responses  \\\n",
       "375  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "376  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "377  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "382  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "383  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "388  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "389  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "400  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "404  {\"Evaluation\": [{\"Constraint\": \"Shift the firs...   \n",
       "405  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "407  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "409  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "410  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "412  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "413  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "415  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "430  {\"Evaluation\": [{\"Constraint\": \"The solution m...   \n",
       "432  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "433  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "434  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "441  {\"Evaluation\": [{\"Constraint\": \"Select only th...   \n",
       "453  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "454  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "461  {\"Evaluation\": [{\"Constraint\": \"Check that all...   \n",
       "463  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "467  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "468  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "478  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "479  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "486  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "495  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "500  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "506  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "507  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "508  {\"Evaluation\": [{\"Constraint\": \"Use pd.melt to...   \n",
       "509  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "516  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "517  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "518  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "525  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "526  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "528  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "530  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "533  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "536  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "550  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "552  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "554  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "555  ```json\\n{\"Evaluation\": [\\n    {\\n        \"Con...   \n",
       "556  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "560  {\"Evaluation\": [{\"Constraint\": \"Dask does not ...   \n",
       "567  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "578  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "579  ```json\\n{\\n    \"Evaluation\": [\\n        {\\n  ...   \n",
       "\n",
       "                                  Constraint_adherence   correctness_level  \\\n",
       "375                           [True, True, True, True]  Completely Correct   \n",
       "376                         [True, False, False, True]               Wrong   \n",
       "377                 [False, False, False, True, False]               Wrong   \n",
       "382                   [True, False, False, True, True]               Wrong   \n",
       "383                   [False, True, False, True, True]               Wrong   \n",
       "388         [True, True, True, True, True, True, True]  Completely Correct   \n",
       "389                     [True, True, True, True, True]               Wrong   \n",
       "400                               [False, True, False]               Wrong   \n",
       "404                        [False, False, False, True]               Wrong   \n",
       "405  [True, True, True, True, True, True, True, False]  Completely Correct   \n",
       "407  [True, True, True, True, True, True, True, Tru...  Completely Correct   \n",
       "409                           [True, True, True, True]  Completely Correct   \n",
       "410                           [True, True, True, True]               Wrong   \n",
       "412                 [False, True, False, False, False]               Wrong   \n",
       "413                 [False, False, True, False, False]               Wrong   \n",
       "415            [True, False, True, True, False, False]               Wrong   \n",
       "430                  [True, False, True, False, False]               Wrong   \n",
       "432                          [True, True, True, False]               Wrong   \n",
       "433  [True, True, True, True, False, False, True, T...               Wrong   \n",
       "434       [True, False, True, True, False, True, True]               Wrong   \n",
       "441                     [True, True, True, True, True]  Completely Correct   \n",
       "453                   [False, False, True, True, True]               Wrong   \n",
       "454                        [False, False, True, False]               Wrong   \n",
       "461  [True, True, False, True, True, True, True, Tr...   Partially Correct   \n",
       "463                    [True, True, True, True, False]               Wrong   \n",
       "467                           [True, True, True, True]  Completely Correct   \n",
       "468                           [True, True, True, True]               Wrong   \n",
       "478               [True, True, True, True, True, True]               Wrong   \n",
       "479        [True, False, True, True, True, True, True]               Wrong   \n",
       "486              [True, True, True, True, True, False]  Completely Correct   \n",
       "495                           [True, True, True, True]               Wrong   \n",
       "500                   [True, False, False, True, True]               Wrong   \n",
       "506         [True, True, True, True, True, True, True]               Wrong   \n",
       "507   [True, True, True, True, True, True, True, True]               Wrong   \n",
       "508                     [True, True, True, True, True]  Completely Correct   \n",
       "509                     [True, True, True, True, True]               Wrong   \n",
       "516          [False, False, False, True, False, False]               Wrong   \n",
       "517        [True, True, True, True, True, True, False]               Wrong   \n",
       "518                         [True, False, True, False]               Wrong   \n",
       "525                        [False, False, False, True]               Wrong   \n",
       "526           [False, False, False, False, True, True]               Wrong   \n",
       "528                          [True, True, False, True]   Partially Correct   \n",
       "530           [True, False, False, True, False, False]               Wrong   \n",
       "533                 [False, True, False, False, False]               Wrong   \n",
       "536                    [True, False, True, True, True]  Completely Correct   \n",
       "550  [False, False, True, True, True, False, False,...               Wrong   \n",
       "552  [False, False, True, True, False, False, False...               Wrong   \n",
       "554      [True, True, False, True, False, False, True]               Wrong   \n",
       "555                           [True, True, True, True]  Completely Correct   \n",
       "556               [True, True, True, True, True, True]  Completely Correct   \n",
       "560  [True, True, True, True, True, True, True, False]               Wrong   \n",
       "567                        [False, False, True, False]               Wrong   \n",
       "578                    [True, False, True, True, True]               Wrong   \n",
       "579               [True, True, True, True, True, True]               Wrong   \n",
       "\n",
       "                                    correctness_reason  \\\n",
       "375  The code is free of syntax errors, logically m...   \n",
       "376  The code has syntax errors and does not correc...   \n",
       "377  The code contains a syntax error due to incorr...   \n",
       "382  The code does not correctly implement the logi...   \n",
       "383  The code does not correctly implement the logi...   \n",
       "388  The code correctly converts the datetime colum...   \n",
       "389  The code has no syntax errors, but it incorrec...   \n",
       "400  The code has syntax errors and does not satisf...   \n",
       "404  The code provided does not correctly implement...   \n",
       "405  The code correctly renames all columns by appe...   \n",
       "407  The code correctly renames columns that do not...   \n",
       "409  The code correctly identifies columns starting...   \n",
       "410  The code has syntax errors because the aggrega...   \n",
       "412  The code contains syntax errors and does not l...   \n",
       "413  The code has syntax errors and does not produc...   \n",
       "415  The code contains syntax errors and does not c...   \n",
       "430  The code has syntax errors and does not correc...   \n",
       "432  The code has syntax errors and does not produc...   \n",
       "433  The code has a syntax error due to incorrect i...   \n",
       "434  The code has syntax errors and does not correc...   \n",
       "441  The code is free of syntax errors, logically s...   \n",
       "453  The code has syntax errors due to incorrect fo...   \n",
       "454  The code uses ffill() which fills with the pre...   \n",
       "461  The code has no syntax errors, but it does not...   \n",
       "463  The code attempts to parse the datetime index ...   \n",
       "467  The code correctly identifies columns prefixed...   \n",
       "468  The code has a syntax error due to the incorre...   \n",
       "478  The code contains a syntax error due to incorr...   \n",
       "479  The code contains syntax errors due to incorre...   \n",
       "486  The code correctly sets the index of the dataf...   \n",
       "495  The code has syntax errors due to incorrect fo...   \n",
       "500  The code has no syntax errors, but it does not...   \n",
       "506  The code has syntax errors and does not satisf...   \n",
       "507  The code has a syntax error in the condition c...   \n",
       "508  The code correctly defines a function to gener...   \n",
       "509  The code provided does not correctly implement...   \n",
       "516  The code contains a syntax error due to incorr...   \n",
       "517  The code has syntax errors due to incorrect fo...   \n",
       "518  The provided code does not correctly implement...   \n",
       "525  The code does not correctly implement the grou...   \n",
       "526  The code contains syntax errors and does not c...   \n",
       "528  The code has no syntax errors and logically so...   \n",
       "530  The code has syntax errors and does not correc...   \n",
       "533  The code contains syntax errors and does not p...   \n",
       "536  The code correctly filters out rows and column...   \n",
       "550  The code provided does not correctly implement...   \n",
       "552  The code contains syntax errors and does not c...   \n",
       "554  The code has syntax errors due to incorrect us...   \n",
       "555  The code correctly uses groupby and apply to a...   \n",
       "556  The code correctly uses the pivot_table functi...   \n",
       "560  The code has syntax errors and does not correc...   \n",
       "567  The code contains syntax errors and does not c...   \n",
       "578  The code has syntax errors and does not correc...   \n",
       "579  The code has a syntax error due to incorrect u...   \n",
       "\n",
       "                             code_correctness_response CSR_per_row SSR_per_row  \n",
       "375  {\"reason\": \"The code is free of syntax errors,...           1    1.000000  \n",
       "376  {\"reason\": \"The code has syntax errors and doe...           0    0.500000  \n",
       "377  {\"reason\": \"The code contains a syntax error d...           0    0.200000  \n",
       "382  {\"reason\": \"The code does not correctly implem...           0    0.600000  \n",
       "383  {\"reason\": \"The code does not correctly implem...           0    0.600000  \n",
       "388  {\"reason\": \"The code correctly converts the da...           1    1.000000  \n",
       "389  ```json\\n{\\n  \"reason\": \"The code has no synta...           1    1.000000  \n",
       "400  {\"reason\": \"The code has syntax errors and doe...           0    0.333333  \n",
       "404  {\"reason\": \"The code provided does not correct...           0    0.250000  \n",
       "405  ```json\\n{\\n  \"reason\": \"The code correctly re...           0    0.875000  \n",
       "407  ```json\\n{\\n  \"reason\": \"The code correctly re...           1    1.000000  \n",
       "409  ```json\\n{\\n  \"reason\": \"The code correctly id...           1    1.000000  \n",
       "410  ```json\\n{\\n  \"reason\": \"The code has syntax e...           1    1.000000  \n",
       "412  {\"reason\": \"The code contains syntax errors an...           0    0.200000  \n",
       "413  {\"reason\": \"The code has syntax errors and doe...           0    0.200000  \n",
       "415  {\"reason\": \"The code contains syntax errors an...           0    0.500000  \n",
       "430  {\"reason\": \"The code has syntax errors and doe...           0    0.400000  \n",
       "432  {\"reason\": \"The code has syntax errors and doe...           0    0.750000  \n",
       "433  {\"reason\": \"The code has a syntax error due to...           0    0.750000  \n",
       "434  {\"reason\": \"The code has syntax errors and doe...           0    0.714286  \n",
       "441  ```json\\n{\\n  \"reason\": \"The code is free of s...           1    1.000000  \n",
       "453  ```json\\n{\\n  \"reason\": \"The code has syntax e...           0    0.600000  \n",
       "454  ```json\\n{\\n  \"reason\": \"The code uses ffill()...           0    0.250000  \n",
       "461  {\"reason\": \"The code has no syntax errors, but...           0    0.888889  \n",
       "463  {\"reason\": \"The code attempts to parse the dat...           0    0.800000  \n",
       "467  {\"reason\": \"The code correctly identifies colu...           1    1.000000  \n",
       "468  {\"reason\": \"The code has a syntax error due to...           1    1.000000  \n",
       "478  {\"reason\": \"The code contains a syntax error d...           1    1.000000  \n",
       "479  ```json\\n{\\n  \"reason\": \"The code contains syn...           0    0.857143  \n",
       "486  {\"reason\": \"The code correctly sets the index ...           0    0.833333  \n",
       "495  {\"reason\": \"The code has syntax errors due to ...           1    1.000000  \n",
       "500  ```json\\n{\\n  \"reason\": \"The code has no synta...           0    0.600000  \n",
       "506  ```json\\n{\\n  \"reason\": \"The code has syntax e...           1    1.000000  \n",
       "507  {\"reason\": \"The code has a syntax error in the...           1    1.000000  \n",
       "508  {\"reason\": \"The code correctly defines a funct...           1    1.000000  \n",
       "509  ```json\\n{\\n  \"reason\": \"The code provided doe...           1    1.000000  \n",
       "516  ```json\\n{\\n  \"reason\": \"The code contains a s...           0    0.166667  \n",
       "517  ```json\\n{\\n  \"reason\": \"The code has syntax e...           0    0.857143  \n",
       "518  {\"reason\": \"The provided code does not correct...           0    0.500000  \n",
       "525  ```json\\n{\\n  \"reason\": \"The code does not cor...           0    0.250000  \n",
       "526  ```json\\n{\\n  \"reason\": \"The code contains syn...           0    0.333333  \n",
       "528  {\"reason\": \"The code has no syntax errors and ...           0    0.750000  \n",
       "530  ```json\\n{\\n  \"reason\": \"The code has syntax e...           0    0.333333  \n",
       "533  {\"reason\": \"The code contains syntax errors an...           0    0.200000  \n",
       "536  ```json\\n{\\n  \"reason\": \"The code correctly fi...           0    0.800000  \n",
       "550  {\"reason\": \"The code provided does not correct...           0    0.375000  \n",
       "552  {\"reason\": \"The code contains syntax errors an...           0    0.200000  \n",
       "554  {\"reason\": \"The code has syntax errors due to ...           0    0.571429  \n",
       "555  ```json\\n{\\n  \"reason\": \"The code correctly us...           1    1.000000  \n",
       "556  {\"reason\": \"The code correctly uses the pivot_...           1    1.000000  \n",
       "560  {\"reason\": \"The code has syntax errors and doe...           0    0.875000  \n",
       "567  ```json\\n{\\n  \"reason\": \"The code contains syn...           0    0.250000  \n",
       "578  {\"reason\": \"The code has syntax errors and doe...           0    0.800000  \n",
       "579  ```json\\n{\\n  \"reason\": \"The code has a syntax...           1    1.000000  \n",
       "\n",
       "[54 rows x 27 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_identification_2 = re.compile(\n",
    "    r'BEGIN\\s+SOLUTION'         # BEGIN SOLUTION\n",
    "    r'[\\s\\S]*?'                 # anything, non-greedy\n",
    "    r'<code>'                   # the opening code tag\n",
    "    r'[\\s\\S]*?'                 # anything, non-greedy\n",
    "    r'\"Conditions\"\\s*:\\s*\\['    # start of the Conditions array\n",
    "    r'[\\s\\S]*?'                 # contents of the array\n",
    "    r'\\]'                       # closing bracket\n",
    "    r'[\\s\\S]*?</code>',         # up through the closing code tag\n",
    "    re.IGNORECASE\n",
    ")\n",
    "type2 = granite_2b_dfs[\"ds_1000\"][granite_2b_dfs[\"ds_1000\"].combined_instruction.str.contains(pattern_identification_2)]\n",
    "type2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d312b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>instruction</th>\n",
       "      <th>code</th>\n",
       "      <th>test</th>\n",
       "      <th>relevant_categories</th>\n",
       "      <th>simplified_instruction</th>\n",
       "      <th>extracted_constraints</th>\n",
       "      <th>final_comprehensive_constraints</th>\n",
       "      <th>filtered_relevant_constraints</th>\n",
       "      <th>...</th>\n",
       "      <th>constraint_presence_response</th>\n",
       "      <th>final_constraints</th>\n",
       "      <th>response</th>\n",
       "      <th>constraint_adherence_responses</th>\n",
       "      <th>Constraint_adherence</th>\n",
       "      <th>correctness_level</th>\n",
       "      <th>correctness_reason</th>\n",
       "      <th>code_correctness_response</th>\n",
       "      <th>CSR_per_row</th>\n",
       "      <th>SSR_per_row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, dataset, instruction, code, test, relevant_categories, simplified_instruction, extracted_constraints, final_comprehensive_constraints, filtered_relevant_constraints, quality_scores, relevance_score, objectivity_score, atomicity_score, unified_quality_score, combined_instruction, constraint_wise_presence, constraint_presence_response, final_constraints, response, constraint_adherence_responses, Constraint_adherence, correctness_level, correctness_reason, code_correctness_response, CSR_per_row, SSR_per_row]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 27 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_identification_3 = re.compile(\n",
    "    r'BEGIN\\s+SOLUTION'       # the header\n",
    "    r'[\\s\\S]*?'                     # any text, non-greedy\n",
    "    r'\"Conditions\"\\s*:\\s*\\[\\s*\\{'   # the start of the JSON‐list\n",
    "    r'[\\s\\S]*?'                     # anything inside\n",
    "    r'\\}\\s*\\]\\s}',                     # closing brace and bracket\n",
    "    re.IGNORECASE                   # if you want case‐insensitive match\n",
    ")\n",
    "type3 = granite_2b_dfs[\"ds_1000\"][granite_2b_dfs[\"ds_1000\"].combined_instruction.str.contains(pattern_identification_3)]\n",
    "type3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe65592",
   "metadata": {},
   "source": [
    "# V8 Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c6afc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 24 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      " 19  response                         1489 non-null   object \n",
      " 20  constraint_adherence_responses   1489 non-null   object \n",
      " 21  Constraint_adherence             1489 non-null   object \n",
      " 22  correctness_level                1489 non-null   object \n",
      " 23  code_correctness_response        1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(19)\n",
      "memory usage: 279.3+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_json(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/LLMjudge_outputs/code_correctness/deepseek-coder-33b-instruct_results_correctness.jsonl\",lines=True)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca32a08",
   "metadata": {},
   "source": [
    "# Prompt for constarints easy and hard classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a31042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint_difficulty_classification(instruction, constraints):\n",
    "    prompt = f\"\"\" \n",
    "    You are an assistant that classifies requirement-constraints into “easy” vs. “hard” based on their implementation complexity, performance sensitivity, and testing burden. \n",
    "\n",
    "• Easy constraints involve only single-step logic, simple built-in calls, or trivial checks (O(1) or O(N) over very small N), have minimal performance impact, and require almost no special testing.  \n",
    "• Hard constraints include anything that goes beyond trivial checks: multi-stage algorithms, strict resource bounds (e.g. O(1) space, custom prime testing), unusual data types or math, high performance or security sensitivity, or require extensive edge-case testing.  \n",
    "\n",
    "JSON Response Format:\n",
    "{{\n",
    "  \"Classification\": [\n",
    "    {{\n",
    "      \"Constraint\": \"<constraint text> (just the text inside quotes, no escape characters)\",\n",
    "      \"Reason\": \"<brief explanation>\",\n",
    "      \"label\": \"hard\" | \"easy\"\n",
    "    }},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Strictly adhere to valid JSON. \n",
    "[Instruction]:\n",
    "{instruction}\n",
    "\n",
    "[Constraints]:\n",
    "{constraints}\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "205fea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_values(text):\n",
    "    \"\"\"\n",
    "    Extracts all occurrences of the \"label\" field that are either \"Easy\" or \"Hard\"\n",
    "    from a JSON-like string block. Returns a list of label strings.\n",
    "    \"\"\"\n",
    "    # Only match labels \"Easy\" or \"Hard\"\n",
    "    matches = re.findall(r'\"label\"\\s*:\\s*\"(easy|hard)\"', text, re.IGNORECASE)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e59f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32f37902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create an array of length N (where N is a positive integer) containing numbers divisible by M (where M is a positive integer) up to X (where X is a positive integer). Each number in the array must be unique and in ascending order, as required. Additionally, the sum of all the numbers in the array should be a prime number, which must be computed without using any built-in functions or libraries to check for prime numbers. The solution should be optimized to find the largest prime sum within the given constraints. Furthermore, the time complexity of the solution should be O(N) and the space complexity should be O(1). The solution should handle cases where no prime number can be obtained as the sum of the array elements; in such cases, the solution should return an empty array. It should also handle cases where multiple prime numbers can be obtained as the sum of the array elements, returning the array with the largest sum that is prime. Additionally, the solution should minimize the number of iterations when checking for prime sums by using an efficient algorithm.\n",
      "[{'type': 'Data Processing and Transformation', 'constraint': 'Each number in the array must be unique and in ascending order.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Mathematical Computation', 'constraint': 'The sum of all the numbers in the array should be a prime number.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Performance and Optimization', 'constraint': 'The time complexity of the solution should be O(N).', 'instruction_part': 'Extracted from instruction'}, {'type': 'Performance and Optimization', 'constraint': 'The space complexity of the solution should be O(1).', 'instruction_part': 'Extracted from instruction'}, {'type': 'Mathematical Computation', 'constraint': 'The solution should be implemented without using any built-in functions or libraries to check for prime numbers.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Error Handling and Robustness', 'constraint': 'The solution should handle cases where no prime number can be obtained as the sum of the array elements. In such cases, the solution should return an empty array.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Error Handling and Robustness', 'constraint': 'The solution should also handle cases where multiple prime numbers can be obtained as the sum of the array elements. In such cases, the solution should return the array with the largest sum that is prime.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Performance and Optimization', 'constraint': 'The solution should be optimized to find the largest prime sum within the given constraints.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Performance and Optimization', 'constraint': 'The solution should minimize the number of iterations when checking for prime sums by using an efficient algorithm.', 'instruction_part': 'Newly Generated'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:\n",
      " ```json\n",
      "{\n",
      "  \"Classification\": [\n",
      "    {\n",
      "      \"Constraint\": \"Each number in the array must be unique and in ascending order.\",\n",
      "      \"Reason\": \"This involves simple checks and ordering, which can be done in a single pass with straightforward logic and minimal complexity.\",\n",
      "      \"label\": \"Easy\"\n",
      "    },\n",
      "    {\n",
      "      \"Constraint\": \"The sum of all the numbers in the array should be a prime number.\",\n",
      "      \"Reason\": \"Checking primality, especially without built-in functions, requires implementing a prime test algorithm which can be non-trivial depending on input size and optimization needs.\",\n",
      "      \"label\": \"Hard\"\n",
      "    },\n",
      "    {\n",
      "      \"Constraint\": \"The time complexity of the solution should be O(N).\",\n",
      "      \"Reason\": \"Ensuring O(N) time complexity requires careful algorithm design and optimization, especially when combined with prime checking and sum calculations.\",\n",
      "      \"label\": \"Hard\"\n",
      "    },\n",
      "    {\n",
      "      \"Constraint\": \"The space complexity of the solution should be O(1).\",\n",
      "      \"Reason\": \"Maintaining constant space while processing arrays and performing prime checks adds complexity, as it restricts auxiliary data structures and caching.\",\n",
      "      \"label\": \"Hard\"\n",
      "    },\n",
      "    {\n",
      "      \"Constraint\": \"The solution should be implemented without using any built-in functions or libraries to check for prime numbers.\",\n",
      "      \"Reason\": \"Implementing a custom prime checking algorithm without built-ins increases complexity and testing burden, especially for correctness and performance.\",\n",
      "      \"label\": \"Hard\"\n",
      "    },\n",
      "    {\n",
      "      \"Constraint\": \"The solution should handle cases where no prime number can be obtained as the sum of the array elements. In such cases, the solution should return an empty array.\",\n",
      "      \"Reason\": \"Handling edge cases and ensuring correct fallback behavior is straightforward and involves simple conditional logic.\",\n",
      "      \"label\": \"Easy\"\n",
      "    },\n",
      "    {\n",
      "      \"Constraint\": \"The solution should also handle cases where multiple prime numbers can be obtained as the sum of the array elements. In such cases, the solution should return the array with the largest sum that is prime.\",\n",
      "      \"Reason\": \"Selecting the largest prime sum among multiple candidates requires additional logic and careful iteration, increasing complexity.\",\n",
      "      \"label\": \"Hard\"\n",
      "    },\n",
      "    {\n",
      "      \"Constraint\": \"The solution should be optimized to find the largest prime sum within the given constraints.\",\n",
      "      \"Reason\": \"Optimization to find the largest prime sum involves advanced algorithmic strategies and performance considerations.\",\n",
      "      \"label\": \"Hard\"\n",
      "    },\n",
      "    {\n",
      "      \"Constraint\": \"The solution should minimize the number of iterations when checking for prime sums by using an efficient algorithm.\",\n",
      "      \"Reason\": \"Minimizing iterations for prime checking demands implementing efficient algorithms and careful performance tuning.\",\n",
      "      \"label\": \"Hard\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "id =  5\n",
    "combined_instruction = df[df[\"id\"]==id][\"combined_instruction\"].values[0]\n",
    "constraints = df[df[\"id\"]==id][\"final_constraints\"].values[0]\n",
    "print(combined_instruction)\n",
    "print(constraints)\n",
    "prompt = constraint_difficulty_classification(combined_instruction,constraints)\n",
    "response = client.get_model_response(user_prompt=prompt)\n",
    "print(\"Classification:\\n\",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fad3e0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Easy', 'Hard', 'Hard', 'Hard', 'Hard', 'Easy', 'Hard', 'Hard', 'Hard']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = extract_label_values(response)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25a864",
   "metadata": {},
   "source": [
    "# Filtering the Benchmark_v9 dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7ffd896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_difficulty_from_block(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the value of \"Difficulty\" from a JSON-like string block.\n",
    "    Returns the difficulty (e.g. \"Easy\", \"Medium\", \"Hard\") or None if not found.\n",
    "    \"\"\"\n",
    "    matches = re.findall(\n",
    "        r'\"Difficulty\"\\s*:\\s*\"(Easy|Medium|Hard|easy|medium|hard)\"',\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    return [m.lower() for m in matches][0] if matches else None\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "s = \"```json\\n{\\n  \\\"Reason\\\": \\\"The task requires detecting a cycle in a linked list and returning the node where the cycle begins, using the two-pointer technique with O(1) memory. This involves understanding Floyd's cycle detection algorithm and careful pointer manipulation, as well as handling edge cases and invalid inputs. While the algorithm is standard, it requires moderate understanding of linked lists and pointer logic.\\\",\\n  \\\"Difficulty\\\": \\\"easy\\\"\\n}\\n```\"\n",
    "print(extract_difficulty_from_block(s))  # Output: medium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4162ee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 21 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      " 19  difficulty                       1489 non-null   object \n",
      " 20  difficulty_response              1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(16)\n",
      "memory usage: 244.4+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"benchmark_dataset/benchmark_v10.jsonl\", lines=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c4c8123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"difficulty\"] = df[\"difficulty_response\"].apply(lambda x: extract_difficulty_from_block(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c79fe566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"benchmark_dataset/benchmark_v10.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "220a1d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "difficulty\n",
       "medium    952\n",
       "hard      324\n",
       "easy      161\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"difficulty\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "91bef701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 23 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               100 non-null    int64  \n",
      " 1   dataset                          100 non-null    object \n",
      " 2   instruction                      100 non-null    object \n",
      " 3   code                             100 non-null    object \n",
      " 4   test                             0 non-null      float64\n",
      " 5   relevant_categories              100 non-null    object \n",
      " 6   simplified_instruction           100 non-null    object \n",
      " 7   extracted_constraints            100 non-null    object \n",
      " 8   final_comprehensive_constraints  100 non-null    object \n",
      " 9   filtered_relevant_constraints    100 non-null    object \n",
      " 10  quality_scores                   100 non-null    object \n",
      " 11  relevance_score                  100 non-null    float64\n",
      " 12  objectivity_score                100 non-null    float64\n",
      " 13  atomicity_score                  100 non-null    float64\n",
      " 14  unified_quality_score            100 non-null    float64\n",
      " 15  combined_instruction             100 non-null    object \n",
      " 16  constraint_wise_presence         100 non-null    object \n",
      " 17  constraint_presence_response     100 non-null    object \n",
      " 18  final_constraints                100 non-null    object \n",
      " 19  difficulty                       94 non-null     object \n",
      " 20  difficulty_response              100 non-null    object \n",
      " 21  constraint_difficulty_labels     100 non-null    object \n",
      " 22  constraint_difficulty_response   100 non-null    object \n",
      "dtypes: float64(5), int64(1), object(17)\n",
      "memory usage: 18.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_2 = pd.read_json(\"benchmark_dataset/benchmark_v10_v2.jsonl\", lines=True)\n",
    "df_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "709ac4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"difficulty\"] = df[\"difficulty_response\"].apply(lambda x: extract_difficulty_from_block(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "024c0b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: count, dtype: int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"difficulty\"].isna()][\"difficulty_response\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "70072bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 21 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      " 19  difficulty                       1489 non-null   object \n",
      " 20  difficulty_response              1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(16)\n",
      "memory usage: 244.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f246af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"benchmark_dataset/benchmark_v10.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4e35a752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 21 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      " 19  difficulty                       1489 non-null   object \n",
      " 20  difficulty_response              1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(16)\n",
      "memory usage: 244.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"benchmark_dataset/benchmark_v10.jsonl\", lines=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2cb80a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1489 entries, 0 to 1488\n",
      "Data columns (total 23 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1489 non-null   int64  \n",
      " 1   dataset                          1489 non-null   object \n",
      " 2   instruction                      1489 non-null   object \n",
      " 3   code                             1489 non-null   object \n",
      " 4   test                             590 non-null    object \n",
      " 5   relevant_categories              1489 non-null   object \n",
      " 6   simplified_instruction           1489 non-null   object \n",
      " 7   extracted_constraints            1489 non-null   object \n",
      " 8   final_comprehensive_constraints  1489 non-null   object \n",
      " 9   filtered_relevant_constraints    1489 non-null   object \n",
      " 10  quality_scores                   1489 non-null   object \n",
      " 11  relevance_score                  1489 non-null   float64\n",
      " 12  objectivity_score                1489 non-null   float64\n",
      " 13  atomicity_score                  1489 non-null   float64\n",
      " 14  unified_quality_score            1489 non-null   float64\n",
      " 15  combined_instruction             1489 non-null   object \n",
      " 16  constraint_wise_presence         1489 non-null   object \n",
      " 17  constraint_presence_response     1489 non-null   object \n",
      " 18  final_constraints                1489 non-null   object \n",
      " 19  instruction_difficulty_labels    1489 non-null   object \n",
      " 20  difficulty_response              1489 non-null   object \n",
      " 21  constraint_difficulty_labels     1489 non-null   object \n",
      " 22  constraint_difficulty_response   1489 non-null   object \n",
      "dtypes: float64(4), int64(1), object(18)\n",
      "memory usage: 267.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_2 = pd.read_json(\"benchmark_dataset/benchmark_v10_v2.jsonl\", lines=True)\n",
    "df_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1d95d592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>instruction</th>\n",
       "      <th>code</th>\n",
       "      <th>test</th>\n",
       "      <th>relevant_categories</th>\n",
       "      <th>simplified_instruction</th>\n",
       "      <th>extracted_constraints</th>\n",
       "      <th>final_comprehensive_constraints</th>\n",
       "      <th>filtered_relevant_constraints</th>\n",
       "      <th>...</th>\n",
       "      <th>atomicity_score</th>\n",
       "      <th>unified_quality_score</th>\n",
       "      <th>combined_instruction</th>\n",
       "      <th>constraint_wise_presence</th>\n",
       "      <th>constraint_presence_response</th>\n",
       "      <th>final_constraints</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>difficulty_response</th>\n",
       "      <th>constraint_difficulty_labels</th>\n",
       "      <th>constraint_difficulty_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1400</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>### 142. Linked List Cycle II\\n\\nGiven the `he...</td>\n",
       "      <td>```python\\nfrom ListNode import *\\n\\n# Definit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Given the `head` of a linked list, return the ...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>...</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.60</td>\n",
       "      <td>### 142. Linked List Cycle II\\n\\nGiven the `he...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>{\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>[medium]</td>\n",
       "      <td>```json\\n{\\n  \"Reason\": \"The task involves det...</td>\n",
       "      <td>[easy, easy, easy, easy, easy, hard, hard, har...</td>\n",
       "      <td>```json\\n{\\n  \"Classification\": [\\n    {\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1401</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>Design a Django database migration that create...</td>\n",
       "      <td>```python\\n# -*- coding: utf-8 -*-\\nfrom __fut...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Design a Django database migration that create...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Design a Django database migration that create...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>{\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[medium]</td>\n",
       "      <td>```json\\n{\\n  \"Reason\": \"The task involves cre...</td>\n",
       "      <td>[easy, easy, easy, easy, easy, easy, easy, eas...</td>\n",
       "      <td>{\\n  \"Classification\": [\\n    {\\n      \"Constr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1402</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>Implement a class `PoseNet` that uses a Tensor...</td>\n",
       "      <td>```python\\nimport tflite_runtime.interpreter a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Implement a class `PoseNet` that uses a Tensor...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Implement a class `PoseNet` that uses a Tensor...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>{\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...</td>\n",
       "      <td>[{'type': 'Library and API Usage', 'constraint...</td>\n",
       "      <td>[hard]</td>\n",
       "      <td>```json\\n{\\n  \"Reason\": \"The task involves mul...</td>\n",
       "      <td>[easy, easy, easy, hard, easy, easy, easy, har...</td>\n",
       "      <td>{\\n  \"Classification\": [\\n    {\\n      \"Constr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1403</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>Create a Python script using Django's admin fu...</td>\n",
       "      <td>```python\\nfrom django.contrib import admin\\nf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Create a Python script using Django's admin fu...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>...</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4.83</td>\n",
       "      <td>Create a Python script using Django's admin fu...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>{\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[medium]</td>\n",
       "      <td>```json\\n{\\n  \"Reason\": \"The task involves cre...</td>\n",
       "      <td>[easy, easy, easy, easy, easy, easy, easy, eas...</td>\n",
       "      <td>```json\\n{\\n  \"Classification\": [\\n    {\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1404</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>Write a Python program that automates the proc...</td>\n",
       "      <td>```python\\nimport json\\nimport requests\\nfrom ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Write a Python program that automates the proc...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Write a Python program that automates the proc...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>{\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>[medium]</td>\n",
       "      <td>```json\\n{\\n  \"Reason\": \"The task involves han...</td>\n",
       "      <td>[easy, easy, easy, easy, easy, easy, easy, eas...</td>\n",
       "      <td>```json\\n{\\n  \"Classification\": [\\n    {\\n    ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                      dataset  \\\n",
       "0  1400  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "1  1401  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "2  1402  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "3  1403  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "4  1404  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  ### 142. Linked List Cycle II\\n\\nGiven the `he...   \n",
       "1  Design a Django database migration that create...   \n",
       "2  Implement a class `PoseNet` that uses a Tensor...   \n",
       "3  Create a Python script using Django's admin fu...   \n",
       "4  Write a Python program that automates the proc...   \n",
       "\n",
       "                                                code  test  \\\n",
       "0  ```python\\nfrom ListNode import *\\n\\n# Definit...   NaN   \n",
       "1  ```python\\n# -*- coding: utf-8 -*-\\nfrom __fut...   NaN   \n",
       "2  ```python\\nimport tflite_runtime.interpreter a...   NaN   \n",
       "3  ```python\\nfrom django.contrib import admin\\nf...   NaN   \n",
       "4  ```python\\nimport json\\nimport requests\\nfrom ...   NaN   \n",
       "\n",
       "                                 relevant_categories  \\\n",
       "0  ['Code Structure and Modularity', 'Input and O...   \n",
       "1  ['Code Structure and Modularity', 'Input and O...   \n",
       "2  ['Code Structure and Modularity', 'Input and O...   \n",
       "3  ['Code Structure and Modularity', 'Input and O...   \n",
       "4  ['Code Structure and Modularity', 'Input and O...   \n",
       "\n",
       "                              simplified_instruction  \\\n",
       "0  Given the `head` of a linked list, return the ...   \n",
       "1  Design a Django database migration that create...   \n",
       "2  Implement a class `PoseNet` that uses a Tensor...   \n",
       "3  Create a Python script using Django's admin fu...   \n",
       "4  Write a Python program that automates the proc...   \n",
       "\n",
       "                               extracted_constraints  \\\n",
       "0  [{'type': 'Input and Output Handling', 'constr...   \n",
       "1  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "2  [{'type': 'Library and API Usage', 'constraint...   \n",
       "3  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "4  [{'type': 'File and Data Management', 'constra...   \n",
       "\n",
       "                     final_comprehensive_constraints  \\\n",
       "0  [{'type': 'Input and Output Handling', 'constr...   \n",
       "1  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "2  [{'type': 'Library and API Usage', 'constraint...   \n",
       "3  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "4  [{'type': 'File and Data Management', 'constra...   \n",
       "\n",
       "                       filtered_relevant_constraints  ... atomicity_score  \\\n",
       "0  [{'type': 'Input and Output Handling', 'constr...  ...            4.60   \n",
       "1  [{'type': 'Code Structure and Modularity', 'co...  ...            5.00   \n",
       "2  [{'type': 'Library and API Usage', 'constraint...  ...            5.00   \n",
       "3  [{'type': 'Code Structure and Modularity', 'co...  ...            4.85   \n",
       "4  [{'type': 'File and Data Management', 'constra...  ...            5.00   \n",
       "\n",
       "   unified_quality_score                               combined_instruction  \\\n",
       "0                   4.60  ### 142. Linked List Cycle II\\n\\nGiven the `he...   \n",
       "1                   5.00  Design a Django database migration that create...   \n",
       "2                   5.00  Implement a class `PoseNet` that uses a Tensor...   \n",
       "3                   4.83  Create a Python script using Django's admin fu...   \n",
       "4                   5.00  Write a Python program that automates the proc...   \n",
       "\n",
       "                            constraint_wise_presence  \\\n",
       "0  [True, True, True, True, True, True, True, Tru...   \n",
       "1  [True, True, True, True, True, True, True, Tru...   \n",
       "2  [True, True, True, True, True, True, True, Tru...   \n",
       "3  [True, True, True, True, True, True, True, Tru...   \n",
       "4  [True, True, True, True, True, True, True, Tru...   \n",
       "\n",
       "                        constraint_presence_response  \\\n",
       "0  {\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...   \n",
       "1  {\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...   \n",
       "2  {\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...   \n",
       "3  {\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...   \n",
       "4  {\\n  \"Evaluation\": [\\n    {\\n      \"Constraint...   \n",
       "\n",
       "                                   final_constraints difficulty  \\\n",
       "0  [{'type': 'Input and Output Handling', 'constr...   [medium]   \n",
       "1  [{'type': 'Code Structure and Modularity', 'co...   [medium]   \n",
       "2  [{'type': 'Library and API Usage', 'constraint...     [hard]   \n",
       "3  [{'type': 'Code Structure and Modularity', 'co...   [medium]   \n",
       "4  [{'type': 'File and Data Management', 'constra...   [medium]   \n",
       "\n",
       "                                 difficulty_response  \\\n",
       "0  ```json\\n{\\n  \"Reason\": \"The task involves det...   \n",
       "1  ```json\\n{\\n  \"Reason\": \"The task involves cre...   \n",
       "2  ```json\\n{\\n  \"Reason\": \"The task involves mul...   \n",
       "3  ```json\\n{\\n  \"Reason\": \"The task involves cre...   \n",
       "4  ```json\\n{\\n  \"Reason\": \"The task involves han...   \n",
       "\n",
       "                        constraint_difficulty_labels  \\\n",
       "0  [easy, easy, easy, easy, easy, hard, hard, har...   \n",
       "1  [easy, easy, easy, easy, easy, easy, easy, eas...   \n",
       "2  [easy, easy, easy, hard, easy, easy, easy, har...   \n",
       "3  [easy, easy, easy, easy, easy, easy, easy, eas...   \n",
       "4  [easy, easy, easy, easy, easy, easy, easy, eas...   \n",
       "\n",
       "                      constraint_difficulty_response  \n",
       "0  ```json\\n{\\n  \"Classification\": [\\n    {\\n    ...  \n",
       "1  {\\n  \"Classification\": [\\n    {\\n      \"Constr...  \n",
       "2  {\\n  \"Classification\": [\\n    {\\n      \"Constr...  \n",
       "3  ```json\\n{\\n  \"Classification\": [\\n    {\\n    ...  \n",
       "4  ```json\\n{\\n  \"Classification\": [\\n    {\\n    ...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edeb97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from utils.compute_clients import create_clients\n",
    "api_key = os.getenv(\"IBM_OPENAI_API_KEY\")\n",
    "client = create_clients(mode=\"GPT-azure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3e1b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(len(df_2)):\n",
    "    if len(df_2[\"final_constraints\"][i]) != len(df_2[\"constraint_difficulty_labels\"][i]):\n",
    "        print(f\"Row {i} has mismatched lengths: {len(df_2['final_constraints'][i])} constraints vs {len(df_2['constraint_difficulty_labels'][i])} difficulties\")\n",
    "        print(\"Constraints:\", df_2[\"final_constraints\"][i])\n",
    "        print(\"Difficulties:\", df_2[\"constraint_difficulty_labels\"][i])\n",
    "        print(\"response_outputs\",df_2[\"constraint_difficulty_response\"][i])\n",
    "        # regenrate the response for this row\n",
    "        # prompt = constraint_difficulty_classification(\n",
    "        #     df_2[\"combined_instruction\"][i],\n",
    "        #     df_2[\"final_constraints\"][i]\n",
    "        # )\n",
    "        # df_2[\"constraint_difficulty_response\"][i] = client.get_model_response(user_prompt=prompt)\n",
    "        # df_2[\"constraint_difficulty_labels\"][i] = extract_label_values(df_2[\"constraint_difficulty_response\"][i])\n",
    "        # print(\"Updated response:\", df_2[\"constraint_difficulty_response\"][i])\n",
    "        # print(\"Updated labels:\", df_2[\"constraint_difficulty_labels\"][i])\n",
    "        # print(len(df_2[\"final_constraints\"][i]) == len(df_2[\"constraint_difficulty_labels\"][i]))\n",
    "        print(\"-\" * 40)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83a265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deffd19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Hard Constraints: 3270\n",
      "Total Easy Constraints: 7204\n",
      "Total Constraints: 10474\n",
      "total_constraints: 10474\n"
     ]
    }
   ],
   "source": [
    "num_hard = df_2[\"constraint_difficulty_labels\"].apply(lambda x: x.count(\"hard\"))\n",
    "num_easy = df_2[\"constraint_difficulty_labels\"].apply(lambda x: x.count(\"easy\"))\n",
    "print(\"Total Hard Constraints:\", num_hard.sum())\n",
    "print(\"Total Easy Constraints:\", num_easy.sum())\n",
    "print(\"Total Constraints:\", (num_hard + num_easy).sum())\n",
    "print(\"total_constraints:\",len(df_2[\"final_constraints\"].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fe9db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_2.rename(columns={\n",
    "    \"difficulty\": \"instruction_difficulty_labels\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b29a1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.to_json(\"benchmark_dataset/benchmark_v10_v2.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c63356e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instruction_difficulty_labels\n",
       "medium    1001\n",
       "hard       324\n",
       "easy       164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2[\"instruction_difficulty_labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "892b6db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: granite-3.3-8b-instruct_results_correctness_metrics_extended.jsonl\n",
      "File path: metrics_outputs/filtered_final_data/granite-3.3-8b-instruct_results_correctness_metrics_extended.jsonl\n",
      "no of columns in df: 28\n",
      "Columns already exist in granite-3.3-8b-instruct_results_correctness_metrics_extended.jsonl, skipping merge.\n",
      "Processing file: mixtral-8x7B-instruct-v0.1_results_correctness_metrics_extended.jsonl\n",
      "File path: metrics_outputs/filtered_final_data/mixtral-8x7B-instruct-v0.1_results_correctness_metrics_extended.jsonl\n",
      "no of columns in df: 28\n",
      "Columns already exist in mixtral-8x7B-instruct-v0.1_results_correctness_metrics_extended.jsonl, skipping merge.\n",
      "Processing file: Llama-3.1-8B-Instruct_results_correctness_metrics_extended.jsonl\n",
      "File path: metrics_outputs/filtered_final_data/Llama-3.1-8B-Instruct_results_correctness_metrics_extended.jsonl\n",
      "no of columns in df: 28\n",
      "Columns already exist in Llama-3.1-8B-Instruct_results_correctness_metrics_extended.jsonl, skipping merge.\n",
      "Processing file: Mistral-Small-3.1-24B-Instruct-2503_results_correctness_metrics_extended.jsonl\n",
      "File path: metrics_outputs/filtered_final_data/Mistral-Small-3.1-24B-Instruct-2503_results_correctness_metrics_extended.jsonl\n",
      "no of columns in df: 28\n",
      "Columns already exist in Mistral-Small-3.1-24B-Instruct-2503_results_correctness_metrics_extended.jsonl, skipping merge.\n",
      "Processing file: granite-3.1-8b-instruct_results_correctness_metrics_extended.jsonl\n",
      "File path: metrics_outputs/filtered_final_data/granite-3.1-8b-instruct_results_correctness_metrics_extended.jsonl\n",
      "no of columns in df: 28\n",
      "Columns already exist in granite-3.1-8b-instruct_results_correctness_metrics_extended.jsonl, skipping merge.\n",
      "Processing file: Qwen3-8B_results_correctness_metrics_extended.jsonl\n",
      "File path: metrics_outputs/filtered_final_data/Qwen3-8B_results_correctness_metrics_extended.jsonl\n",
      "no of columns in df: 28\n",
      "Columns already exist in Qwen3-8B_results_correctness_metrics_extended.jsonl, skipping merge.\n",
      "Processing file: llama-3-3-70b-instruct_results_correctness_metrics_extended.jsonl\n",
      "File path: metrics_outputs/filtered_final_data/llama-3-3-70b-instruct_results_correctness_metrics_extended.jsonl\n",
      "no of columns in df: 28\n",
      "Columns already exist in llama-3-3-70b-instruct_results_correctness_metrics_extended.jsonl, skipping merge.\n",
      "Processing file: deepseek-coder-33b-instruct_results_correctness_metrics_extended.jsonl\n",
      "File path: metrics_outputs/filtered_final_data/deepseek-coder-33b-instruct_results_correctness_metrics_extended.jsonl\n",
      "no of columns in df: 28\n",
      "Columns already exist in deepseek-coder-33b-instruct_results_correctness_metrics_extended.jsonl, skipping merge.\n",
      "Processing file: granite-3.1-2b-instruct_results_correctness_metrics_extended.jsonl\n",
      "File path: metrics_outputs/filtered_final_data/granite-3.1-2b-instruct_results_correctness_metrics_extended.jsonl\n",
      "no of columns in df: 28\n",
      "Columns already exist in granite-3.1-2b-instruct_results_correctness_metrics_extended.jsonl, skipping merge.\n",
      "Processing file: phi-4_results_correctness_metrics_extended.jsonl\n",
      "File path: metrics_outputs/filtered_final_data/phi-4_results_correctness_metrics_extended.jsonl\n",
      "no of columns in df: 28\n",
      "Columns already exist in phi-4_results_correctness_metrics_extended.jsonl, skipping merge.\n",
      "Processing file: mixtral-8x22B-instruct-v0.1_results_correctness_metrics_extended.jsonl\n",
      "File path: metrics_outputs/filtered_final_data/mixtral-8x22B-instruct-v0.1_results_correctness_metrics_extended.jsonl\n",
      "no of columns in df: 28\n",
      "Columns already exist in mixtral-8x22B-instruct-v0.1_results_correctness_metrics_extended.jsonl, skipping merge.\n"
     ]
    }
   ],
   "source": [
    "# now I have one df_2 and a folder of dfs , now read every jsonl file from the folder convert it to df and then i want to add 2 columns [instruction_difficulty_labels,constraint_difficulty_labels] from df_2 to each of the dfs in the folder by matching their values in the id column if the ids in df_2 not present in the corresponding df then continue and then save them back to the folder with the same name\n",
    "import os\n",
    "import json\n",
    "import pandas as pd \n",
    "df_2 = pd.read_json(\"benchmark_dataset/benchmark_v10_v2.jsonl\", lines=True)\n",
    "# folder_path = \"LLMjudge_outputs/filtered_final_data/\"\n",
    "# folder_path = \"LLMjudge_outputs/code_correctness/\"\n",
    "folder_path = \"metrics_outputs/filtered_final_data/\"\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename==\"metrics_summary_v4.jsonl\":\n",
    "        continue\n",
    "    if filename.endswith(\".jsonl\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        print(f\"File path: {file_path}\")\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        print(\"no of columns in df:\", len(df.columns))\n",
    "        # Merge df with df_2 on 'id' column\n",
    "        # check if the new columns exist in the  DataFrame\n",
    "        if \"instruction_difficulty_labels\" in df.columns and \"constraint_difficulty_labels\" in df.columns:\n",
    "            print(f\"Columns already exist in {filename}, skipping merge.\")\n",
    "            continue\n",
    "        merged_df = df.merge(df_2[[\"id\", \"instruction_difficulty_labels\", \"constraint_difficulty_labels\"]],\n",
    "                             on=\"id\", how=\"left\")\n",
    "        # Save the merged DataFrame back to the same file\n",
    "        print(f\"Merging {filename} with df_2...\")\n",
    "        print(f\"Original shape: {df.shape}, Merged shape: {merged_df.shape}\")\n",
    "        # check for any NaN values in the new columns\n",
    "       \n",
    "        if merged_df[\"instruction_difficulty_labels\"].isna().any() or merged_df[\"constraint_difficulty_labels\"].isna().any():\n",
    "            print(\"Warning: Some rows have NaN values in the new columns.\")\n",
    "        # Save the merged DataFrame back to the file\n",
    "        merged_df.to_json(file_path, orient='records', lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "125ba524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: granite-3.3-8b-instruct_results_correctness_metrics_extended.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SSR by Instruction Difficulty for granite-3.3-8b-instruct_results_correctness_metrics_extended.jsonl:\n",
      "instruction_difficulty_labels\n",
      "easy      0.869271\n",
      "hard      0.772841\n",
      "medium    0.841583\n",
      "Name: SSR_per_row, dtype: float64\n",
      "\n",
      "Average CSR by Instruction Difficulty for granite-3.3-8b-instruct_results_correctness_metrics_extended.jsonl:\n",
      "instruction_difficulty_labels\n",
      "easy      0.590909\n",
      "hard      0.247387\n",
      "medium    0.437403\n",
      "Name: CSR_per_row, dtype: float64\n",
      "\n",
      "Processing file: mixtral-8x7B-instruct-v0.1_results_correctness_metrics_extended.jsonl\n",
      "Average SSR by Instruction Difficulty for mixtral-8x7B-instruct-v0.1_results_correctness_metrics_extended.jsonl:\n",
      "instruction_difficulty_labels\n",
      "easy      0.554584\n",
      "hard      0.665187\n",
      "medium    0.665533\n",
      "Name: SSR_per_row, dtype: float64\n",
      "\n",
      "Average CSR by Instruction Difficulty for mixtral-8x7B-instruct-v0.1_results_correctness_metrics_extended.jsonl:\n",
      "instruction_difficulty_labels\n",
      "easy      0.287879\n",
      "hard      0.139373\n",
      "medium    0.241113\n",
      "Name: CSR_per_row, dtype: float64\n",
      "\n",
      "Processing file: Llama-3.1-8B-Instruct_results_correctness_metrics_extended.jsonl\n",
      "Average SSR by Instruction Difficulty for Llama-3.1-8B-Instruct_results_correctness_metrics_extended.jsonl:\n",
      "instruction_difficulty_labels\n",
      "easy      0.854126\n",
      "hard      0.761044\n",
      "medium    0.860900\n",
      "Name: SSR_per_row, dtype: float64\n",
      "\n",
      "Average CSR by Instruction Difficulty for Llama-3.1-8B-Instruct_results_correctness_metrics_extended.jsonl:\n",
      "instruction_difficulty_labels\n",
      "easy      0.636364\n",
      "hard      0.219512\n",
      "medium    0.485317\n",
      "Name: CSR_per_row, dtype: float64\n",
      "\n",
      "Processing file: Mistral-Small-3.1-24B-Instruct-2503_results_correctness_metrics_extended.jsonl\n",
      "Average SSR by Instruction Difficulty for Mistral-Small-3.1-24B-Instruct-2503_results_correctness_metrics_extended.jsonl:\n",
      "instruction_difficulty_labels\n",
      "easy      0.937175\n",
      "hard      0.850574\n",
      "medium    0.931019\n",
      "Name: SSR_per_row, dtype: float64\n",
      "\n",
      "Average CSR by Instruction Difficulty for Mistral-Small-3.1-24B-Instruct-2503_results_correctness_metrics_extended.jsonl:\n",
      "instruction_difficulty_labels\n",
      "easy      0.742424\n",
      "hard      0.414634\n",
      "medium    0.678516\n",
      "Name: CSR_per_row, dtype: float64\n",
      "\n",
      "Processing file: granite-3.1-8b-instruct_results_correctness_metrics_extended.jsonl\n",
      "Average SSR by Instruction Difficulty for granite-3.1-8b-instruct_results_correctness_metrics_extended.jsonl:\n",
      "instruction_difficulty_labels\n",
      "easy      0.814586\n",
      "hard      0.750661\n",
      "medium    0.821682\n",
      "Name: SSR_per_row, dtype: float64\n",
      "\n",
      "Average CSR by Instruction Difficulty for granite-3.1-8b-instruct_results_correctness_metrics_extended.jsonl:\n",
      "instruction_difficulty_labels\n",
      "easy      0.454545\n",
      "hard      0.184669\n",
      "medium    0.349304\n",
      "Name: CSR_per_row, dtype: float64\n",
      "\n",
      "Processing file: Qwen3-8B_results_correctness_metrics_extended.jsonl\n",
      "Average SSR by Instruction Difficulty for Qwen3-8B_results_correctness_metrics_extended.jsonl:\n",
      "instruction_difficulty_labels\n",
      "easy      0.907702\n",
      "hard      0.858420\n",
      "medium    0.938866\n",
      "Name: SSR_per_row, dtype: float64\n",
      "\n",
      "Average CSR by Instruction Difficulty for Qwen3-8B_results_correctness_metrics_extended.jsonl:\n",
      "instruction_difficulty_labels\n",
      "easy      0.696970\n",
      "hard      0.459930\n",
      "medium    0.710974\n",
      "Name: CSR_per_row, dtype: float64\n",
      "\n",
      "Processing file: results_easy_hard_instruction.jsonl\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'instruction_difficulty_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(file_path, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate average SSR for each value in instruction_difficulty_labels\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m avg_ssr_instruction \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstruction_difficulty_labels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSR_per_row\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage SSR by Instruction Difficulty for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mavg_ssr_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate average CSR for each value in instruction_difficulty_labels\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/internship/lib/python3.10/site-packages/pandas/core/frame.py:9190\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   9188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 9190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9193\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9196\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/internship/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1330\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1330\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m~/miniconda3/envs/internship/lib/python3.10/site-packages/pandas/core/groupby/grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'instruction_difficulty_labels'"
     ]
    }
   ],
   "source": [
    "# now take a folder of jsonl files and calculate the average ssr and the average CSR for each value in instruction_difficulty_labels and also average ssr for each value in constraint_difficulty_labels like ssr for easy constraints and ssr for hard constraints\n",
    "import os\n",
    "import pandas as pd\n",
    "folder_path = \"metrics_outputs/filtered_final_data/\"\n",
    "result_df = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename == \"metrics_summary_v4.jsonl\" :\n",
    "        continue\n",
    "    if filename.endswith(\".jsonl\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        # Calculate average SSR for each value in instruction_difficulty_labels\n",
    "        avg_ssr_instruction = df.groupby(\"instruction_difficulty_labels\")[\"SSR_per_row\"].mean()\n",
    "        \n",
    "        print(f\"Average SSR by Instruction Difficulty for {filename}:\\n{avg_ssr_instruction}\\n\")\n",
    "        \n",
    "        # Calculate average CSR for each value in instruction_difficulty_labels\n",
    "        avg_csr_instruction = df.groupby(\"instruction_difficulty_labels\")[\"CSR_per_row\"].mean()\n",
    "        print(f\"Average CSR by Instruction Difficulty for {filename}:\\n{avg_csr_instruction}\\n\")\n",
    "        \n",
    "        # Save the results to a new file\n",
    "        result_df.append({\n",
    "            \"filename\": filename,\n",
    "            \"avg_ssr_instruction\": avg_ssr_instruction.to_dict(),\n",
    "            \"avg_csr_instruction\": avg_csr_instruction.to_dict(),\n",
    "        })\n",
    "result_file_path = os.path.join(folder_path, f\"results_easy_hard_instruction.jsonl\")\n",
    "pd.DataFrame(result_df).to_json(result_file_path, orient='records', lines=True)\n",
    "print(\"results saved to:\", result_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afc649ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'easy': 0.8453692771833333, 'hard': 0.7343230085836236, 'medium': 0.8673727354545595}\n"
     ]
    }
   ],
   "source": [
    "print(avg_ssr_instruction.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8f912aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11 entries, 0 to 10\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   filename             11 non-null     object\n",
      " 1   avg_ssr_instruction  11 non-null     object\n",
      " 2   avg_csr_instruction  11 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 392.0+ bytes\n",
      "SSR trend not holding in mixtral-8x7B-instruct-v0.1_results_correctness_metrics_extended.jsonl: {'easy': 0.5545843046, 'hard': 0.6651870584, 'medium': 0.6655326384}\n",
      "SSR trend not holding in Llama-3.1-8B-Instruct_results_correctness_metrics_extended.jsonl: {'easy': 0.8541264291, 'hard': 0.7610444192, 'medium': 0.8608996962000001}\n",
      "SSR trend not holding in granite-3.1-8b-instruct_results_correctness_metrics_extended.jsonl: {'easy': 0.814585793, 'hard': 0.7506612532, 'medium': 0.8216819481000001}\n",
      "SSR trend not holding in Qwen3-8B_results_correctness_metrics_extended.jsonl: {'easy': 0.9077023986, 'hard': 0.8584199096, 'medium': 0.9388659705}\n",
      "CSR trend not holding in Qwen3-8B_results_correctness_metrics_extended.jsonl: {'easy': 0.696969697, 'hard': 0.45993031360000003, 'medium': 0.7109737249}\n",
      "SSR trend not holding in llama-3-3-70b-instruct_results_correctness_metrics_extended.jsonl: {'easy': 0.9249458874, 'hard': 0.8397763483, 'medium': 0.9302326387000001}\n",
      "SSR trend not holding in deepseek-coder-33b-instruct_results_correctness_metrics_extended.jsonl: {'easy': 0.8041670451, 'hard': 0.6876027621, 'medium': 0.8403014940000001}\n",
      "CSR trend not holding in deepseek-coder-33b-instruct_results_correctness_metrics_extended.jsonl: {'easy': 0.4393939394, 'hard': 0.1463414634, 'medium': 0.4435857805}\n",
      "SSR trend not holding in granite-3.1-2b-instruct_results_correctness_metrics_extended.jsonl: {'easy': 0.6738751148000001, 'hard': 0.653646929, 'medium': 0.7097590972000001}\n",
      "SSR trend not holding in mixtral-8x22B-instruct-v0.1_results_correctness_metrics_extended.jsonl: {'easy': 0.8453692772, 'hard': 0.7343230086, 'medium': 0.8673727355}\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.read_json(result_file_path, lines=True)\n",
    "result_df.info()\n",
    "# now check if the trend is not holding easy>medium>hard for both SSR and CSR, if not then print the filename and the values\n",
    "for filename, row in result_df.iterrows():\n",
    "    avg_ssr = row[\"avg_ssr_instruction\"]\n",
    "    avg_csr = row[\"avg_csr_instruction\"]\n",
    "    \n",
    "    # Check if the trend is not holding\n",
    "    if not (avg_ssr.get(\"easy\", 0) >= avg_ssr.get(\"medium\", 0) >= avg_ssr.get(\"hard\", 0)):\n",
    "        print(f\"SSR trend not holding in {row['filename']}: {avg_ssr}\")\n",
    "\n",
    "    if not (avg_csr.get(\"easy\", 0) >= avg_csr.get(\"medium\", 0) >= avg_csr.get(\"hard\", 0)):\n",
    "        print(f\"CSR trend not holding in {row['filename']}: {avg_csr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e96ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: granite-3.3-8b-instruct_results_correctness_metrics_extended.jsonl\n",
      "Computed SSR for granite-3.3-8b-instruct_results_correctness_metrics_extended.jsonl: easy=0.8470223599734337, hard=0.7942210074189769\n",
      "Processing file: mixtral-8x7B-instruct-v0.1_results_correctness_metrics_extended.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed SSR for mixtral-8x7B-instruct-v0.1_results_correctness_metrics_extended.jsonl: easy=0.6807615674119991, hard=0.6427176884029676\n",
      "Processing file: Llama-3.1-8B-Instruct_results_correctness_metrics_extended.jsonl\n",
      "Computed SSR for Llama-3.1-8B-Instruct_results_correctness_metrics_extended.jsonl: easy=0.8598627407571396, hard=0.8024209293244826\n",
      "Processing file: Mistral-Small-3.1-24B-Instruct-2503_results_correctness_metrics_extended.jsonl\n",
      "Computed SSR for Mistral-Small-3.1-24B-Instruct-2503_results_correctness_metrics_extended.jsonl: easy=0.924507416426832, hard=0.8789535337758688\n",
      "Processing file: granite-3.1-8b-instruct_results_correctness_metrics_extended.jsonl\n",
      "Computed SSR for granite-3.1-8b-instruct_results_correctness_metrics_extended.jsonl: easy=0.8293114899269427, hard=0.7746973838344396\n",
      "Processing file: Qwen3-8B_results_correctness_metrics_extended.jsonl\n",
      "Computed SSR for Qwen3-8B_results_correctness_metrics_extended.jsonl: easy=0.9280655157149181, hard=0.8852010933229207\n",
      "Processing file: llama-3-3-70b-instruct_results_correctness_metrics_extended.jsonl\n",
      "Computed SSR for llama-3-3-70b-instruct_results_correctness_metrics_extended.jsonl: easy=0.920726306465899, hard=0.8672393596251464\n",
      "Processing file: deepseek-coder-33b-instruct_results_correctness_metrics_extended.jsonl\n",
      "Computed SSR for deepseek-coder-33b-instruct_results_correctness_metrics_extended.jsonl: easy=0.820836098208361, hard=0.7344787192502928\n",
      "Processing file: granite-3.1-2b-instruct_results_correctness_metrics_extended.jsonl\n",
      "Computed SSR for granite-3.1-2b-instruct_results_correctness_metrics_extended.jsonl: easy=0.7235502434705622, hard=0.6837172979304958\n",
      "Processing file: phi-4_results_correctness_metrics_extended.jsonl\n",
      "Computed SSR for phi-4_results_correctness_metrics_extended.jsonl: easy=0.9251218431546301, hard=0.9042594763579523\n",
      "Processing file: mixtral-8x22B-instruct-v0.1_results_correctness_metrics_extended.jsonl\n",
      "Computed SSR for mixtral-8x22B-instruct-v0.1_results_correctness_metrics_extended.jsonl: easy=0.8466135458167331, hard=0.787973447871925\n",
      "                                             filename  ssr_easy  ssr_hard\n",
      "0   granite-3.3-8b-instruct_results_correctness_me...  0.847022  0.794221\n",
      "1   mixtral-8x7B-instruct-v0.1_results_correctness...  0.680762  0.642718\n",
      "2   Llama-3.1-8B-Instruct_results_correctness_metr...  0.859863  0.802421\n",
      "3   Mistral-Small-3.1-24B-Instruct-2503_results_co...  0.924507  0.878954\n",
      "4   granite-3.1-8b-instruct_results_correctness_me...  0.829311  0.774697\n",
      "5   Qwen3-8B_results_correctness_metrics_extended....  0.928066  0.885201\n",
      "6   llama-3-3-70b-instruct_results_correctness_met...  0.920726  0.867239\n",
      "7   deepseek-coder-33b-instruct_results_correctnes...  0.820836  0.734479\n",
      "8   granite-3.1-2b-instruct_results_correctness_me...  0.723550  0.683717\n",
      "9    phi-4_results_correctness_metrics_extended.jsonl  0.925122  0.904259\n",
      "10  mixtral-8x22B-instruct-v0.1_results_correctnes...  0.846614  0.787973\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def compute_ssr_by_difficulty(constraint_labels, constraint_adherence):\n",
    "    counts = Counter({\"easy\": 0, \"hard\": 0})\n",
    "    trues  = Counter({\"easy\": 0, \"hard\": 0})\n",
    "\n",
    "    for lbl, ok in zip(constraint_labels, constraint_adherence):\n",
    "        lbl = lbl.lower()\n",
    "        if lbl in counts:\n",
    "            counts[lbl] += 1\n",
    "            if ok:\n",
    "                trues[lbl] += 1\n",
    "\n",
    "    return {\n",
    "        \"easy\":  trues[\"easy\"]  / counts[\"easy\"]  if counts[\"easy\"]  > 0 else None,\n",
    "        \"hard\":  trues[\"hard\"]  / counts[\"hard\"]  if counts[\"hard\"]  > 0 else None,\n",
    "    }\n",
    "\n",
    "def summarize_ssr_for_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Reads each JSONL file in folder_path (already merged with labels & adherence),\n",
    "    computes SSR for easy vs. hard constraints, and returns a DataFrame:\n",
    "      filename | ssr_easy | ssr_hard\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname == \"metrics_summary_v4.jsonl\" or fname == \"results_easy_hard_instruction.jsonl\":\n",
    "            continue\n",
    "        if not fname.endswith(\".jsonl\"):\n",
    "            continue\n",
    "        path = os.path.join(folder_path, fname)\n",
    "        print(f\"Processing file: {fname}\")\n",
    "        df = pd.read_json(path, lines=True)\n",
    "\n",
    "        # flatten all rows' lists into two flat lists\n",
    "        all_labels    = [lbl for row in df[\"constraint_difficulty_labels\"] for lbl in row]\n",
    "        all_adherence = [ok  for row in df[\"Constraint_adherence\"] for ok  in row]\n",
    "\n",
    "        ssr = compute_ssr_by_difficulty(all_labels, all_adherence)\n",
    "        records.append({\n",
    "            \"filename\": fname,\n",
    "            \"ssr_easy\": ssr[\"easy\"],\n",
    "            \"ssr_hard\": ssr[\"hard\"],\n",
    "        })\n",
    "        print(f\"Computed SSR for {fname}: easy={ssr['easy']}, hard={ssr['hard']}\")\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Usage\n",
    "folder_path = \"metrics_outputs/filtered_final_data/\"\n",
    "results_df = summarize_ssr_for_folder(folder_path)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd7c1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_json(os.path.join(folder_path, \"Constraint_easy_hard_ssr.jsonl\"), orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "544aafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the trend is not holding easy>hard for  SSR , if not then print the filename and the values\n",
    "for _, row in results_df.iterrows():\n",
    "    ssr_easy = row[\"ssr_easy\"]\n",
    "    ssr_hard = row[\"ssr_hard\"]\n",
    "    \n",
    "    # Check if the trend is not holding\n",
    "    if ssr_easy is None or ssr_hard is None or not (ssr_easy >= ssr_hard):\n",
    "        print(f\"SSR trend not holding in {row['filename']}: easy={ssr_easy}, hard={ssr_hard}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa0ec9e-d0d0-4a11-8595-e0394b134628",
   "metadata": {},
   "source": [
    "# Compute metrics for easy and hard and medium difficulties of the instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3761e951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated summary file with avg CSR and SSR by instruction difficulty.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Path to summary file and folder with detailed JSONLs\n",
    "folder_path = \"metrics_outputs/filtered_final_data/\"\n",
    "summary_file_path = os.path.join(folder_path, \"metrics_summary_v4.jsonl\")\n",
    "\n",
    "# Load existing summary\n",
    "with open(summary_file_path, \"r\") as f:\n",
    "    summaries = [json.loads(line) for line in f]\n",
    "\n",
    "# Index summary entries by filename\n",
    "summary_dict = {entry[\"filename\"]: entry for entry in summaries}\n",
    "\n",
    "# Process each *_metrics_extended.jsonl file\n",
    "for filename in os.listdir(folder_path):\n",
    "    if not filename.endswith(\"_metrics_extended.jsonl\"):\n",
    "        continue\n",
    "\n",
    "    # Convert to match the filename key in summary\n",
    "    base_filename = filename.replace(\"_metrics_extended\", \"\")\n",
    "\n",
    "    if base_filename not in summary_dict:\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # # Parse constraints and adherence lists\n",
    "    # df[\"final_constraints\"] = df[\"final_constraints\"].apply(\n",
    "    #     lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "    # )\n",
    "    # df[\"constraint_adherence_list\"] = df[\"constraint_adherence_list\"].apply(\n",
    "    #     lambda x: list(map(int, ast.literal_eval(x))) if isinstance(x, str) else x\n",
    "    # )\n",
    "\n",
    "    # # Compute CSR/SSR per row\n",
    "    # df[\"CSR_per_row\"] = df[\"constraint_adherence_list\"].apply(lambda lst: int(all(lst)))\n",
    "    # df[\"SSR_per_row\"] = df[\"constraint_adherence_list\"].apply(lambda lst: sum(lst)/len(lst) if lst else 0)\n",
    "\n",
    "    # Compute CSR & SSR by instruction difficulty\n",
    "    if \"instruction_difficulty_labels\" in df.columns:\n",
    "        avg_csr = df.groupby(\"instruction_difficulty_labels\")[\"CSR_per_row\"].mean().to_dict()\n",
    "        avg_ssr = df.groupby(\"instruction_difficulty_labels\")[\"SSR_per_row\"].mean().to_dict()\n",
    "    else:\n",
    "        avg_csr = {}\n",
    "        avg_ssr = {}\n",
    "\n",
    "    # Update the summary entry\n",
    "    summary_dict[base_filename][\"avg_csr_by_instruction_difficulty\"] = avg_csr\n",
    "    summary_dict[base_filename][\"avg_ssr_by_instruction_difficulty\"] = avg_ssr\n",
    "\n",
    "# Save updated summaries back\n",
    "with open(summary_file_path, \"w\") as f:\n",
    "    for entry in summary_dict.values():\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"✅ Updated summary file with avg CSR and SSR by instruction difficulty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ef9a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 252 entries, 0 to 17\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   num_constraints  252 non-null    int64  \n",
      " 1   CSR_per_row      252 non-null    float64\n",
      " 2   count            252 non-null    int64  \n",
      " 3   model            252 non-null    object \n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 17.9+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 252 entries, 0 to 17\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   num_constraints  252 non-null    int64  \n",
      " 1   SSR_per_row      252 non-null    float64\n",
      " 2   count            252 non-null    int64  \n",
      " 3   model            252 non-null    object \n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 17.9+ KB\n",
      "None\n",
      "   num_constraints  CSR_per_row  count  \\\n",
      "0                1     0.750000      4   \n",
      "1                2     0.631579     19   \n",
      "2                3     0.450980     51   \n",
      "3                4     0.443038     79   \n",
      "4                5     0.514286    140   \n",
      "\n",
      "                                         model  \n",
      "0  granite-3.3-8b-instruct_results_correctness  \n",
      "1  granite-3.3-8b-instruct_results_correctness  \n",
      "2  granite-3.3-8b-instruct_results_correctness  \n",
      "3  granite-3.3-8b-instruct_results_correctness  \n",
      "4  granite-3.3-8b-instruct_results_correctness  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Folder path containing *_metrics_extended.jsonl files\n",
    "folder_path = \"metrics_outputs/filtered_final_data/\"\n",
    "\n",
    "# Initialize storage for results\n",
    "csr_data = []\n",
    "ssr_data = []\n",
    "\n",
    "# Process each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\"_metrics_extended.jsonl\"):\n",
    "        model_name = filename.replace(\"_metrics_extended.jsonl\", \"\")\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        \n",
    "        # # Ensure final_constraints is parsed\n",
    "        # df[\"final_constraints\"] = df[\"final_constraints\"].apply(\n",
    "        #     lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "        # )\n",
    "        \n",
    "        # # Calculate number of constraints per record\n",
    "        # df[\"num_constraints\"] = df[\"final_constraints\"].apply(len)\n",
    "        \n",
    "        # Group by number of constraints and compute mean CSR, SSR, and count\n",
    "        grouped = df.groupby(\"num_constraints\").agg({\n",
    "            \"CSR_per_row\": \"mean\",\n",
    "            \"SSR_per_row\": \"mean\",\n",
    "            \"final_constraints\": \"count\"  # This gives count of rows per group\n",
    "        }).rename(columns={\"final_constraints\": \"count\"}).reset_index()\n",
    "        \n",
    "        grouped[\"model\"] = model_name\n",
    "\n",
    "        csr_data.append(grouped[[\"num_constraints\", \"CSR_per_row\", \"count\", \"model\"]])\n",
    "        ssr_data.append(grouped[[\"num_constraints\", \"SSR_per_row\", \"count\", \"model\"]])\n",
    "\n",
    "# Combine data\n",
    "csr_df = pd.concat(csr_data)\n",
    "ssr_df = pd.concat(ssr_data)\n",
    "\n",
    "# Save to disk\n",
    "csr_df.to_json(os.path.join(folder_path, \"csr_data_num_constraints.jsonl\"), orient=\"records\", lines=True)\n",
    "ssr_df.to_json(os.path.join(folder_path, \"ssr_data_num_constraints.jsonl\"), orient=\"records\", lines=True)\n",
    "\n",
    "# Debug print\n",
    "print(csr_df.info())\n",
    "print(ssr_df.info())\n",
    "print(csr_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02acd0eb-32b4-4170-ba68-f3e7fdd57d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: granite-3.3-8b-instruct_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: granite-3.3-8b-instruct_results_correctness_metrics_extended.jsonl\n",
      "Processing: mixtral-8x7B-instruct-v0.1_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: mixtral-8x7B-instruct-v0.1_results_correctness_metrics_extended.jsonl\n",
      "Processing: Llama-3.1-8B-Instruct_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: Llama-3.1-8B-Instruct_results_correctness_metrics_extended.jsonl\n",
      "Processing: Mistral-Small-3.1-24B-Instruct-2503_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: Mistral-Small-3.1-24B-Instruct-2503_results_correctness_metrics_extended.jsonl\n",
      "Processing: granite-3.1-8b-instruct_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: granite-3.1-8b-instruct_results_correctness_metrics_extended.jsonl\n",
      "Processing: Qwen3-8B_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: Qwen3-8B_results_correctness_metrics_extended.jsonl\n",
      "Processing: granite-3.3-8b-instruct_reasoning_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: granite-3.3-8b-instruct_reasoning_results_correctness_metrics_extended.jsonl\n",
      "Processing: o3-mini_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: o3-mini_results_correctness_metrics_extended.jsonl\n",
      "Processing: llama-3-3-70b-instruct_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: llama-3-3-70b-instruct_results_correctness_metrics_extended.jsonl\n",
      "Processing: deepseek-coder-33b-instruct_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: deepseek-coder-33b-instruct_results_correctness_metrics_extended.jsonl\n",
      "Processing: granite-3.1-2b-instruct_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: granite-3.1-2b-instruct_results_correctness_metrics_extended.jsonl\n",
      "Processing: phi-4_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: phi-4_results_correctness_metrics_extended.jsonl\n",
      "Processing: gpt-4o-2024-08-06_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: gpt-4o-2024-08-06_results_correctness_metrics_extended.jsonl\n",
      "Processing: mixtral-8x22B-instruct-v0.1_results_correctness_metrics_extended.jsonl\n",
      "✅ Updated and saved: mixtral-8x22B-instruct-v0.1_results_correctness_metrics_extended.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def add_num_constraints_column(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if \"results_correctness\" in filename and filename.endswith(\".jsonl\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing: {filename}\")\n",
    "\n",
    "            try:\n",
    "                df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "                # Ensure final_constraints is a list\n",
    "                df[\"final_constraints\"] = df[\"final_constraints\"].apply(\n",
    "                    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "                )\n",
    "\n",
    "                # Add num_constraints column\n",
    "                df[\"num_constraints\"] = df[\"final_constraints\"].apply(len)\n",
    "\n",
    "                # Save back to the same file\n",
    "                df.to_json(file_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "                print(f\"✅ Updated and saved: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to process {filename}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    folder = \"metrics_outputs/filtered_final_data/\"  # Replace with your actual folder path\n",
    "    add_num_constraints_column(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64a559ba-7fbd-4a2b-94f2-6d8d5b1c000c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Mean  Min  25%  50% (Median)  75%   Max\n",
      "Extracted Constraints  4.61  0.0  3.0           4.0  6.0  18.0\n",
      "New Constraints        2.43  0.0  1.0           2.0  4.0   9.0\n",
      "Total Constraints      7.04  0.0  5.0           7.0  9.0  18.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_json(\"metrics_outputs/filtered_final_data/granite-3.1-8b-instruct_results_correctness_metrics_extended.jsonl\", lines=True)  # update path if needed\n",
    "\n",
    "def count_constraints(row):\n",
    "    extracted = 0\n",
    "    new = 0\n",
    "    try:\n",
    "        constraints = ast.literal_eval(row) if isinstance(row, str) else row\n",
    "        for c in constraints:\n",
    "            if c[\"instruction_part\"] == \"Extracted from instruction\":\n",
    "                extracted += 1\n",
    "            elif c[\"instruction_part\"] == \"Newly Generated\":\n",
    "                new += 1\n",
    "    except:\n",
    "        pass\n",
    "    return pd.Series([extracted, new, extracted + new])\n",
    "\n",
    "# Apply constraint counting logic\n",
    "df[[\"Extracted Constraints\", \"New Constraints\", \"Total Constraints\"]] = df[\"final_constraints\"].apply(count_constraints)\n",
    "\n",
    "# Compute descriptive stats\n",
    "columns = [\"Extracted Constraints\", \"New Constraints\", \"Total Constraints\"]\n",
    "summary = df[columns].describe(percentiles=[0.25, 0.5, 0.75]).loc[[\"mean\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]]\n",
    "summary.index = [\"Mean\", \"Min\", \"25%\", \"50% (Median)\", \"75%\", \"Max\"]\n",
    "summary = summary.T.round(2)\n",
    "\n",
    "# Display as table\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5316a9f9-dcb5-497b-93a7-13b28bc6d590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (107 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /u/sravanigunnu/miniconda3/envs/internship/lib/python3.10/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /u/sravanigunnu/miniconda3/envs/internship/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /u/sravanigunnu/miniconda3/envs/internship/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /u/sravanigunnu/miniconda3/envs/internship/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /u/sravanigunnu/miniconda3/envs/internship/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m139.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m168.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[?25l\u001b[31mERROR: Could not install packages due to an OSError: [Errno 122] Disk quota exceeded\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/6\u001b[0m [pyparsing]\n",
      "\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc72d4bb-f9bd-4664-b83a-319b78b9e2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
