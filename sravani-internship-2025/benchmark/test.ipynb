{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543569f-21f8-4179-a6e2-300e485c49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d642f6-1134-4fcf-9d46-b6c56108bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f19a1c-c614-484b-a313-6e397a3d00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c541f-b729-4f1e-ab2e-6e3bcfc46953",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd287c-7130-44ca-9dce-60a0180aecb0",
   "metadata": {},
   "source": [
    "# Testing Huggingface models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c249047c-693c-403e-b24e-b6569e1007aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0ab93-a89c-431f-89ab-c223a80b24f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "prompt = \"Explain how photosynthesis works in simple terms.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a426e0-0b0c-4c02-9164-b2a500d1eda1",
   "metadata": {},
   "source": [
    "# Testing using VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c9239-658f-4bc1-b0d2-6c4753df20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"NCCL_CUMEM_ENABLE\"] = \"1\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f02dc-f6ee-4c6d-98ed-52859f4a9e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a53ba-2a39-4ee9-84e2-77f45e8879af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "llm = LLM(model=\"facebook/opt-125m\")\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340dc55b-349a-4090-9174-fdd4b01fc8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "llm = LLM(model=\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c17e330-eb76-4518-ad9c-efa349387cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_outputs(outputs):\n",
    "    print(\"\\nGenerated Outputs:\\n\" + \"-\" * 80)\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(f\"Prompt: {prompt!r}\\n\")\n",
    "        print(f\"Generated text: {generated_text!r}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa026e-80a5-407e-b921-fbd9b01c2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write an essay about the importance of higher education.\",\n",
    "    },\n",
    "]\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "outputs = llm.chat(conversation, sampling_params, use_tqdm=False)\n",
    "print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd66b9b-f002-4cab-8d41-7096ae3826b6",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "219a8401-a01a-4357-a121-45a2266c5da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 17 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1500 non-null   int64  \n",
      " 1   id.1                             1500 non-null   int64  \n",
      " 2   dataset                          1500 non-null   object \n",
      " 3   instruction                      1500 non-null   object \n",
      " 4   code                             1500 non-null   object \n",
      " 5   test                             600 non-null    object \n",
      " 6   relevant_categories              1500 non-null   object \n",
      " 7   simplified_instruction           1500 non-null   object \n",
      " 8   extracted_constraints            1500 non-null   object \n",
      " 9   final_comprehensive_constraints  1500 non-null   object \n",
      " 10  filtered_relevant_constraints    1500 non-null   object \n",
      " 11  quality_scores                   1500 non-null   object \n",
      " 12  relevance_score                  1500 non-null   float64\n",
      " 13  objectivity_score                1500 non-null   float64\n",
      " 14  atomicity_score                  1500 non-null   float64\n",
      " 15  unified_quality_score            1500 non-null   float64\n",
      " 16  combined_instruction             1500 non-null   object \n",
      "dtypes: float64(4), int64(2), object(11)\n",
      "memory usage: 199.3+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"benchmark_v1.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d324c799-c805-4525-b50f-183e020befb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "679c2134-4bc7-4906-963e-9b63f9f6b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.head(10).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a8002b-4893-4363-966f-04ed9e59733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate_responses(prompts, **sampling_kwargs):\n",
    "    messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},{\"role\": \"user\", \"content\": prompt}] for prompt in prompts]\n",
    "    print(messages)\n",
    "    sampling_params = SamplingParams(**sampling_kwargs)\n",
    "    outputs = llm.chat(messages, sampling_params, use_tqdm=True)\n",
    "    # Extracting the text from the outputs\n",
    "    final_outputs = [out.outputs[0].text for out in outputs if out.outputs]\n",
    "    with open(\"vllm_outputs.txt\", \"w\") as f:\n",
    "        for output in final_outputs:\n",
    "            print(\"Output started...................................................\")\n",
    "            print(output)\n",
    "            print(\"output ended.....................................................\")\n",
    "    print(\"Outputs saved to vllm_outputs.txt\")\n",
    "    return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dfab9-25db-41f7-87ef-4606d8f599f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_outputs = []\n",
    "batch_size = 8\n",
    "for i in tqdm(range(0, len(df), batch_size), desc=\"Generating\"):\n",
    "    batch_prompts = df[\"combined_instruction\"].iloc[i:i+batch_size].tolist()\n",
    "    responses = generate_responses(\n",
    "        batch_prompts,\n",
    "        temperature=0,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    all_outputs.extend(responses)\n",
    "    print(f\"Batch {i // batch_size + 1}/{(len(df) + batch_size - 1) // batch_size} processed.\")\n",
    "len(all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bffaef-38d5-41d1-920b-d2667c2e80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation =[ [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write an essay about the importance of higher education.\",\n",
    "    },\n",
    "],[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"what is deeplearning\",\n",
    "    },\n",
    "] ]\n",
    "\n",
    "outputs = llm.chat(conversation, sampling_params, use_tqdm=False)\n",
    "print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a3d29-ad42-4f3b-9140-08b9cd40eb40",
   "metadata": {},
   "source": [
    "# Testing shanmukh's huggingfcae script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a265d5-fc46-4e84-8248-edf6e9634641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "import torch\n",
    "\n",
    "model_path=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "device=\"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=device,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path\n",
    ")\n",
    "\n",
    "conv = [{\"role\": \"user\", \"content\":\"Redesign a common household item to make it more sustainable and user-friendly. Explain the changes and their benefits.\"}]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(conv, return_tensors=\"pt\", thinking=True, return_dict=True, add_generation_prompt=True).to(device)\n",
    "\n",
    "set_seed(42)\n",
    "output = model.generate(\n",
    "    **input_ids,\n",
    "    max_new_tokens=8192,\n",
    ")\n",
    "\n",
    "prediction = tokenizer.decode(output[0, input_ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26fbbfa-9b96-455e-a1bd-63a695dd28af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "model_path = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "batch_size = 4  # You can adjust this\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "    torch_dtype=dtype\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# List of conversations (each with system/user messages)\n",
    "conversations = [\n",
    "    [{\"role\": \"user\", \"content\": \"Redesign a common household item to make it more sustainable and user-friendly. Explain the changes and their benefits.\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"What are the economic benefits of electric vehicles?\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"How does photosynthesis work in plants?\"}]\n",
    "]\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Collect predictions\n",
    "all_predictions = []\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(0, len(conversations), batch_size), desc=\"Generating\"):\n",
    "    batch = conversations[i:i + batch_size]\n",
    "\n",
    "    # Tokenize all conversations in the batch\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        batch,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_dict=True,\n",
    "        add_generation_prompt=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate responses\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,  # Adjust as needed\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Decode only the generated part (skip input tokens)\n",
    "    for j in range(len(batch)):\n",
    "        generated_tokens = outputs[j][inputs[\"input_ids\"].shape[1]:]\n",
    "        prediction = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        all_predictions.append(prediction)\n",
    "\n",
    "# Print all predictions\n",
    "for i, pred in enumerate(all_predictions):\n",
    "    print(f\"\\n--- Response {i+1} ---\\n{pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c864e-1aa4-43da-afbe-51f697b0c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HuggingFace Client (Batched) --------\n",
    "class HuggingFaceClient:\n",
    "    def __init__(self, model_name, hf_token, device=\"cuda\"):\n",
    "        import torch\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed, logging\n",
    "        logging.set_verbosity_error()\n",
    "        self.torch = torch\n",
    "        self.set_seed = set_seed\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=self.device,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            token=hf_token\n",
    "        )\n",
    "        self.model.eval()\n",
    "        self.set_seed(42)\n",
    "\n",
    "    def generate_responses(self, prompts, **sampling_kwargs):\n",
    "        batch_size = sampling_kwargs.get(\"batch_size\", 4)\n",
    "        max_new_tokens = sampling_kwargs.get(\"max_tokens\", 1024)\n",
    "        temperature = sampling_kwargs.get(\"temperature\", 0.7)\n",
    "        top_p = sampling_kwargs.get(\"top_p\", 0.9)\n",
    "\n",
    "        all_predictions = []\n",
    "        conversations = [[{\"role\": \"user\", \"content\": prompt}] for prompt in prompts]\n",
    "\n",
    "        for i in tqdm(range(0, len(conversations), batch_size), desc=\"HF Generating\"):\n",
    "            batch = conversations[i:i + batch_size]\n",
    "\n",
    "            inputs = self.tokenizer.apply_chat_template(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_dict=True,\n",
    "                add_generation_prompt=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            with self.torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p\n",
    "                )\n",
    "\n",
    "            for j in range(len(batch)):\n",
    "                input_len = inputs[\"input_ids\"].shape[1]\n",
    "                generated_tokens = outputs[j][input_len:]\n",
    "                prediction = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "                all_predictions.append(prediction)\n",
    "\n",
    "        return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed93aad-a1b7-47ab-ad4a-409bb165c345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "hf_token =\"\"\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "batch_size = 8\n",
    "all_outputs=[]\n",
    "client = HuggingFaceClient(model_name=model_name, hf_token=hf_token)\n",
    "for i in tqdm(range(0, len(prompts), batch_size), desc=\"Batching\"):\n",
    "        batch_prompts = prompts[i:i+batch_size]\n",
    "        responses = client.generate_responses(\n",
    "            batch_prompts,\n",
    "            temperature=0.7,\n",
    "            max_tokens=1024,\n",
    "            batch_size=8\n",
    "        )\n",
    "        all_outputs.extend(responses)\n",
    "for output in all_outputs:\n",
    "    print(\"Output :\\n\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee6974e-8748-4c12-a4ef-eb841123ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809933fc-a243-43f8-90b9-8ff8b4760a32",
   "metadata": {},
   "source": [
    "## openai testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2502d418-b0d0-4f0a-a929-5d4a5443f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- OpenAI Client (Concurrent Inference) --------\n",
    "class OpenAIClient:\n",
    "    def __init__(self, model_name, openai_key, max_workers=100):\n",
    "        from openai import OpenAI\n",
    "        import time\n",
    "        self.client = OpenAI(api_key=openai_key)\n",
    "        self.model_name = model_name\n",
    "        self.time = time\n",
    "        self.max_workers = max_workers\n",
    "        self.SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
    "\n",
    "    def get_response_v2(self, messages, max_retries=1):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model_name,\n",
    "                    messages=messages,\n",
    "                    temperature=0\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"Error on attempt {attempt + 1}: {e}\")\n",
    "                self.time.sleep(2)\n",
    "        return \"[Error]\"\n",
    "\n",
    "    def generate_responses(self, prompts, **kwargs):\n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        # Prepare messages with system prompt\n",
    "        messages_list = [[\n",
    "            {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ] for prompt in prompts]\n",
    "\n",
    "        # Run inference concurrently\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            results = list(tqdm(\n",
    "                executor.map(lambda m: self.get_response_v2(m), messages_list),\n",
    "                total=len(messages_list),\n",
    "                desc=\"OpenAI Generating\"\n",
    "            ))\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec82b94-246a-4d08-b579-2fa7653cf83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAIClient(model_name=model_name, openai_key=openai_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c574cac7-7cd8-4fff-8f1f-9b00b4a5b652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batching:   0%|                                                                                    | 0/2 [00:00<?, ?it/s]\n",
      "OpenAI Generating:   0%|                                                                           | 0/8 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on attempt 1: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error on attempt 1: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error on attempt 1: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error on attempt 1: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error on attempt 1: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error on attempt 1: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error on attempt 1: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error on attempt 1: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "OpenAI Generating:  12%|████████▍                                                          | 1/8 [00:06<00:42,  6.08s/it]\u001b[A\n",
      "OpenAI Generating: 100%|███████████████████████████████████████████████████████████████████| 8/8 [00:06<00:00,  1.21it/s]\u001b[A\n",
      "Batching:  50%|██████████████████████████████████████                                      | 1/2 [00:06<00:06,  6.64s/it]\n",
      "OpenAI Generating:   0%|                                                                           | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on attempt 1: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error on attempt 1: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "OpenAI Generating: 100%|███████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.17s/it]\u001b[A\n",
      "Batching: 100%|████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output :\n",
      "\n",
      "[Error]\n",
      "Output :\n",
      "\n",
      "[Error]\n",
      "Output :\n",
      "\n",
      "[Error]\n",
      "Output :\n",
      "\n",
      "[Error]\n",
      "Output :\n",
      "\n",
      "[Error]\n",
      "Output :\n",
      "\n",
      "[Error]\n",
      "Output :\n",
      "\n",
      "[Error]\n",
      "Output :\n",
      "\n",
      "[Error]\n",
      "Output :\n",
      "\n",
      "[Error]\n",
      "Output :\n",
      "\n",
      "[Error]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "prompts = df[\"combined_instruction\"].tolist()\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "batch_size = 8\n",
    "all_outputs=[]\n",
    "for i in tqdm(range(0, len(prompts), batch_size), desc=\"Batching\"):\n",
    "        batch_prompts = prompts[i:i+batch_size]\n",
    "        responses = client.generate_responses(\n",
    "            batch_prompts,\n",
    "            temperature=0.7,\n",
    "            max_tokens=1024,\n",
    "            batch_size=8\n",
    "        )\n",
    "        all_outputs.extend(responses)\n",
    "for output in all_outputs:\n",
    "    print(\"Output :\\n\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c001831b-2050-4f7c-82c4-0d2b5e30ded6",
   "metadata": {},
   "source": [
    "# Rits testing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6e239d-f08d-479a-a9b7-3f4a5161deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_CALLS_PER_MIN = 1500\n",
    "\n",
    "\n",
    "class LLMClient:\n",
    "\n",
    "    def __init__(self, api_key,model_id,client_type=\"rits\", base_url=None):\n",
    "        if client_type == \"rits\":\n",
    "            llm = OpenAI(\n",
    "                api_key=api_key,\n",
    "                base_url=base_url,\n",
    "                default_headers={\"RITS_API_KEY\": api_key},\n",
    "            )\n",
    "        elif client_type == \"GPT\":\n",
    "            llm = OpenAI(\n",
    "                api_key=api_key)\n",
    "        self.llm = llm\n",
    "        self.model_id = model_id\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    def get_model_response(\n",
    "        self,\n",
    "        messages=None,\n",
    "        system_prompt=None,\n",
    "        user_prompt=None,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.1\n",
    "    ):\n",
    "        # Setup the sampling parameters for generation\n",
    "        if messages is None:\n",
    "            if system_prompt:\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ]\n",
    "            else:\n",
    "                messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "        response = self.llm.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            messages=messages,\n",
    "            max_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    def apply_chat_template(self, messages_list):\n",
    "        prompts = [\n",
    "            self.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            for messages in messages_list\n",
    "        ]\n",
    "        return prompts\n",
    "\n",
    "    def get_model_response_batch(\n",
    "        self, system_prompt=None, user_prompts=None, max_new_tokens=1024, temperature=0.1\n",
    "    ):\n",
    "        non_none_user_prompts = [ele for ele in user_prompts if ele is not None]\n",
    "        if system_prompt:\n",
    "            messages_list = [\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ]\n",
    "                for user_prompt in non_none_user_prompts\n",
    "            ]\n",
    "        else:\n",
    "            messages_list = [\n",
    "                [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "                for user_prompt in non_none_user_prompts\n",
    "            ]\n",
    "        with ThreadPoolExecutor(max_workers=NUM_CALLS_PER_MIN) as executor:\n",
    "            response_texts = list(\n",
    "                tqdm(\n",
    "                    executor.map(\n",
    "                        lambda messages: self.call_api(\n",
    "                            messages, max_new_tokens, temperature\n",
    "                        ),\n",
    "                        messages_list,\n",
    "                    ),\n",
    "                    total=len(messages_list),\n",
    "                    desc=\"Processing\",\n",
    "                )\n",
    "            )\n",
    "        response_iter = iter(response_texts)\n",
    "        all_response_texts = [\n",
    "            next(response_iter) if ele is not None else None for ele in user_prompts\n",
    "        ]\n",
    "        return all_response_texts\n",
    "\n",
    "    @sleep_and_retry\n",
    "    @limits(calls=1500, period=60)\n",
    "    def call_api(self, messages, max_new_tokens, temperature):\n",
    "        response = self.get_model_response(\n",
    "            messages=messages, max_new_tokens=max_new_tokens, temperature=temperature\n",
    "        )\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d882a90c-3dbf-491e-baf4-be6b95082e08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48032bfa1e1d782cece12fedb6b3fb40\n",
      "Generating responses for 10 prompts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import numpy as np\n",
      "import math\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import get_window\n",
      "\n",
      "def task_func(amplitude, frequency, time):\n",
      "    \"\"\"\n",
      "    Generates and plots a complex wave with a specified amplitude and frequency over given time points,\n",
      "    applying a Hann window to reduce edge effects.\n",
      "\n",
      "    Parameters:\n",
      "    amplitude (float): The amplitude of the complex wave.\n",
      "    frequency (float): The frequency of the complex wave.\n",
      "    time (numpy.ndarray): The time points at which the complex wave is generated.\n",
      "\n",
      "    Returns:\n",
      "    numpy.ndarray: The generated complex wave as a numpy array of complex numbers.\n",
      "    matplotlib.figure.Figure: The figure object of the plot.\n",
      "    matplotlib.axes.Axes: The axes object of the plot.\n",
      "    \"\"\"\n",
      "\n",
      "    # Generate the complex wave using the formula wave = amplitude * np.exp(1j * 2 * math.pi * frequency * time)\n",
      "    wave = amplitude * np.exp(1j * 2 * math.pi * frequency * time)\n",
      "\n",
      "    # Apply a Hann window to the wave to reduce edge effects\n",
      "    hann_window = get_window('hann', len(time))\n",
      "    wave_windowed = wave * hann_window[:, None]\n",
      "\n",
      "    # Create a new figure and axis object\n",
      "    fig, ax = plt.subplots()\n",
      "\n",
      "    # Plot the real and imaginary parts of the complex wave\n",
      "    ax.plot(time, np.real(wave_windowed), label='Real part')\n",
      "    ax.plot(time, np.imag(wave_windowed), label='Imaginary part')\n",
      "\n",
      "    # Set the plot title, x-label, and y-label\n",
      "    ax.set_title('Complex Wave with Hann Window')\n",
      "    ax.set_xlabel('Time')\n",
      "    ax.set_ylabel('Amplitude')\n",
      "\n",
      "    # Add a legend to the plot\n",
      "    ax.legend()\n",
      "\n",
      "    # Return the generated complex wave and the plot object\n",
      "    return wave_windowed, fig, ax\n",
      "\n",
      "# Example usage:\n",
      "time = np.linspace(0, 1, 1000)\n",
      "amplitude = 1.0\n",
      "frequency = 10.0\n",
      "\n",
      "wave, fig, ax = task_func(amplitude, frequency, time)\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "This code defines a function `task_func` that generates a complex wave with a specified amplitude and frequency over given time points, applies a Hann window to reduce edge effects, and plots the real and imaginary parts of the complex wave. The function returns the generated complex wave and the plot object.\n",
      "\n",
      "In the example usage, we generate a complex wave with an amplitude of 1.0, a frequency of 10.0, and time points from 0 to 1 with a step size of 0.001. We then call the `task_func` function to generate the complex wave and plot it. Finally, we display the plot using `plt.show()`.\n",
      "```python\n",
      "import os\n",
      "import logging\n",
      "\n",
      "def task_func(directory):\n",
      "    \"\"\"\n",
      "    Removes all jQuery files from a specified directory and logs the removed files.\n",
      "\n",
      "    Args:\n",
      "        directory (str): The path to the directory containing jQuery files.\n",
      "\n",
      "    Returns:\n",
      "        tuple: A tuple containing the number of files removed and a list of the names of the removed files.\n",
      "\n",
      "    Raises:\n",
      "        FileNotFoundError: If the specified directory does not exist.\n",
      "    \"\"\"\n",
      "\n",
      "    # Initialize the logger\n",
      "    logging.basicConfig(filename='jquery_removal.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
      "\n",
      "    # Check if the directory exists\n",
      "    if not os.path.exists(directory):\n",
      "        raise FileNotFoundError(f\"The directory '{directory}' does not exist.\")\n",
      "\n",
      "    # Initialize counters for removed files\n",
      "    removed_count = 0\n",
      "    removed_files = []\n",
      "\n",
      "    # Iterate over all files in the directory\n",
      "    for filename in os.listdir(directory):\n",
      "        # Construct the full path to the file\n",
      "        filepath = os.path.join(directory, filename)\n",
      "\n",
      "        # Check if the file is a jQuery file\n",
      "        if 'jquery' in filename.lower():\n",
      "            try:\n",
      "                # Attempt to remove the file\n",
      "                os.remove(filepath)\n",
      "                # Log the removed file\n",
      "                logging.info(f\"Removed file: {filename}\")\n",
      "                # Increment the removed count\n",
      "                removed_count += 1\n",
      "                # Add the file to the list of removed files\n",
      "                removed_files.append(filename)\n",
      "            except Exception as e:\n",
      "                # Log any errors that occur during removal\n",
      "                logging.error(f\"Error removing file '{filename}': {str(e)}\")\n",
      "\n",
      "    # Return the number of removed files and the list of removed files\n",
      "    return removed_count, removed_files\n",
      "\n",
      "# Example usage:\n",
      "try:\n",
      "    removed_count, removed_files = task_func('/path/to/directory')\n",
      "    print(f\"Removed {removed_count} files: {removed_files}\")\n",
      "except FileNotFoundError as e:\n",
      "    print(e)\n",
      "```\n",
      "\n",
      "This code defines a function `task_func` that takes a directory path as input and removes all jQuery files from that directory. It logs the removed files to a file named `jquery_removal.log` and returns a tuple containing the number of removed files and a list of their names. If the specified directory does not exist, it raises a `FileNotFoundError`. The function handles directories with no jQuery files without raising unnecessary errors.\n",
      "**urls.py**\n",
      "```python\n",
      "from django.urls import path\n",
      "from . import views\n",
      "\n",
      "urlpatterns = [\n",
      "    # URL pattern for the index view\n",
      "    path('', views.index, name='index'),\n",
      "    \n",
      "    # URL pattern for the movie detail view\n",
      "    path('movies/<slug:slug>/', views.movie_detail, name='movie_detail'),\n",
      "    \n",
      "    # URL pattern for the movie create view\n",
      "    path('movies/create/', views.movie_create, name='movie_create'),\n",
      "    \n",
      "    # URL pattern for the studio create view\n",
      "    path('studios/create/', views.studio_create, name='studio_create'),\n",
      "    \n",
      "    # URL pattern for the hall create view\n",
      "    path('halls/create/', views.hall_create, name='hall_create'),\n",
      "    \n",
      "    # URL pattern for the country create view\n",
      "    path('countries/create/', views.country_create, name='country_create'),\n",
      "]\n",
      "\n",
      "# URL pattern for serving static files in development\n",
      "if settings.DEBUG:\n",
      "    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\n",
      "```\n",
      "\n",
      "**settings.py**\n",
      "```python\n",
      "# Media settings\n",
      "MEDIA_URL = '/media/'\n",
      "MEDIA_ROOT = os.path.join(BASE_DIR, 'media')\n",
      "```\n",
      "\n",
      "**views.py**\n",
      "```python\n",
      "from django.shortcuts import render, redirect\n",
      "from django.contrib import messages\n",
      "from .models import Movie, Studio, Hall, Country\n",
      "from .forms import MovieForm, StudioForm, HallForm, CountryForm\n",
      "\n",
      "def index(request):\n",
      "    \"\"\"View function to display the list of movies.\"\"\"\n",
      "    movies = Movie.objects.all()\n",
      "    return render(request, 'movies/index.html', {'movies': movies})\n",
      "\n",
      "def movie_detail(request, slug):\n",
      "    \"\"\"View function to display detailed information about a specific movie.\"\"\"\n",
      "    movie = Movie.objects.get(slug=slug)\n",
      "    return render(request, 'movies/movie_detail.html', {'movie': movie})\n",
      "\n",
      "def movie_create(request):\n",
      "    \"\"\"View function to provide a form for creating a new movie entry.\"\"\"\n",
      "    if request.method == 'POST':\n",
      "        form = MovieForm(request.POST, request.FILES)\n",
      "        if form.is_valid():\n",
      "            form.save()\n",
      "            messages.success(request, 'Movie created successfully!')\n",
      "            return redirect('index')\n",
      "    else:\n",
      "        form = MovieForm()\n",
      "    return render(request, 'movies/movie_create.html', {'form': form})\n",
      "\n",
      "def studio_create(request):\n",
      "    \"\"\"View function to provide a form for creating a new studio entry.\"\"\"\n",
      "    if request.method == 'POST':\n",
      "        form = StudioForm(request.POST)\n",
      "        if form.is_valid():\n",
      "            form.save()\n",
      "            messages.success(request, 'Studio created successfully!')\n",
      "            return redirect('index')\n",
      "    else:\n",
      "        form = StudioForm()\n",
      "    return render(request, 'movies/studio_create.html', {'form': form})\n",
      "\n",
      "def hall_create(request):\n",
      "    \"\"\"View function to provide a form for creating a new hall entry.\"\"\"\n",
      "    if request.method == 'POST':\n",
      "        form = HallForm(request.POST)\n",
      "        if form.is_valid():\n",
      "            form.save()\n",
      "            messages.success(request, 'Hall created successfully!')\n",
      "            return redirect('index')\n",
      "    else:\n",
      "        form = HallForm()\n",
      "    return render(request, 'movies/hall_create.html', {'form': form})\n",
      "\n",
      "def country_create(request):\n",
      "    \"\"\"View function to provide a form for creating a new country entry.\"\"\"\n",
      "    if request.method == 'POST':\n",
      "        form = CountryForm(request.POST)\n",
      "        if form.is_valid():\n",
      "            form.save()\n",
      "            messages.success(request, 'Country created successfully!')\n",
      "            return redirect('index')\n",
      "    else:\n",
      "        form = CountryForm()\n",
      "    return render(request, 'movies/country_create.html', {'form': form})\n",
      "```\n",
      "\n",
      "**forms.py**\n",
      "```python\n",
      "from django import forms\n",
      "from .models import Movie, Studio, Hall, Country\n",
      "\n",
      "class MovieForm(forms.ModelForm):\n",
      "    class Meta:\n",
      "        model = Movie\n",
      "        fields = ('title', 'description', 'studio', 'hall', 'country', 'release_date', 'poster')\n",
      "\n",
      "class StudioForm(forms.ModelForm):\n",
      "    class Meta:\n",
      "        model = Studio\n",
      "        fields = ('name', 'description')\n",
      "\n",
      "class HallForm(forms.ModelForm):\n",
      "    class Meta:\n",
      "        model = Hall\n",
      "        fields = ('name', 'description', 'capacity')\n",
      "\n",
      "class CountryForm(forms.ModelForm):\n",
      "    class Meta:\n",
      "        model = Country\n",
      "        fields = ('name', 'description')\n",
      "```\n",
      "\n",
      "**models.py**\n",
      "```python\n",
      "from django.db import models\n",
      "\n",
      "class Movie(models.Model):\n",
      "    title = models.CharField(max_length=255)\n",
      "    description = models.TextField()\n",
      "    studio = models.ForeignKey('Studio', on_delete=models.CASCADE)\n",
      "    hall = models.ForeignKey('Hall', on_delete=models.CASCADE)\n",
      "    country = models.ForeignKey('Country', on_delete=models.CASCADE)\n",
      "    release_date = models.DateField()\n",
      "    poster = models.ImageField(upload_to='posters/')\n",
      "\n",
      "class Studio(models.Model):\n",
      "    name = models.CharField(max_length=255)\n",
      "    description = models.TextField()\n",
      "\n",
      "class Hall(models.Model):\n",
      "    name = models.CharField(max_length=255)\n",
      "    description = models.TextField()\n",
      "    capacity = models.IntegerField()\n",
      "\n",
      "class Country(models.Model):\n",
      "    name = models.CharField(max_length=255)\n",
      "**System Design**\n",
      "\n",
      "We will design a system using Python classes and methods to handle the transition of tender statuses. We will use a dictionary to store the rules for each tender type.\n",
      "\n",
      "```python\n",
      "import logging\n",
      "from dataclasses import dataclass\n",
      "from enum import Enum\n",
      "from typing import Dict, List\n",
      "\n",
      "# Define a logger\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "# Define an Enum for tender statuses\n",
      "class TenderStatus(Enum):\n",
      "    PENDING = 1\n",
      "    QUALIFICATION = 2\n",
      "    AUCTION = 3\n",
      "    UNSUCCESSFUL = 4\n",
      "\n",
      "# Define a dataclass for tender data\n",
      "@dataclass\n",
      "class Tender:\n",
      "    id: int\n",
      "    type: str\n",
      "    bids: int\n",
      "    complaints: int\n",
      "    auction_period: int\n",
      "    lots: List['Lot']\n",
      "\n",
      "# Define a dataclass for lot data\n",
      "@dataclass\n",
      "class Lot:\n",
      "    id: int\n",
      "    auction_period: int\n",
      "\n",
      "# Define a dictionary to store the rules for each tender type\n",
      "rules: Dict[str, Dict[str, Dict[str, str]]] = {\n",
      "    'below-threshold': {\n",
      "        'qualification': {'bids': '>= 1'},\n",
      "        'auction': {'bids': '>= 1', 'complaints': '== 0'},\n",
      "        'unsuccessful': {'bids': '< 1'}\n",
      "    },\n",
      "    'open UA': {\n",
      "        'qualification': {'bids': '>= 1'},\n",
      "        'auction': {'bids': '>= 1', 'complaints': '== 0'},\n",
      "        'unsuccessful': {'bids': '< 1'}\n",
      "    },\n",
      "    'open UA defense': {\n",
      "        'qualification': {'bids': '>= 1'},\n",
      "        'auction': {'bids': '>= 1', 'complaints': '== 0'},\n",
      "        'unsuccessful': {'bids': '< 1'}\n",
      "    }\n",
      "}\n",
      "\n",
      "class TenderManager:\n",
      "    def __init__(self):\n",
      "        self.tenders = {}\n",
      "\n",
      "    def add_tender(self, tender: Tender):\n",
      "        self.tenders[tender.id] = tender\n",
      "\n",
      "    def transition_status(self, tender_id: int, new_status: str):\n",
      "        tender = self.tenders.get(tender_id)\n",
      "        if tender is None:\n",
      "            logger.error(f\"Tender {tender_id} not found\")\n",
      "            return False\n",
      "\n",
      "        # Get the rules for the tender type\n",
      "        rules_for_tender = rules.get(tender.type)\n",
      "        if rules_for_tender is None:\n",
      "            logger.error(f\"Rules for tender type {tender.type} not found\")\n",
      "            return False\n",
      "\n",
      "        # Get the rules for the new status\n",
      "        rules_for_new_status = rules_for_tender.get(new_status)\n",
      "        if rules_for_new_status is None:\n",
      "            logger.error(f\"Rules for new status {new_status} not found\")\n",
      "            return False\n",
      "\n",
      "        # Check if the tender meets the rules for the new status\n",
      "        for field, value in rules_for_new_status.items():\n",
      "            if field == 'bids':\n",
      "                if tender.bids < int(value.split(' ')[1]):\n",
      "                    logger.error(f\"Tender {tender_id} does not meet the rules for {new_status} status\")\n",
      "                    return False\n",
      "            elif field == 'complaints':\n",
      "                if tender.complaints != int(value.split(' ')[1]):\n",
      "                    logger.error(f\"Tender {tender_id} does not meet the rules for {new_status} status\")\n",
      "                    return False\n",
      "            elif field == 'auction_period':\n",
      "                if tender.auction_period != int(value.split(' ')[1]):\n",
      "                    logger.error(f\"Tender {tender_id} does not meet the rules for {new_status} status\")\n",
      "                    return False\n",
      "\n",
      "        # Transition the tender status\n",
      "        tender.status = new_status\n",
      "        logger.info(f\"Tender {tender_id} transitioned to {new_status} status\")\n",
      "\n",
      "        # Set the auction period for tenders without bids\n",
      "        if new_status == 'auction' and tender.bids == 0:\n",
      "            tender.auction_period = 7  # default auction period\n",
      "\n",
      "        # Transition lot statuses\n",
      "        for lot in tender.lots:\n",
      "            if new_status == 'auction':\n",
      "                lot.status = 'auction'\n",
      "                logger.info(f\"Lot {lot.id} transitioned to {new_status} status\")\n",
      "            elif new_status == 'unsuccessful':\n",
      "                lot.status = 'unsuccessful'\n",
      "                logger.info(f\"Lot {lot.id} transitioned to {new_status} status\")\n",
      "\n",
      "        return True\n",
      "```\n",
      "\n",
      "**Unit Tests**\n",
      "\n",
      "We will write unit tests using the `unittest` framework to verify that the system correctly transitions tenders and lots between statuses under various conditions.\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "from tender_manager import TenderManager, Tender, Lot, TenderStatus\n",
      "\n",
      "class TestTenderManager(unittest.TestCase):\n",
      "    def setUp(self):\n",
      "        self.tender_manager = TenderManager()\n",
      "\n",
      "    def test_transition_status(self):\n",
      "        tender = Tender(1, 'below-threshold', 1,\n",
      "```python\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.exceptions import NotFittedError\n",
      "\n",
      "# Constants\n",
      "FEATURES = ['feature '+str(i) for i in range(1, 11)]\n",
      "TARGET = 'target'\n",
      "\n",
      "def task_func(df):\n",
      "    \"\"\"\n",
      "    Train a linear regression model on a given DataFrame.\n",
      "\n",
      "    Args:\n",
      "        df (pd.DataFrame): Input DataFrame.\n",
      "\n",
      "    Returns:\n",
      "        LinearRegression: The trained linear regression model.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the input df is not a DataFrame.\n",
      "    \"\"\"\n",
      "    # Check if input is a DataFrame\n",
      "    if not isinstance(df, pd.DataFrame):\n",
      "        raise ValueError(\"Input must be a pandas DataFrame\")\n",
      "\n",
      "    # Check if the DataFrame contains the required features and target\n",
      "    if not all(feature in df.columns for feature in FEATURES) or TARGET not in df.columns:\n",
      "        raise ValueError(\"DataFrame must contain all features and target\")\n",
      "\n",
      "    # Split the data into features and target\n",
      "    X = df[FEATURES]\n",
      "    y = df[TARGET]\n",
      "\n",
      "    # Split the data into training and testing sets\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "    # Create a pipeline with StandardScaler and LinearRegression\n",
      "    model = make_pipeline(StandardScaler(), LinearRegression())\n",
      "\n",
      "    # Train the model\n",
      "    model.fit(X_train, y_train)\n",
      "\n",
      "    return model\n",
      "```\n",
      "\n",
      "Example usage:\n",
      "\n",
      "```python\n",
      "# Create a sample DataFrame\n",
      "data = {\n",
      "    'feature 1': [1, 2, 3, 4, 5],\n",
      "    'feature 2': [2, 3, 5, 7, 11],\n",
      "    'feature 3': [3, 5, 7, 11, 13],\n",
      "    'feature 4': [4, 6, 8, 10, 12],\n",
      "    'feature 5': [5, 7, 9, 11, 13],\n",
      "    'feature 6': [6, 8, 10, 12, 14],\n",
      "    'feature 7': [7, 9, 11, 13, 15],\n",
      "    'feature 8': [8, 10, 12, 14, 16],\n",
      "    'feature 9': [9, 11, 13, 15, 17],\n",
      "    'feature 10': [10, 12, 14, 16, 18],\n",
      "    'target': [10, 15, 20, 25, 30]\n",
      "}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Train the model\n",
      "model = task_func(df)\n",
      "\n",
      "# Print the model's coefficients\n",
      "print(model.named_steps['linearregression'].coef_)\n",
      "```\n",
      "\n",
      "This code defines a function `task_func` that trains a linear regression model on a given DataFrame. It first checks if the input is a DataFrame and raises a `ValueError` if it's not. Then it splits the data into features and target, and further splits the data into training and testing sets. It creates a pipeline with `StandardScaler` and `LinearRegression`, trains the model, and returns the trained model. The example usage demonstrates how to create a sample DataFrame, train the model, and print the model's coefficients.\n",
      "You can achieve this by passing a dictionary to the `aggfunc` parameter of the `pivot_table` function. This dictionary will map the column names to their respective aggregation functions.\n",
      "\n",
      "Here's the solution:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "np.random.seed(1)\n",
      "df = pd.DataFrame({\n",
      "    'A' : ['abc', 'def', 'xyz', 'abc'] * 3,\n",
      "    'B' : ['A', 'B', 'C'] * 4,\n",
      "    'D' : np.random.randn(12),\n",
      "    'E' : np.random.randn(12)\n",
      "})\n",
      "\n",
      "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
      "\n",
      "print(result)\n",
      "```\n",
      "\n",
      "In this code:\n",
      "\n",
      "- We create a dictionary `{'D': np.sum, 'E': np.mean}` that maps the column names 'D' and 'E' to their respective aggregation functions `np.sum` and `np.mean`.\n",
      "- We pass this dictionary to the `aggfunc` parameter of the `pivot_table` function.\n",
      "- The `pivot_table` function will apply the specified aggregation functions to the respective columns in the pivot table.\n",
      "\n",
      "This will produce a pivot table where the values in column 'D' are summed and the values in column 'E' are averaged.\n",
      "To efficiently combine the dataframes based on timestamps without looping through every row of df2, you can utilize the pandas 'merge_asof' function. Here's the solution:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Create dataframes\n",
      "df1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n",
      "                    'data': [111, 222, 333, 444]})\n",
      "df2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n",
      "                    'stuff': [101, 202, 303, 404, 505]})\n",
      "\n",
      "# Convert Timestamp to datetime\n",
      "df1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\n",
      "df2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\n",
      "\n",
      "# Merge df1 and df2 using merge_asof\n",
      "result = pd.merge_asof(df2, df1, on='Timestamp', direction='forward')\n",
      "\n",
      "# Fill NaN values in 'data' column with None\n",
      "result['data'] = result['data'].fillna(None)\n",
      "\n",
      "# Reorder columns to match the original order of df2\n",
      "result = result[['Timestamp', 'stuff', 'data']]\n",
      "\n",
      "print(result)\n",
      "```\n",
      "\n",
      "This solution first converts the 'Timestamp' column in both dataframes to datetime format. Then, it uses the 'merge_asof' function to merge df1 and df2 based on the 'Timestamp' column. The 'direction' parameter is set to 'forward' to ensure that the resulting dataframe maintains the original order of df2 after merging. Finally, it fills NaN values in the 'data' column with None and reorders the columns to match the original order of df2.\n",
      "\n",
      "The output of this solution will be:\n",
      "\n",
      "```\n",
      "                 Timestamp  stuff   data\n",
      "0 2019-04-02 11:00:14      101   222.0\n",
      "1 2019-04-02 11:00:15      202   222.0\n",
      "2 2019-04-02 11:00:16      303   333.0\n",
      "3 2019-04-02 11:00:30      404   444.0\n",
      "4 2019-04-02 11:00:31      505    NaN\n",
      "```\n",
      "\n",
      "This solution efficiently combines the dataframes based on timestamps without looping through every row of df2, ensuring that the resulting dataframe maintains the original order of df2 after merging and handles cases where there is no matching timestamp in df1 for a timestamp in df2 by filling with None.\n",
      "**Recursive Function to Remove Vowels and Reverse String**\n",
      "\n",
      "```python\n",
      "def remove_vowels_and_reverse(s):\n",
      "    \"\"\"\n",
      "    Removes all lowercase and uppercase vowels from the input string and returns the new string in reverse order.\n",
      "\n",
      "    Args:\n",
      "        s (str): The input string.\n",
      "\n",
      "    Returns:\n",
      "        str: The new string with vowels removed and in reverse order.\n",
      "    \"\"\"\n",
      "    # Base case: If the string is empty, return an empty string.\n",
      "    if not s:\n",
      "        return \"\"\n",
      "\n",
      "    # Check if the first character is a vowel.\n",
      "    if s[0].lower() in \"aeiou\":\n",
      "        # If it's a vowel, recursively call the function on the rest of the string.\n",
      "        return remove_vowels_and_reverse(s[1:])\n",
      "    else:\n",
      "        # If it's not a vowel, recursively call the function on the rest of the string and prepend the current character.\n",
      "        return s[0] + remove_vowels_and_reverse(s[1:])\n",
      "```\n",
      "\n",
      "**Unit Tests**\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "\n",
      "class TestRemoveVowelsAndReverse(unittest.TestCase):\n",
      "    def test_no_vowels(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"hello\"), \"olleh\")\n",
      "\n",
      "    def test_only_vowels(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"aeiou\"), \"\")\n",
      "\n",
      "    def test_special_characters(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"hello!@#$\"), \"!@#$olleh\")\n",
      "\n",
      "    def test_multi_line_string(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"hello\\nworld\"), \"dlrow\\olleh\")\n",
      "\n",
      "    def test_empty_string(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"\"), \"\")\n",
      "\n",
      "    def test_single_character(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"a\"), \"\")\n",
      "\n",
      "    def test_long_string(self):\n",
      "        long_string = \"a\" * 10000\n",
      "        self.assertEqual(remove_vowels_and_reverse(long_string), \"\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    unittest.main()\n",
      "```\n",
      "\n",
      "**Explanation**\n",
      "\n",
      "The `remove_vowels_and_reverse` function uses a recursive approach to remove vowels from the input string and reverse the order of the characters. It checks if the first character of the string is a vowel, and if so, recursively calls the function on the rest of the string. If the first character is not a vowel, it prepends the character to the result of the recursive call.\n",
      "\n",
      "The unit tests cover various cases, including strings with no vowels, strings with only vowels, strings with special characters, multi-line strings, empty strings, single characters, and long strings. The tests ensure that the function handles these cases correctly and efficiently.\n",
      "Here's a Python solution that meets the requirements:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "class InvalidTriangleError(Exception):\n",
      "    \"\"\"Custom exception for invalid triangle.\"\"\"\n",
      "    pass\n",
      "\n",
      "def compute_triangle_area(a, b, c):\n",
      "    \"\"\"\n",
      "    Compute the area of a triangle given its three sides.\n",
      "\n",
      "    Args:\n",
      "        a (int): Length of the first side.\n",
      "        b (int): Length of the second side.\n",
      "        c (int): Length of the third side.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the triangle.\n",
      "\n",
      "    Raises:\n",
      "        InvalidTriangleError: If the given sides cannot form a valid triangle.\n",
      "    \"\"\"\n",
      "    # Check if all input parameters are positive integers\n",
      "    if not (isinstance(a, int) and isinstance(b, int) and isinstance(c, int)):\n",
      "        raise TypeError(\"All sides must be integers.\")\n",
      "    if a <= 0 or b <= 0 or c <= 0:\n",
      "        raise ValueError(\"All sides must be positive.\")\n",
      "\n",
      "    # Check if the sides can form a valid triangle\n",
      "    if a + b <= c or a + c <= b or b + c <= a:\n",
      "        raise InvalidTriangleError(\"Invalid triangle: the given sides cannot form a triangle.\")\n",
      "\n",
      "    # Compute the semi-perimeter\n",
      "    s = (a + b + c) / 2\n",
      "\n",
      "    # Compute the area using Heron's formula\n",
      "    area = math.sqrt(s * (s - a) * (s - b) * (s - c))\n",
      "\n",
      "    return area\n",
      "\n",
      "# Example usage:\n",
      "try:\n",
      "    area = compute_triangle_area(6, 8, 10)\n",
      "    print(area)  # Output: 24.0\n",
      "except InvalidTriangleError as e:\n",
      "    print(e)  # Output: Invalid triangle: the given sides cannot form a triangle.\n",
      "```\n",
      "\n",
      "This solution defines a custom exception `InvalidTriangleError` to handle cases where the given sides cannot form a valid triangle. It then checks if the input parameters are positive integers and if the sides can form a valid triangle using the conditions stated in the problem. If the sides can form a valid triangle, it computes and returns the area of the triangle using Heron's formula.\n",
      "Here's a Python function that uses a single loop iteration to return the third character in the string, ignoring leading and trailing whitespace characters.\n",
      "\n",
      "```python\n",
      "def get_third_char(word):\n",
      "    \"\"\"\n",
      "    Returns the third character in the string, ignoring leading and trailing whitespace characters.\n",
      "\n",
      "    Args:\n",
      "        word (str): The input string.\n",
      "\n",
      "    Returns:\n",
      "        str: The third character in the string.\n",
      "    \"\"\"\n",
      "    # Initialize an index to keep track of the current position in the string\n",
      "    index = 0\n",
      "    \n",
      "    # Iterate over each character in the string\n",
      "    for char in word:\n",
      "        # If the current character is not a whitespace, increment the index\n",
      "        if char != ' ':\n",
      "            index += 1\n",
      "            # If the index is 2, return the current character\n",
      "            if index == 2:\n",
      "                return char\n",
      "\n",
      "# Example usage:\n",
      "word = \"   hello   \"\n",
      "print(get_third_char(word))  # Output: \"l\"\n",
      "```\n",
      "\n",
      "This function works by iterating over each character in the string. It keeps track of the current position in the string using an index. When it encounters a non-whitespace character, it increments the index. If the index reaches 2, it returns the current character, which is the third character in the string. This approach ensures that the function uses a single loop iteration and handles cases where the input string has leading or trailing whitespace characters.\n",
      "Saved output to: ./Llama-3.1-8B-Instruct_results.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "random_state = 42\n",
    "\n",
    "def main(input_csv, output_dir, api_key, model_id, base_url, temperature=0.1, system_prompt=None):\n",
    "    df_ini = pd.read_csv(input_csv)\n",
    "    df = df_ini.sample(10,random_state=random_state).copy()  # Limit to first 10 rows for testing\n",
    "\n",
    "    user_prompts = df[\"combined_instruction\"].tolist()\n",
    "    model_name = model_id.split(\"/\")[-1]\n",
    "    output_path = os.path.join(output_dir, f\"{model_name}_results.jsonl\")\n",
    "    client = LLMClient(\n",
    "        api_key=api_key,\n",
    "        model_id=model_id,\n",
    "        client_type=\"rits\",\n",
    "        base_url=base_url\n",
    "    )\n",
    "\n",
    "    print(f\"Generating responses for {len(user_prompts)} prompts.\")\n",
    "    responses = client.get_model_response_batch(\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompts=user_prompts,\n",
    "        temperature=temperature\n",
    "\n",
    "    )\n",
    "\n",
    "    output_data = []\n",
    "    for row, response in zip(df.to_dict(orient=\"records\"), responses):\n",
    "        row[\"response\"] = response\n",
    "        print(response)\n",
    "        output_data.append(row)\n",
    "\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in output_data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Saved output to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"benchmark_v4.csv\"\n",
    "    output_dir = \".\"\n",
    "    api_key = os.getenv(\"RITS_API_KEY\")\n",
    "    model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    print(api_key)\n",
    "    base_url = \"https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com/llama-3-1-8b-instruct/v1\"\n",
    "    temperature = 0.1\n",
    "    \n",
    "    main(\n",
    "        input_csv=input_csv,\n",
    "        output_dir=output_dir,\n",
    "        api_key=api_key,\n",
    "        model_id=model_id,\n",
    "        base_url=base_url,\n",
    "        temperature=temperature\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6c45c-7149-4f2a-a5ea-c3a2c338d5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
