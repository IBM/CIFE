{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKh3V9eq1qMR"
   },
   "source": [
    "# Step 1 - Get and Prepare Input Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUGmsicmm35R"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "# Number of records we will process\n",
    "N = 60\n",
    "version = \"v3\" # benchmark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6bOFHnf19R2"
   },
   "source": [
    "## Get the Test Dataset for constraint generation from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bD2RxdK3udCJ",
    "outputId": "6689bab2-145c-48b2-8218-aca1509b3827"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/gunnusravani/CIF_Benchmark/refs/heads/main/data/outputs/constraint_category_data.csv -O \"constraint_category_initial_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "idPxEght0JDE",
    "outputId": "beb97a1f-6e57-4712-da46-ab0ca0b8be73"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "full_df = pd.read_csv(\"constraint_category_initial_data.csv\")\n",
    "full_df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cFdw1yNskEhZ",
    "outputId": "2ff5611f-9b59-4fe3-9ec1-6fa3e0b7c324"
   },
   "outputs": [],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XWTBOz040Y6p",
    "outputId": "48500861-fdd9-4608-83f9-89d01c03031d"
   },
   "outputs": [],
   "source": [
    "full_df = full_df[[\"dataset\",\"instruction\",\"code\"]]\n",
    "full_df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GtImSMWypbY8",
    "outputId": "1ef12101-fdfc-46db-95ac-653a18cffbd5"
   },
   "outputs": [],
   "source": [
    "test_df = full_df.sample(n=N)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKUXPyPjfkFy"
   },
   "source": [
    "## Decide Dataset Sampling Ratios Based on Real-World Complexity\n",
    "- ShareGPT - 25%\n",
    "- DS-1000 - 15%\n",
    "- Bigcodebench -25%\n",
    "- MCeval - 35%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/gunnusravani/CIF_Benchmark/refs/heads/main/data/outputs/benchmark_data.csv -O \"benchmark_data_initial.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv(\"./benchmark/benchmark_v1_with_stats.csv\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GixRQJVfkFy"
   },
   "source": [
    "## Write transformation scripts for each dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aq9hochWfkFy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfKpa6YJfkFy"
   },
   "source": [
    "## Create a benchmark dataset of 1000–1200 examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "benchmark_df = pd.read_csv(\"benchmark_data_initial.csv\")\n",
    "benchmark_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Slection For Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = test_df.copy()\n",
    "df = benchmark_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxQC9_FffkFy"
   },
   "source": [
    "# Step 2 - Define Constraint Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYDjA7kifkFy"
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Code Structure and Modularity\",\n",
    "    \"Input and Output Handling\",\n",
    "    \"Error Handling and Robustness\",\n",
    "    \"Data Processing and Transformation\",\n",
    "    \"Performance and Optimization\",\n",
    "    \"Library and API Usage\",\n",
    "    \"Testing and Debugging\",\n",
    "    \"Documentation and Readability\",\n",
    "    \"Security and Privacy\",\n",
    "    \"Reproducibility and Consistency\",\n",
    "    \"Mathematical Computation\",\n",
    "    \"File and Data Management\",\n",
    "    \"UI and Interaction\",\n",
    "]\n",
    "categories_str = \"\\n\".join(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGVmx9sMJEba"
   },
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGGGL3UwJDbQ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_json(string,col_name=\"Constraints\"):\n",
    "    try:\n",
    "        json_string = string.strip().replace('```json\\n', '', 1).replace('\\n```', '', 1)\n",
    "        constraint_json = json.loads(json_string)\n",
    "        return constraint_json.get(col_name, [])\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e} in string: {string}\")\n",
    "        return []\n",
    "    except AttributeError as e:\n",
    "        print(f\"Attribute error: {e} in string: {string}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WC-NjoOU4sRn"
   },
   "source": [
    "# Step 3 Generating Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9p8TwLCgruf",
    "outputId": "96f6ceb8-1ae7-4a30-cc8e-8b9108341075"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bK083pfaBHqY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"/content\"):\n",
    "    from google.colab import userdata\n",
    "    openai_api_key = userdata.get('OPENAPI_KEY')\n",
    "else:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print(\"Running in local environment, loaded key from .env\")\n",
    "\n",
    "# Check if key was actually found\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set. Please check your environment or .env file.\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaCSMGRwpCNv"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are a helpful assistant. You will be given a programming instruction and the corresponding code.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJHT1AtRor37"
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_model_response_batch(user_prompts=None, system_prompt=SYSTEM_PROMPT):\n",
    "    messages_list = [[{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            }, {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }] for user_prompt in user_prompts]\n",
    "    with ThreadPoolExecutor(max_workers=1500) as executor:\n",
    "            response_texts = list(tqdm(\n",
    "                executor.map(lambda messages: get_response_v2(messages), messages_list),\n",
    "                total=len(messages_list),\n",
    "                desc=\"Processing\"\n",
    "            ))\n",
    "    return response_texts\n",
    "\n",
    "def get_response_v2(messages, max_retries=1):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "            )\n",
    "            # print(response)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt+1}: {e}\")\n",
    "            time.sleep(2)\n",
    "    return \"[]\"\n",
    "\n",
    "def get_response(user_prompt,system_prompt=SYSTEM_PROMPT, max_retries=1):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "            )\n",
    "            # print(response)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt+1}: {e}\")\n",
    "            time.sleep(2)\n",
    "    return \"[]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bt5vhZ43-r0R"
   },
   "source": [
    "## Stage 1: Relevant Category Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ob6sN7kiIFN0",
    "outputId": "c244530d-51e1-44ac-ce57-a1a544b98f26",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_STAGE1 = \"\"\"\n",
    "You are a categorization expert for programming tasks. Your goal is to accurately map a programming instruction and its associated code to all applicable high-level constraint categories from a predefined list.\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt_stage1_select_categories(instruction: str, code: str, all_constraint_categories: list[str]) -> str:\n",
    "    categories_str = \"\\n\".join(all_constraint_categories)\n",
    "    prompt = f\"\"\"\n",
    "    \"task\": \"Classify the natural language `instruction` and its corresponding `code` into all relevant high-level constraint categories from the provided comprehensive list.\",\n",
    "    \"context\": \"You are given an `instruction` that describes a coding task, `code` for context, and a list of `all_constraint_categories`.\",\n",
    "    \"goal\": \"Select **all** relevant categories from the `all_constraint_categories` list that apply directly, indirectly, or even potentially to the given `instruction` and `code`.\n",
    "    For each category, think carefully about both explicit requirements and implicit expectations that might arise in real-world programming tasks.\n",
    "    If there is **any plausible reason**—whether due to the instruction's wording, the code's structure, edge cases, potential extensions, or real-world usage scenarios—that the category might impose constraints, then **include it**.\n",
    "\n",
    "    Be exhaustive on the side of inclusion. The goal is to **maximize coverage** of all semantically or practically relevant constraint categories, not to limit selection to a minimal subset.\n",
    "    Your output must be a JSON object containing a single field: 'relevant_categories'.\n",
    "    \"JSON Response Format\": {{\n",
    "        \"relevant_categories\": [\n",
    "            \"List of selected relevant categories from 'all_constraint_categories'. Each string should be the exact name of the matching supercategory.\"\n",
    "        ]\n",
    "    }},\n",
    "    \"Inputs Required\": {{\n",
    "        \"instruction\": {instruction}\n",
    "        \"code\": {code}\n",
    "        \"all_constraint_categories\": {categories_str}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "prompt = get_prompt_stage1_select_categories(full_df.iloc[26][\"instruction\"],full_df.iloc[26][\"code\"],categories)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qBeTjJ3TTLyD",
    "outputId": "27e8c9fe-41d7-46a9-f8d3-698974f4ebe0"
   },
   "outputs": [],
   "source": [
    "print(full_df.iloc[26][\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0k_sB5cDFH0",
    "outputId": "8a7446f5-1f2e-4e6e-903f-7394ef43690d"
   },
   "outputs": [],
   "source": [
    "relevant_categories = get_response(prompt,SYSTEM_PROMPT_STAGE1)\n",
    "print(relevant_categories)\n",
    "relevant_categories = extract_json(relevant_categories,\"relevant_categories\")\n",
    "print(relevant_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHKGZzhisA3O"
   },
   "source": [
    "## Stage 2: Instruction Simplification & Implicit Constraint Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbeKh3Z7_OlU",
    "outputId": "bbc6c294-3df5-46b2-8f87-09726ee8d90f"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_STAGE2 = \"\"\"\n",
    "You are an expert natural language processor and code instruction analyst. Your task is to meticulously read a programming instruction, identify and extract any embedded constraints, separate them from the core problem description, and map them to provided high-level categories. You must preserve any provided starter code or solution snippets.\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt_stage2_extract_and_simplify(instruction: str, relevant_categories: list[str]) -> str:\n",
    "    relevant_categories_str = \", \".join(relevant_categories)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    \"task\": \"Carefully analyze the provided programming `instruction`. Your main goal is to separate the core problem description from any explicit constraints or directives embedded within the instruction's text. You must return the core problem description as 'simplified_instruction' and all identified explicit constraints as 'extracted_constraints'.\",\n",
    "\n",
    "    \"context\": \"You are provided with an `instruction` that describes a coding task and a list of `relevant_categories` that apply to this instruction. The `instruction` might contain specific rules, formatting requirements, or implementation details interwoven into its narrative. You need to pull these out.\",\n",
    "\n",
    "    \"goal\": \"Perform the following steps:\n",
    "    1. **Identify and Extract Explicit Constraints:** Read the `instruction` and pinpoint all **explicit** constraints, directives, or specific requirements (e.g., about function names, return types, error handling, algorithms to use, variable naming, documentation, etc.).\n",
    "    2. **Split Compound Constraints into Atomic Ones:** If any sentence or clause contains multiple distinct requirements, split it into **multiple entries**, ensuring **each extracted constraint refers to exactly one requirement or condition**.\n",
    "       Do not combine multiple checks (e.g., multiple `if` conditions or causes for `ValueError`) into one constraint.\n",
    "    3. **Simplify Instruction:** Create a 'simplified_instruction' by removing only these identified explicit constraints, leaving behind the core problem explanation.\n",
    "        **CRITICAL:** If the original `instruction` contains any starter code (e.g., code in ```python blocks) or any solution snippets demarcated by 'BEGIN SOLUTION', you **MUST NOT** modify or remove these from the 'simplified_instruction'.\n",
    "        **Important:** If the original instruction contains any explanation or examples that are not constraints, you should keep them in the 'simplified_instruction' as they are part of the core problem description.\n",
    "    4. **Format Extracted Constraints with Categories:** For each extracted constraint:\n",
    "        - Include a separate item in the 'extracted_constraints' list.\n",
    "        - Set its 'type' to one of the categories from `relevant_categories`.\n",
    "        - Set 'instruction_part' to 'Extracted from instruction'.\n",
    "\n",
    "    Example:\n",
    "    - Original compound constraint: \"Raise ValueError if input is None or if it's an empty list.\"\n",
    "    - Extracted as:\n",
    "        {{\n",
    "            \"type\": \"Error Handling and Robustness\",\n",
    "            \"constraint\": \"Raise ValueError if input is None.\",\n",
    "            \"instruction_part\": \"Extracted from instruction\"\n",
    "        }},\n",
    "        {{\n",
    "            \"type\": \"Error Handling and Robustness\",\n",
    "            \"constraint\": \"Raise ValueError if input is an empty list.\",\n",
    "            \"instruction_part\": \"Extracted from instruction\"\n",
    "        }}\n",
    "\n",
    "    Your output must be a JSON object with two fields: 'simplified_instruction' and 'extracted_constraints'.\n",
    "    \",\n",
    "\n",
    "    \"JSON Response Format\": {{\n",
    "        \"simplified_instruction\": \"The core problem description from the original instruction, with explicit constraints/directives removed. Starter code (e.g., ```python blocks) and solution snippets (e.g., marked by 'BEGIN SOLUTION') must be preserved.\",\n",
    "        \"extracted_constraints\": [\n",
    "            {{\n",
    "                \"type\": \"Constraint_Category_Name\",  // MUST be one of the provided relevant_categories\n",
    "                \"constraint\": \"A single, atomic extracted constraint.\",\n",
    "                \"instruction_part\": \"Extracted from instruction\"\n",
    "            }}\n",
    "        ]\n",
    "    }},\n",
    "\n",
    "    \"Inputs Required\": {{\n",
    "        \"instruction\": {instruction}\n",
    "        \"relevant_categories\": {relevant_categories_str}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "prompt_stage2 = get_prompt_stage2_extract_and_simplify(full_df.iloc[26][\"instruction\"],relevant_categories)\n",
    "print(prompt_stage2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHfpMLDsRkjZ",
    "outputId": "79f17444-a43c-471f-b5f8-16b89262dbe2"
   },
   "outputs": [],
   "source": [
    "instruction2 = \"Write a function that sorts a list of numbers using merge sort with time complexity O(n log n).\"\n",
    "code2 = \"def merge_sort(arr):\\n    if len(arr) > 1:\\n        mid = len(arr) // 2\\n        left_half = arr[:mid]\\n        right_half = arr[mid:]\\n\\n        merge_sort(left_half)\\n        merge_sort(right_half)\\n\\n        i = j = k = 0\\n\\n        while i < len(left_half) and j < len(right_half):\\n            if left_half[i] < right_half[j]:\\n                arr[k] = left_half[i]\\  i += 1\\n            else:\\n                arr[k] = right_half[j]\\  j += 1\\n            k += 1\\n\\n        while i < len(left_half):\\n            arr[k] = left_half[i]\\  i += 1\\  k += 1\\n\\n        while j < len(right_half):\\n            arr[k] = right_half[j]\\  j += 1\\  k += 1\\n\\n    return arr\"\n",
    "example_prompt2 = get_prompt_stage2_extract_and_simplify(instruction2,relevant_categories)\n",
    "print(example_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7LO1PWwP19q",
    "outputId": "09518435-713a-484b-9e34-a0dda03e0a8a"
   },
   "outputs": [],
   "source": [
    "response = get_response(example_prompt2,SYSTEM_PROMPT_STAGE2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"Write a Python function `rotate_text` that takes a string `text` and an integer `rotation` as its parameters and returns a new string where each letter in the original string is rotated by the given rotation amount through the alphabet. The rotation should maintain the case of the original letters (i.e., uppercase letters remain uppercase, and lowercase letters remain lowercase) and should leave non-alphabetic characters unchanged.\n",
    "\n",
    "For example, with a rotation of 1, 'a' becomes 'b', 'z' becomes 'a', 'A' becomes 'B', and 'Z' becomes 'A'. The function should handle negative rotation values as well, which rotate the letters in the opposite direction.\n",
    "\"\"\"\n",
    "relevant_categories = ['Input and Output Handling', 'Code Structure and Modularity', 'Scalability and Maintainability']\n",
    "example_prompt = get_prompt_stage2_extract_and_simplify(instruction,relevant_categories)\n",
    "response = get_response(example_prompt,SYSTEM_PROMPT_STAGE2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"Edit the given code to fix the bug and increase the difficulty:\n",
    "\n",
    "```python\n",
    "def calculate_tax(price):\n",
    "    tax_rate = 0.20\n",
    "    total = price + (price * tax_rate)\n",
    "    \n",
    "    # Apply a discount of 10% if the price is greater than $1000\n",
    "    if price > 1000:\n",
    "        discount = price * 0.10\n",
    "        total -= discount\n",
    "    \n",
    "    # Round the total to the nearest cent\n",
    "    total = round(total, 2)\n",
    "    \n",
    "    # Add a surcharge of 5% if the total is greater than $500 and less than $1000\n",
    "    if total > 500 and total < 1000:\n",
    "        surcharge = total * 0.05\n",
    "        total += surcharge\n",
    "    \n",
    "    # Add a service fee of $50 if the total is less than $100\n",
    "    if total < 100:\n",
    "        total += 50\n",
    "    \n",
    "    return total\n",
    "```\n",
    "\n",
    "Explanation of changes:\n",
    "\n",
    "1. Added an additional condition to check if the total is greater than $500 and less than $1000. If it is, a surcharge of 5% is added to the total.\n",
    "2. This additional condition adds complexity to the function, requiring the programmer to consider multiple scenarios and perform additional calculations.\"\"\"\n",
    "relevant_categories = ['Code Structure and Modularity', 'Input and Output Handling', 'Documentation and Readability']\n",
    "example_prompt = get_prompt_stage2_extract_and_simplify(instruction,relevant_categories)\n",
    "response = get_response(example_prompt,SYSTEM_PROMPT_STAGE2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"Problem:\n",
    "I have two csr_matrix, c1, c2.\n",
    "\n",
    "I want a new matrix Feature = [c1, c2]. But if I directly concatenate them horizontally this way, there's an error that says the matrix Feature is a list. How can I achieve the matrix concatenation and still get the same type of matrix, i.e. a csr_matrix?\n",
    "\n",
    "And it doesn't work if I do this after the concatenation: Feature = csr_matrix(Feature) It gives the error:\n",
    "\n",
    "Traceback (most recent call last):\n",
    "  File \"yelpfilter.py\", line 91, in <module>\n",
    "    Feature = csr_matrix(Feature)\n",
    "  File \"c:\\python27\\lib\\site-packages\\scipy\\sparse\\compressed.py\", line 66, in __init__\n",
    "    self._set_self( self.__class__(coo_matrix(arg1, dtype=dtype)) )\n",
    "  File \"c:\\python27\\lib\\site-packages\\scipy\\sparse\\coo.py\", line 185, in __init__\n",
    "    self.row, self.col = M.nonzero()\n",
    "TypeError: __nonzero__ should return bool or int, returned numpy.bool_\n",
    "\n",
    "A:\n",
    "<code>\n",
    "from scipy import sparse\n",
    "c1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\n",
    "c2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n",
    "</code>\n",
    "Feature = ... # put solution in this variable\n",
    "BEGIN SOLUTION\n",
    "<code>\"\"\"\n",
    "relevant_categories = ['Code Structure and Modularity', 'Input and Output Handling', 'Error Handling and Robustness', 'Data Processing and Transformation', 'Library and API Usage', 'Testing and Debugging', 'Documentation and Readability', 'Reproducibility and Consistency', 'Mathematical Computation']\n",
    "example_prompt = get_prompt_stage2_extract_and_simplify(instruction,relevant_categories)\n",
    "response = get_response(example_prompt,SYSTEM_PROMPT_STAGE2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FB_TYpTPdOPg",
    "outputId": "277f2a28-c52d-4738-8b25-933b49dc2929"
   },
   "outputs": [],
   "source": [
    "response_stage2 = get_response(prompt_stage2,SYSTEM_PROMPT_STAGE2)\n",
    "print(response_stage2)\n",
    "simplified_instruction = extract_json(response_stage2,\"simplified_instruction\")\n",
    "extracted_constraints = extract_json(response_stage2,\"extracted_constraints\")\n",
    "print(simplified_instruction)\n",
    "print(extracted_constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4g4NyWUtdJgH"
   },
   "source": [
    "##   Stage 3: New Constraint Generation & Merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZcJ6DMwfFan",
    "outputId": "dccc0d47-c545-4023-b939-c0270bd4d9dd"
   },
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT_STAGE3_NEW = \"\"\"\n",
    "You are a highly skilled constraint curator and generator for code development tasks. Your job is to combine existing constraints with newly generated ones, ensuring the final list is comprehensive, free of conflicts and duplicates, and adheres to strict quality and relevance standards.\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt_stage3_generate_and_merge_constraints(\n",
    "    instruction: str,\n",
    "    code: str,\n",
    "    relevant_categories: list[str],\n",
    "    extracted_constraints: list # These are from Stage 2 output\n",
    ") -> str:\n",
    "    relevant_categories_str = \", \".join(relevant_categories)\n",
    "    extracted_constraints_str = json.dumps(extracted_constraints, indent=2)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    \"task\": \"Generate additional high-quality constraints, combine them with provided `extracted_constraints`, and refine the final comprehensive list based on quality principles and category alignment.\",\n",
    "\n",
    "    \"context\": \"You are provided with a `instruction` (core problem), `code` for context, a list of `relevant_categories` for this task, and `extracted_constraints` that were found directly in the original instruction. Your job is to build the definitive, final set of constraints for this benchmark entry.\",\n",
    "\n",
    "    \"goal\": \"Perform the following steps:\n",
    "    1.  **Generate New Constraints:** Based on the `instruction`, `code`, and `relevant_categories`, generate 5 to 10 **new** natural language constraints. These new constraints must be specific, objective, and leave little room for subjective interpretation. Each should pose a tangible challenge and be clearly verifiable. For each newly generated constraint, set its `instruction_part` to 'Newly Generated'.\n",
    "        **Note:** If the combined total with extracted constraints exceeds 15, generate fewer new ones; if it falls short of 5, generate more to meet the minimum.\n",
    "    2.  **Combine and Curate:** Create a single preliminary list of all constraints by merging the `extracted_constraints` with your newly generated constraints.\n",
    "    3.  **Refine and Resolve Conflicts/Duplicates:**\n",
    "        * Review the combined list for any duplicate constraints (exact text or very similar meaning) or any subtle contradictions between extracted and newly generated constraints.\n",
    "        * **Crucially, if a newly generated constraint is a duplicate of, or is semantically covered by, an 'Extracted from instruction' constraint, you MUST discard the newly generated constraint in favor of the extracted one. Do NOT change or tweak the 'Extracted from instruction' constraints.**\n",
    "        * Resolve other conflicts by refining or merging existing constraints, prioritizing clarity and adhering to the 'Prioritize Modification over Deletion' principle.\n",
    "    4.  **Finalize Count (5-15 Constraints):** Ensure the final list contains between **5 and 15** unique, valid constraints.\n",
    "        * If the curated list has less than 5, identify which of the `relevant_categories` are underrepresented and create *additional* high-quality constraints for those categories to reach the minimum.\n",
    "        * If the curated list has more than 15, select the most critical and impactful constraints that best cover the `relevant_categories` to reduce the list to 15, without losing essential information.\n",
    "    5.  **Validate Category Alignment:** For every constraint in the final list, ensure its 'type' field is one of the categories explicitly listed in the `relevant_categories` list. If a type does not perfectly match, correct it to the most appropriate category from `relevant_categories`. There should be a perfect overlap.\n",
    "\n",
    "    **Principles for All Constraints (Extracted & Newly Generated):**\n",
    "        * **Actionable, Precise, Objective:** Ensure all constraints are specific, measurable, and unambiguous.\n",
    "        * **Accept Requirements for Additional Code:** If meeting a constraint primarily requires generating *additional* code or functionality (rather than fundamentally altering the core solution), ensure the constraint is phrased to encourage this.\n",
    "        * **No Unresolved Directives:** Ensure the final generated constraints do not contain any unreplaced directive keywords (e.g., `{{keyword}}`).\n",
    "\n",
    "    Your output should be a JSON object containing a single field: 'final_comprehensive_constraints'.\n",
    "    \",\n",
    "\n",
    "    \"JSON Response Format\": {{\n",
    "        \"final_comprehensive_constraints\": [\n",
    "            {{\n",
    "                \"type\": \"Constraint_Category_Name\", // MUST be from the 'relevant_categories' list\n",
    "                \"constraint\": \"Final, specific, objective, and atomic constraint statement.\",\n",
    "                \"instruction_part\": \"Original source: 'Extracted from instruction' or 'Newly Generated' or 'Combined/Refined'\"\n",
    "            }},\n",
    "            // ... list of 5 to 15 curated constraints\n",
    "        ]\n",
    "    }},\n",
    "\n",
    "    \"Inputs Required\": {{\n",
    "        \"instruction\": {instruction},\n",
    "        \"code\": {code},\n",
    "        \"relevant_categories\": {relevant_categories_str},\n",
    "        \"extracted_constraints\": {extracted_constraints_str}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "prompt_stage3 = get_prompt_stage3_generate_and_merge_constraints(full_df.iloc[26][\"instruction\"],full_df.iloc[26][\"code\"],relevant_categories,extracted_constraints)\n",
    "print(prompt_stage3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kUl7v77Zwik3",
    "outputId": "31252fe2-5b91-4ea9-9407-cbf9ec21ff9b"
   },
   "outputs": [],
   "source": [
    "response_stage3 = get_response(prompt_stage3,SYSTEM_PROMPT_STAGE3_NEW)\n",
    "print(response_stage3)\n",
    "final_comprehensive_constraints = extract_json(response_stage3,\"final_comprehensive_constraints\")\n",
    "print(final_comprehensive_constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mAixJw5bNLp"
   },
   "source": [
    "## Stage 4: Final Relevance Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWyGpscMRlIS",
    "outputId": "19e8766b-394c-4274-d16c-f854fae4374e"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_STAGE4_FINAL_RELEVANCE_FILTER = \"\"\"\n",
    "You are a meticulous validator of programming task specifications. Your role is to critically assess each constraint for relevance to a simplified instruction’s core intent, and then provide a refined list of only the relevant constraints.\n",
    "\n",
    "You must generate a reasoning statement for each constraint first, and then decide whether the constraint is relevant based on that reasoning. Do not make a binary decision without a clear explanation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_prompt_stage4_final_relevance_and_filter(instruction: str, final_comprehensive_constraints: list) -> str:\n",
    "    import json\n",
    "    constraints_str = json.dumps(final_comprehensive_constraints, indent=2)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "\"task\": \"Critically evaluate each constraint in the `final_comprehensive_constraints` list against the `instruction` to determine its direct relevance. For each constraint, first write a reasoning statement explaining your judgment. Then, based on the reasoning, assign a boolean `is_relevant` value. Finally, provide a new list containing only the relevant constraints.\",\n",
    "\n",
    "\"context\": \"You are provided with a `instruction` and a list of `final_comprehensive_constraints` that are intended to guide code generation for this task. Your job is to assess each constraint's logical consistency and direct relevance to the simplified core problem.\",\n",
    "\n",
    "\"goal\": \"Perform the following steps:\n",
    "1. **Understand the Instruction:** Read and understand the core task described in the `instruction`.\n",
    "2. **Evaluate Constraints with Justification:**\n",
    "   a. For each constraint:\n",
    "     - Write a reasoning paragraph explaining whether and how the constraint supports, complements, or conflicts with the `instruction`.\n",
    "     - Then, based on your reasoning, assign a boolean value `is_relevant`.\n",
    "     - If the constraint is vague, off-topic, too generic, or in conflict with the instruction, mark it as false.\n",
    "     - If the constraint is about documentation, include it only if it directly supports the core problem or is necessary for understanding the code's functionality. \n",
    "3. **Filter the Relevant Ones:** Build a new list containing only those constraints with `is_relevant: true`.\n",
    "4. **Explain Exclusions:** At the end, explain which constraints were removed and why (summary).\n",
    "\n",
    "Output Format:\n",
    "\n",
    "Return a single JSON object with the following structure:\n",
    "\n",
    "{{\n",
    "  \"evaluated_constraints\": [\n",
    "    {{\n",
    "      \"constraint\": \"The original constraint text.\",\n",
    "      \"reasoning\": \"Detailed explanation of why this constraint is or is not relevant.\",\n",
    "      \"is_relevant\": true // or false, based on the reasoning\n",
    "    }},\n",
    "    // one per constraint\n",
    "  ],\n",
    "  \"filtered_relevant_constraints\": [\n",
    "    {{\n",
    "      \"type\": \"Constraint_Category_Name\",\n",
    "      \"constraint\": \"Relevant constraint statement.\",\n",
    "      \"instruction_part\": \"Original source: 'Extracted from instruction', 'Newly Generated', or 'Combined/Refined'\"\n",
    "    }}\n",
    "    // only constraints marked relevant\n",
    "  ],\n",
    "  \"reasoning_for_removal\": \"Summary of why certain constraints were removed.\"\n",
    "}}\n",
    "\n",
    "Inputs Required:\n",
    "- instruction: {instruction}\n",
    "- final_comprehensive_constraints: {constraints_str}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "prompt_stage4 = get_prompt_stage4_final_relevance_and_filter(full_df.iloc[26][\"instruction\"],final_comprehensive_constraints)\n",
    "print(prompt_stage4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWIB22mcRvdF",
    "outputId": "a16a787e-c873-462b-815a-958b2c04ec22"
   },
   "outputs": [],
   "source": [
    "response_stage4 = get_response(prompt_stage4,SYSTEM_PROMPT_STAGE4_FINAL_RELEVANCE_FILTER)\n",
    "print(response_stage4)\n",
    "final_final_constraints = extract_json(response_stage4,\"filtered_relevant_constraints\")\n",
    "print(final_final_constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5F0sALbfMoD"
   },
   "source": [
    "## Implemeting the 4 step process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IGOMyDAfWse"
   },
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9EGEKwMfbDW",
    "outputId": "439a071a-f682-497a-a8a4-1f6ea7737c59"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "if os.path.exists(\"/content\"):\n",
    "    output_path = \"./\"\n",
    "else:\n",
    "    root = \"../../\"\n",
    "    output_path = root+\"data/benchmark/\"+version+\"/\"\n",
    "    folder = Path(output_path)\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlSgehzDfclQ"
   },
   "source": [
    "### Implementing stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCrgLJ5VBPtc"
   },
   "outputs": [],
   "source": [
    "# Run on first 20 rows (you can change this limit)\n",
    "def map_categories(df,output_pth,input_col1,input_col2,output_col,categories=categories):\n",
    "    results = []\n",
    "    prompts = []\n",
    "    for i, row in df.iterrows():\n",
    "        # print(f\"Processing row {i}\")\n",
    "        prompt = get_prompt_stage1_select_categories(row[input_col1], row[input_col2], categories)\n",
    "        prompts.append(prompt)\n",
    "    categories_list = get_model_response_batch(prompts,SYSTEM_PROMPT_STAGE1)\n",
    "\n",
    "    \n",
    "    categories_list = [extract_json(categories,\"relevant_categories\") for categories in categories_list]\n",
    "    results = categories_list\n",
    "\n",
    "    df[output_col] = results\n",
    "    df.to_csv(output_pth, index=False)\n",
    "    print(\"Saved the file successfully\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8mRcfYnxfJX_",
    "outputId": "da06d934-05f4-4a90-c252-c3e93e50abf8"
   },
   "outputs": [],
   "source": [
    "map_categories(df,output_path+\"step1_with_categories.csv\",\"instruction\",\"code\",\"relevant_categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "a709b720",
    "outputId": "415b4c71-a3d2-4f88-bc76-3479002830cf"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "all_categories = [category for sublist in df['relevant_categories'] for category in sublist]\n",
    "category_counts = Counter(all_categories)\n",
    "category_df = pd.DataFrame.from_dict(category_counts, orient='index', columns=['Count'])\n",
    "category_df = category_df.sort_values(by='Count', ascending=False)\n",
    "display(category_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LU3_Pe0ngM6N"
   },
   "source": [
    "### Implementing stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjDpLCS3mvCl",
    "outputId": "5491565d-8963-4abc-bd48-061ee3a5860b"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Ald7GFkCadK"
   },
   "outputs": [],
   "source": [
    "def extract_and_simplify(df,output_pth,input_col1,input_col2):\n",
    "    extracted_constraints = []\n",
    "    prompts = []\n",
    "    for _,row in df.iterrows():\n",
    "        # print(f\"Processing row {i}\")\n",
    "        prompt = get_prompt_stage2_extract_and_simplify(row[input_col1], row[input_col2])\n",
    "        prompts.append(prompt)\n",
    "    responses = get_model_response_batch(prompts,SYSTEM_PROMPT_STAGE2)\n",
    "    simplified_instructions = [extract_json(response,\"simplified_instruction\") for response in responses]\n",
    "    extracted_constraints = [extract_json(response,\"extracted_constraints\") for response in responses]\n",
    "    df[\"simplified_instruction\"] = simplified_instructions\n",
    "    df[\"extracted_constraints\"] = extracted_constraints\n",
    "\n",
    "    df.to_csv(output_pth, index=False)\n",
    "    print(\"Saved the file successfully\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNXyD5VIhYDn",
    "outputId": "71f8d1fc-d28f-468b-c484-2cf1e1fd4dd2"
   },
   "outputs": [],
   "source": [
    "extract_and_simplify(df,output_path+\"step2_with_simplified_instruction_and_constraints.csv\",\"instruction\",\"relevant_categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syq5kg7wox-7"
   },
   "source": [
    "### Implementing stage 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZurSMxgaoxa9"
   },
   "outputs": [],
   "source": [
    "def generate_new_constraints(df,output_pth,input_col1,input_col2,input_col3,input_col4):\n",
    "    prompts = []\n",
    "    for _, row in df.iterrows():\n",
    "        # print(f\"Processing row {i}\")\n",
    "        prompt = get_prompt_stage3_generate_and_merge_constraints(row[input_col1], row[input_col2], row[input_col3], row[input_col4])\n",
    "        prompts.append(prompt)\n",
    "    responses = get_model_response_batch(prompts,SYSTEM_PROMPT_STAGE3_NEW)\n",
    "    final_constraints = [extract_json(response,\"final_comprehensive_constraints\") for response in responses]\n",
    "    df[\"final_comprehensive_constraints\"] = final_constraints\n",
    "    df.to_csv(output_pth, index=False)\n",
    "    print(\"Saved the file successfully\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yXuXUVNpD-J"
   },
   "outputs": [],
   "source": [
    "generate_new_constraints(df,output_path+\"step3_with_final_comprehensive_constraints.csv\",\"instruction\",\"code\",\"relevant_categories\",\"extracted_constraints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pU93RkK9miqo"
   },
   "source": [
    "### Impementing the Stage 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RV49wOCqUr-"
   },
   "outputs": [],
   "source": [
    "def final_relevance_filter(df,output_pth,input_col1,input_col2):\n",
    "    results = []\n",
    "    prompts = []\n",
    "    for _, row in df.iterrows():\n",
    "        # print(f\"Processing row {i}\")\n",
    "        prompt = get_prompt_stage4_final_relevance_and_filter(row[input_col1], row[input_col2])\n",
    "        prompts.append(prompt)\n",
    "    responses = get_model_response_batch(prompts,SYSTEM_PROMPT_STAGE4_FINAL_RELEVANCE_FILTER)\n",
    "    df[\"filtered_relevant_constraints\"] = [extract_json(response,\"filtered_relevant_constraints\") for response in responses]\n",
    "    df.to_csv(output_pth, index=False)\n",
    "    print(\"Saved the file successfully\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EfgYUXCCqf3m",
    "outputId": "7f2a6898-2962-47a9-f49a-8256e7983264"
   },
   "outputs": [],
   "source": [
    "final_relevance_filter(df,output_path+\"step4_with_filtered_relevant_constraints.csv\",\"instruction\",\"final_comprehensive_constraints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSuK7L4sN0I0",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Validating the Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmutP4qhN3oy"
   },
   "outputs": [],
   "source": [
    "# SYSTEM_PROMPT for the Quality Judge LLM\n",
    "SYSTEM_PROMPT_QUALITY_JUDGE = \"\"\"\n",
    "You are an expert in meticulously evaluating the quality of programming constraints.\n",
    "Your task is to objectively assess a provided list of generated constraints based on strict quality criteria.\n",
    "For each constraint, you will provide specific scores for Atomicity, Relevance, and Objectivity, along with detailed reasoning and suggestions for improvement.\n",
    "Finally, you will synthesize these individual evaluations into a unified quality score and an overall analysis for the entire set of constraints.\n",
    "Your judgment must be impartial and directly tied to the definitions provided.\n",
    "\"\"\"\n",
    "\n",
    "# User-facing prompt for the Quality Judge LLM\n",
    "def get_quality_judge_prompt(original_instruction, original_code, generated_constraint_list):\n",
    "    return f\"\"\"\n",
    "Original Instruction: {original_instruction}\n",
    "\n",
    "Original Code (for context, if available):\n",
    "```python\n",
    "{original_code}\n",
    "```\n",
    "\n",
    "List of Generated Constraints to Evaluate:\n",
    "{generated_constraint_list}\n",
    "\n",
    "Constraint Quality Criteria:\n",
    "Each score ranges from 1 (poor) to 5 (excellent).\n",
    "\n",
    "Atomicity Score (1–5):\n",
    "Definition: Measures whether a constraint expresses exactly one indivisible requirement.\n",
    "An atomic constraint should not contain multiple independent rules or directives.\n",
    "Examples:\n",
    "- 1 (Non-Atomic): \"Return a float and raise ValueError for invalid input.\"\n",
    "- 5 (Atomic): \"Raise ValueError for invalid input.\"\n",
    "\n",
    "Relevance Score (1–5):\n",
    "Definition: Measures how well the constraint aligns with the core task described in the instruction.\n",
    "A relevant constraint addresses functionality, behavior, or structure directly related to the task.\n",
    "Examples:\n",
    "- 1 (Off-topic): \"Avoid using global variables.\"\n",
    "- 5 (Directly relevant): \"Raise ValueError if the input DataFrame is empty.\"\n",
    "\n",
    "Objectivity Score (1–5):\n",
    "Definition: Measures whether the constraint can be evaluated without personal interpretation or judgment.\n",
    "An objective constraint uses measurable, observable, or clearly definable criteria.\n",
    "Examples:\n",
    "- 1 (Subjective): \"The code should be intuitive and clean.\"\n",
    "- 5 (Objective): \"The function must return a list of integers.\"\n",
    "\n",
    "Output Format:\n",
    "Your response must be a single JSON object.\n",
    "\n",
    "{{\n",
    "  \"constraint_evaluations\": [\n",
    "    {{\n",
    "      \"constraint_text\": \"The exact text of the constraint from the input list.\",\n",
    "      \"atomicity_score\": int,      // Score from 1 to 5\n",
    "      \"relevance_score\": int,      // Score from 1 to 5\n",
    "      \"objectivity_score\": int,    // Score from 1 to 5\n",
    "      \"reasoning\": \"Detailed explanation for each score and, if any scores are low, suggestions for improving atomicity, relevance, or objectivity.\"\n",
    "    }}\n",
    "    // ... for each constraint in the input list\n",
    "  ],\n",
    "  \"avg_atomicity\": float,         // Average atomicity score\n",
    "  \"avg_relevance\": float,         // Average relevance score\n",
    "  \"avg_objectivity\": float,       // Average objectivity score\n",
    "  \"unified_quality_score\": float, // Average of the three above\n",
    "  \"overall_analysis\": \"A summary of the overall quality of the constraint set, including strengths and weaknesses.\"\n",
    "}}\"\"\"\n",
    "\n",
    "prompt = get_quality_judge_prompt(test_df.iloc[509][\"instruction\"],test_df.iloc[509][\"code\"],test_df.iloc[509][\"final_comprehensive_constraints\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ljhcp_ElO8mA"
   },
   "outputs": [],
   "source": [
    "response = get_response(prompt,SYSTEM_PROMPT_QUALITY_JUDGE)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ev9o_p-QtD9"
   },
   "outputs": [],
   "source": [
    "def get_dict(response):\n",
    "    try:\n",
    "        json_string = response.strip().replace('```json\\n', '', 1).replace('\\n```', '', 1)\n",
    "        dict_json = json.loads(json_string)\n",
    "        return dict_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e} in string: {response}\")\n",
    "        return []\n",
    "    except AttributeError as e:\n",
    "        print(f\"Attribute error: {e} in string: {response}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1zFWoZsR0pp"
   },
   "outputs": [],
   "source": [
    "# extract the final scores and keep them in separated 4 columns\n",
    "def extract_quality_scores(df,score_column=\"quality_scores\"):\n",
    "    df[\"relevance_score\"] = df[score_column].apply(lambda x: x.get(\"avg_relevance\", 0) if isinstance(x, dict) else 0)\n",
    "    df[\"objectivity_score\"] = df[score_column].apply(lambda x: x.get(\"avg_objectivity\", 0) if isinstance(x, dict) else 0)\n",
    "    df[\"atomicity_score\"] = df[score_column].apply(lambda x: x.get(\"avg_atomicity\", 0) if isinstance(x, dict) else 0)\n",
    "    df[\"unified_quality_score\"] = df[score_column].apply(lambda x: x.get(\"unified_quality_score\", 0) if isinstance(x, dict) else 0)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68_Tmk0VO8gm"
   },
   "outputs": [],
   "source": [
    "def measure_constraints(df,output_pth,input_col1,input_col2,input_col3,output_col):\n",
    "    results = []\n",
    "    prompts = []\n",
    "    for _, row in df.iterrows():\n",
    "        # print(f\"Processing row {i}\")\n",
    "        prompt = get_quality_judge_prompt(row[input_col1], row[input_col2], row[input_col3])\n",
    "        prompts.append(prompt)\n",
    "    responses = get_model_response_batch(prompts,SYSTEM_PROMPT_QUALITY_JUDGE)\n",
    "    # print(responses)\n",
    "    results = [get_dict(response) for response in responses]\n",
    "    df[output_col] = results\n",
    "    df = extract_quality_scores(df)\n",
    "    print(df.head())\n",
    "    df.to_csv(output_pth, index=False)\n",
    "    print(\"Saved the file successfully\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huk0oPYGRJ6H"
   },
   "outputs": [],
   "source": [
    "df = measure_constraints(df,output_path+\"step5_with_quality_scores.csv\",\"instruction\",\"code\",\"filtered_relevant_constraints\",\"quality_scores\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(columns=[\"\",\"quality_scores\",\"unified_quality_score\"])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(columns=['quality_scores', 'specificity_score', 'objectivity_score',\n",
    "#        'atomicity_score', 'unified_quality_score', 'relevance_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the Instruction and Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bdd4aa8"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "n = 375\n",
    "# n=12\n",
    "# n = 11\n",
    "\n",
    "original_instruction = df.iloc[n][\"instruction\"]\n",
    "new_constraints = df.iloc[n][\"filtered_relevant_constraints\"]\n",
    "\n",
    "#filter new_constraints on 'instruction_part': 'Newly Generated'\n",
    "def filter_new_constraints(constraints):\n",
    "    constraints = ast.literal_eval(constraints)\n",
    "    return [c[\"constraint\"] for c in constraints if c.get(\"instruction_part\") == \"Newly Generated\"]\n",
    "\n",
    "new_constraints = filter_new_constraints(new_constraints)\n",
    "print(f\"Original Instruction: {original_instruction}\")\n",
    "print(\"New Constraints:\", new_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def insert_constraints_prompt(original_instruction,new_constraints):\n",
    "    prompt = f\"\"\" Context: I have instruction for LLM to generate response and I also have some set of conditions for the model to follow while generating the response related to the instruction.\n",
    "    Task: Take the instruction and the conditions provided and insert the conditions into the instruction and return the new instruction.\n",
    "    Rules for completing the task:\n",
    "    1. For every condition , find a relevant position in the instruction where this condition could be easily be inserted and still the instruction make sense.\n",
    "    2. Depending on the need, paraphrase the condition to suit the relevant position in the instruction so that the condition is naturally inserted into the instruction.\n",
    "    3. Do not delete, reorder, or alter any original content.\n",
    "    4. Keep all punctuation, line breaks, return types, and code formatting intact.\n",
    "    5. Solution snippets demarcated by BEGIN SOLUTION, you **MUST NOT** modify or remove these from the new_instruction\n",
    "    ## Input format:\n",
    "    {{\"Instruction\": {original_instruction}\n",
    "    \"Conditions\": {new_constraints} }}\n",
    "    ## response format\n",
    "    {{\"new_instruction\": \"The new instruction with the conditions merged in a natural way.}}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# prompt = insert_constraints_prompt(original_instruction,new_constraints)\n",
    "# print(\"prompt: \\n\")\n",
    "# print(prompt)\n",
    "# print(\"original_instruction:\", original_instruction)\n",
    "# print(\"------------------\")\n",
    "# print(\"new_constraints:\", new_constraints)\n",
    "# print(\"------------------\")\n",
    "# response = get_response(prompt)\n",
    "# print(\"response: ...................\")\n",
    "# print(response)\n",
    "# print(\"New Instruction:\", extract_json(response, \"new_instruction\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_instruction(df,output_pth,input_col1,input_col2,output_col):\n",
    "    prompts = []\n",
    "    for _, row in df.iterrows():\n",
    "        # print(f\"Processing row {i}\")\n",
    "        prompt = insert_constraints_prompt(row[input_col1], row[input_col2])\n",
    "        prompts.append(prompt)\n",
    "    responses = get_model_response_batch(prompts)\n",
    "    new_instructions = [extract_json(response, \"new_instruction\") for response in responses]\n",
    "    df[output_col] = new_instructions\n",
    "    df.to_csv(output_pth, index=False)\n",
    "    print(\"Saved the file successfully\")\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_instruction(df,output_path+\"step6_with_combined_instruction.csv\",\"instruction\",\"filtered_relevant_constraints\",\"combined_instruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(columns = [\"combined_instruction\"])\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwXWl422fkF4"
   },
   "source": [
    "## Constraints Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_v1 = df.copy()\n",
    "benchmark_v1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_v1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_v1.to_csv(root+\"benchmark/benchmark_v1.csv\",index=True,index_label=[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"../../benchmark/\"\n",
    "benchmark_v1 = pd.read_csv(output_path+\"benchmark_v1.csv\")\n",
    "benchmark_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "root = \"../..\"\n",
    "data = pd.read_csv(root+\"/benchmark/outputs/benchmark_google_flan-t5-base.csv\")\n",
    "data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[0,17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_v1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    " \n",
    "score_columns = ['relevance_score', 'objectivity_score', 'atomicity_score','unified_quality_score']\n",
    "score_stats = benchmark_v1[score_columns].agg(['max', 'min', 'mean'])\n",
    "print(\"Score Statistics:\\n\", score_stats)\n",
    "\n",
    "def count_constraints(constraints, source):\n",
    "    if pd.isna(constraints):\n",
    "        return 0\n",
    "    try:\n",
    "        parsed = literal_eval(constraints)\n",
    "        return sum(1 for item in parsed if item.get('instruction_part') == source)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing: {constraints}\\n{e}\")\n",
    "        return 0\n",
    "\n",
    "# Create new columns\n",
    "benchmark_v1['num_extracted_constraints'] = benchmark_v1['filtered_relevant_constraints'].apply(lambda x: count_constraints(x, 'Extracted from instruction'))\n",
    "benchmark_v1['num_new_constraints'] = benchmark_v1['filtered_relevant_constraints'].apply(lambda x: count_constraints(x, 'Newly Generated'))\n",
    "benchmark_v1['num_total_constraints'] = benchmark_v1['num_extracted_constraints'] + benchmark_v1['num_new_constraints']\n",
    "\n",
    "\n",
    "constraint_stats = benchmark_v1[['num_extracted_constraints', 'num_new_constraints', 'num_total_constraints']].mean()\n",
    "print(\"\\nConstraint Count Averages:\\n\", constraint_stats)\n",
    "\n",
    "# benchmark_v1.to_csv(root+'benchmark/benchmark_v1_with_stats.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_v1['num_total_constraints'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_v1['num_extracted_constraints'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_v1['num_new_constraints'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Refining the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../../benchmark/benchmark_v1.csv\")  # or pd.read_parquet(\"your_file.parquet\")\n",
    "\n",
    "# Regex pattern to detect and remove the solution block\n",
    "solution_pattern = re.compile(r\"BEGIN SOLUTION.*?END SOLUTION\", re.DOTALL)\n",
    "\n",
    "# Function to clean combined_instruction only if it wrongly includes the solution block\n",
    "def clean_combined_instruction(row):\n",
    "    instr = row['instruction']\n",
    "    comb_instr = row['combined_instruction']\n",
    "    \n",
    "    instr_has_solution = bool(solution_pattern.search(instr))\n",
    "    comb_has_solution = bool(solution_pattern.search(comb_instr))\n",
    "    \n",
    "    # If BEGIN SOLUTION block exists only in combined_instruction, remove it\n",
    "    if comb_has_solution and not instr_has_solution:\n",
    "        print(\"removing the snippet from combined_solution\")\n",
    "        comb_instr = solution_pattern.sub(\"\", comb_instr).strip()\n",
    "        row['combined_instruction'] = comb_instr\n",
    "    \n",
    "    return row\n",
    "\n",
    "# Apply the cleaning logic row by row\n",
    "df = df.apply(clean_combined_instruction, axis=1)\n",
    "\n",
    "# Save to a new file or overwrite\n",
    "df.to_csv(\"../../benchmark/benchmark_v2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    " # or pd.read_parquet(...)\n",
    "\n",
    "# Define the 12 valid supercategories\n",
    "valid_categories = [\n",
    "    \"Code Structure and Modularity\",\n",
    "    \"Input and Output Handling\",\n",
    "    \"Error Handling and Robustness\",\n",
    "    \"Data Processing and Transformation\",\n",
    "    \"Performance and Optimization\",\n",
    "    \"Library and API Usage\",\n",
    "    \"Testing and Debugging\",\n",
    "    \"Documentation and Readability\",\n",
    "    \"Security and Privacy\",\n",
    "    \"Reproducibility and Consistency\",\n",
    "    \"Mathematical Computation\",\n",
    "    \"File and Data Management\",\n",
    "    \"UI and Interaction\",\n",
    "]\n",
    "\n",
    "# Track rows with invalid types\n",
    "invalid_entries = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        constraints = row[\"filtered_relevant_constraints\"]\n",
    "        for c in constraints:\n",
    "            constraint_type = c.get(\"type\", \"\")\n",
    "            if constraint_type not in valid_categories:\n",
    "                invalid_entries.append({\n",
    "                    \"row_index\": i,\n",
    "                    \"invalid_type\": constraint_type,\n",
    "                    \"constraint\": c.get(\"constraint\", \"\")\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Row {i} could not be parsed: {e}\")\n",
    "\n",
    "# Convert to DataFrame for easy inspection\n",
    "invalid_df = pd.DataFrame(invalid_entries)\n",
    "\n",
    "invalid_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# or use read_parquet()\n",
    "\n",
    "# Convert string representation of lists to actual lists\n",
    "def is_empty_list(value):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(value) if isinstance(value, str) else value\n",
    "        return isinstance(parsed, list) and len(parsed) == 0\n",
    "    except Exception:\n",
    "        return False  # skip invalid entries\n",
    "\n",
    "# Count empty lists in 'combined_solution' column\n",
    "empty_list_count = df['combined_instruction'].apply(is_empty_list).sum()\n",
    "\n",
    "print(f\"Number of empty lists in 'combined_solution': {empty_list_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# or read_parquet(...)\n",
    "\n",
    "# Identify rows where combined_instruction is an empty list string\n",
    "def is_empty_list_string(value):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(value) if isinstance(value, str) else value\n",
    "        return isinstance(parsed, list) and len(parsed) == 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Filter the rows that failed previously\n",
    "mask = df['combined_instruction'].apply(is_empty_list_string)\n",
    "df_failed = df[mask].copy()\n",
    "\n",
    "print(f\"Found {len(df_failed)} rows with empty combined_instruction\")\n",
    "\n",
    "# Re-run the combine_instruction function only on failed rows\n",
    "df_failed_updated = combine_instruction(\n",
    "    df_failed,\n",
    "    output_pth=\"re_generated_only.csv\",  # optional file\n",
    "    input_col1=\"instruction\",\n",
    "    input_col2=\"filtered_relevant_constraints\",\n",
    "    output_col=\"combined_instruction\"\n",
    ")\n",
    "\n",
    "# Replace back the corrected rows in the original DataFrame\n",
    "df.update(df_failed_updated)\n",
    "\n",
    "# Save the final cleaned and updated DataFrame\n",
    "df.to_csv(\"../../benchmark/benchmark_v3.csv\", index=False)\n",
    "print(\"All empty combined_instruction rows regenerated and merged back.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "def is_empty_list(value):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(value) if isinstance(value, str) else value\n",
    "        return isinstance(parsed, list) and len(parsed) == 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Filter out rows with empty list in combined_instruction\n",
    "df_cleaned = df[~df[\"combined_instruction\"].apply(is_empty_list)]\n",
    "\n",
    "# (Optional) Save the cleaned DataFrame\n",
    "df_cleaned.to_csv(\"benchmark_v3.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Removed {len(df) - len(df_cleaned)} rows with empty lists in 'combined_instruction'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Matching the proper categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_category_prompt(constraint, invalid_type, valid_categories):\n",
    "    category_list = \"\\n\".join(f\"- {cat}\" for cat in valid_categories)\n",
    "    return f\"\"\"Task: You are given a constraint and an incorrect category label. Your job is to map this constraint to the most appropriate correct category from the list of valid categories.\n",
    "\n",
    "Constraint: \"{constraint}\"\n",
    "\n",
    "Invalid Category: \"{invalid_type}\"\n",
    "\n",
    "Valid Categories:\n",
    "{category_list}\n",
    "\n",
    "Return your response in the format:\n",
    "{{\"correct_category\": \"<best matching category from the list>\"}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_invalid_categories(invalid_df, valid_categories):\n",
    "    prompts = [\n",
    "        map_category_prompt(row[\"constraint\"], row[\"invalid_type\"], valid_categories)\n",
    "        for _, row in invalid_df.iterrows()\n",
    "    ]\n",
    "    \n",
    "    responses = get_model_response_batch(prompts)\n",
    "    corrected = [extract_json(resp, \"correct_category\") for resp in responses]\n",
    "    \n",
    "    invalid_df[\"corrected_type\"] = corrected\n",
    "    return invalid_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_category_corrections(df, invalid_df):\n",
    "    # Convert column to Python objects\n",
    "    df[\"filtered_relevant_constraints\"] = df[\"filtered_relevant_constraints\"].apply(ast.literal_eval)\n",
    "\n",
    "    # Group corrections by row_index\n",
    "    correction_map = {}\n",
    "    for _, row in invalid_df.iterrows():\n",
    "        idx = row[\"row_index\"]\n",
    "        constraint_text = row[\"constraint\"]\n",
    "        corrected_type = row[\"corrected_type\"]\n",
    "        if idx not in correction_map:\n",
    "            correction_map[idx] = {}\n",
    "        correction_map[idx][constraint_text] = corrected_type\n",
    "\n",
    "    # Apply corrections\n",
    "    for idx, corrections in correction_map.items():\n",
    "        updated = []\n",
    "        for constraint in df.at[idx, \"filtered_relevant_constraints\"]:\n",
    "            text = constraint.get(\"constraint\")\n",
    "            if text in corrections:\n",
    "                constraint[\"type\"] = corrections[text]\n",
    "            updated.append(constraint)\n",
    "        df.at[idx, \"filtered_relevant_constraints\"] = updated\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"corrected_constraint_categories.csv\", index=False)\n",
    "print(\"✅ Saved updated constraints with corrected categories.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# === Step 1: Define valid categories ===\n",
    "VALID_CATEGORIES = [\n",
    "    \"Code Structure and Modularity\",\n",
    "    \"Input and Output Handling\",\n",
    "    \"Error Handling and Robustness\",\n",
    "    \"Data Processing and Transformation\",\n",
    "    \"Performance and Optimization\",\n",
    "    \"Library and API Usage\",\n",
    "    \"Testing and Debugging\",\n",
    "    \"Documentation and Readability\",\n",
    "    \"Security and Privacy\",\n",
    "    \"Reproducibility and Consistency\",\n",
    "    \"Mathematical Computation\",\n",
    "    \"File and Data Management\",\n",
    "    \"UI and Interaction\",\n",
    "]\n",
    "\n",
    "# === Step 2: Prompt for category correction ===\n",
    "def map_category_prompt(constraint, invalid_type, valid_categories):\n",
    "    category_list = \"\\n\".join(f\"- {cat}\" for cat in valid_categories)\n",
    "    return f\"\"\"Task: You are given a constraint and an incorrect category label. Your job is to map this constraint to the most appropriate correct category from the list of valid categories.\n",
    "\n",
    "Constraint: \"{constraint}\"\n",
    "\n",
    "Invalid Category: \"{invalid_type}\"\n",
    "\n",
    "Valid Categories:\n",
    "{category_list}\n",
    "\n",
    "Return your response in the format:\n",
    "{{\"correct_category\": \"<best matching category from the list>\"}}\n",
    "\"\"\"\n",
    "\n",
    "# === Step 3: Extract invalid constraint entries ===\n",
    "def extract_invalid_constraints(df):\n",
    "    invalid_entries = []\n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            constraints = ast.literal_eval(row[\"filtered_relevant_constraints\"])\n",
    "            for c in constraints:\n",
    "                constraint_type = c.get(\"type\", \"\")\n",
    "                if constraint_type not in VALID_CATEGORIES:\n",
    "                    invalid_entries.append({\n",
    "                        \"row_index\": i,\n",
    "                        \"invalid_type\": constraint_type,\n",
    "                        \"constraint\": c.get(\"constraint\", \"\")\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Row {i} parsing error: {e}\")\n",
    "    return pd.DataFrame(invalid_entries)\n",
    "\n",
    "# === Step 4: Get corrected categories from model ===\n",
    "def correct_invalid_categories(invalid_df, valid_categories):\n",
    "    prompts = [\n",
    "        map_category_prompt(row[\"constraint\"], row[\"invalid_type\"], valid_categories)\n",
    "        for _, row in invalid_df.iterrows()\n",
    "    ]\n",
    "    \n",
    "    responses = get_model_response_batch(prompts)\n",
    "    corrected = [extract_json(resp, \"correct_category\") for resp in responses]\n",
    "    \n",
    "    invalid_df[\"corrected_type\"] = corrected\n",
    "    return invalid_df\n",
    "\n",
    "\n",
    "def apply_category_corrections(df, invalid_df):\n",
    "    df[\"filtered_relevant_constraints\"] = df[\"filtered_relevant_constraints\"].apply(ast.literal_eval)\n",
    "\n",
    "    correction_map = {}\n",
    "    for _, row in invalid_df.iterrows():\n",
    "        idx = row[\"row_index\"]\n",
    "        constraint_text = row[\"constraint\"]\n",
    "        corrected_type = row[\"corrected_type\"]\n",
    "        if idx not in correction_map:\n",
    "            correction_map[idx] = {}\n",
    "        correction_map[idx][constraint_text] = corrected_type\n",
    "\n",
    "    for idx, corrections in correction_map.items():\n",
    "        updated = []\n",
    "        for c in df.at[idx, \"filtered_relevant_constraints\"]:\n",
    "            if c[\"constraint\"] in corrections:\n",
    "                c[\"type\"] = corrections[c[\"constraint\"]]\n",
    "            updated.append(c)\n",
    "        df.at[idx, \"filtered_relevant_constraints\"] = updated\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_constraint_category_mapping(df, output_file=\"corrected_categories.csv\"):\n",
    "    print(\"🔍 Extracting invalid constraint categories...\")\n",
    "    invalid_df = extract_invalid_constraints(df)\n",
    "    print(f\"Found {len(invalid_df)} invalid entries.\")\n",
    "\n",
    "    if invalid_df.empty:\n",
    "        print(\"✅ No invalid categories found.\")\n",
    "        return df\n",
    "\n",
    "    print(\"🤖 Sending prompts to model for correction...\")\n",
    "    invalid_df = correct_invalid_categories(invalid_df, VALID_CATEGORIES)\n",
    "\n",
    "    print(\"🔄 Applying corrections back to DataFrame...\")\n",
    "    df = apply_category_corrections(df, invalid_df)\n",
    "\n",
    "    print(f\"💾 Saving to {output_file} ...\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(\"✅ Done.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = run_constraint_category_mapping(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(\"../../benchmark/benchmark_v4.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Responses Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_CALLS_PER_MIN = 1500\n",
    "\n",
    "class LLMClient:\n",
    "\n",
    "    def __init__(self, api_key,model_id,client_type=\"rits\", base_url=None):\n",
    "        if client_type == \"rits\":\n",
    "            llm = OpenAI(\n",
    "                api_key=api_key,\n",
    "                base_url=base_url,\n",
    "                default_headers={\"RITS_API_KEY\": api_key},\n",
    "            )\n",
    "        elif client_type == \"GPT\":\n",
    "            llm = OpenAI(\n",
    "                api_key=api_key,\n",
    "                base_url=base_url,)\n",
    "        self.llm = llm\n",
    "        self.model_id = model_id\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    def get_model_response(\n",
    "        self,\n",
    "        messages=None,\n",
    "        system_prompt=None,\n",
    "        user_prompt=None,\n",
    "        max_new_tokens=2048,\n",
    "        min_new_tokens=30,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.8,\n",
    "        repetition_penalty=1.05,\n",
    "    ):\n",
    "        # Setup the sampling parameters for generation\n",
    "        if messages is None:\n",
    "            if system_prompt:\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ]\n",
    "            else:\n",
    "                messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "        response = self.llm.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            messages=messages,\n",
    "            max_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    def apply_chat_template(self, messages_list):\n",
    "        prompts = [\n",
    "            self.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            for messages in messages_list\n",
    "        ]\n",
    "        return prompts\n",
    "\n",
    "    def get_model_response_batch(\n",
    "        self, system_prompt=None, user_prompts=None, max_new_tokens=400, temperature=0.7\n",
    "    ):\n",
    "        non_none_user_prompts = [ele for ele in user_prompts if ele is not None]\n",
    "        if system_prompt:\n",
    "            messages_list = [\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ]\n",
    "                for user_prompt in non_none_user_prompts\n",
    "            ]\n",
    "        else:\n",
    "            messages_list = [\n",
    "                [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "                for user_prompt in non_none_user_prompts\n",
    "            ]\n",
    "        with ThreadPoolExecutor(max_workers=NUM_CALLS_PER_MIN) as executor:\n",
    "            response_texts = list(\n",
    "                tqdm(\n",
    "                    executor.map(\n",
    "                        lambda messages: self.call_api(\n",
    "                            messages, max_new_tokens, temperature\n",
    "                        ),\n",
    "                        messages_list,\n",
    "                    ),\n",
    "                    total=len(messages_list),\n",
    "                    desc=\"Processing\",\n",
    "                )\n",
    "            )\n",
    "        response_iter = iter(response_texts)\n",
    "        all_response_texts = [\n",
    "            next(response_iter) if ele is not None else None for ele in user_prompts\n",
    "        ]\n",
    "        return all_response_texts\n",
    "\n",
    "    @sleep_and_retry\n",
    "    @limits(calls=1500, period=60)\n",
    "    def call_api(self, messages, max_new_tokens, temperature):\n",
    "        response = self.get_model_response(\n",
    "            messages=messages, max_new_tokens=max_new_tokens, temperature=temperature\n",
    "        )\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "random_state = 42\n",
    "\n",
    "def main(input_csv, output_dir, api_key, model_id, base_url, temperature=0.1, system_prompt=None):\n",
    "    df_ini = pd.read_csv(input_csv)\n",
    "    df = df_ini.sample(10,random_state=random_state).copy()  # Limit to first 10 rows for testing\n",
    "\n",
    "    user_prompts = df[\"combined_instruction\"].tolist()\n",
    "    model_name = model_id.split(\"/\")[-1]\n",
    "    output_path = os.path.join(output_dir, f\"{model_name}_results.jsonl\")\n",
    "    client = LLMClient(\n",
    "        api_key=api_key,\n",
    "        model_id=model_id,\n",
    "        client_type=\"rits\",\n",
    "        base_url=base_url\n",
    "    )\n",
    "\n",
    "    print(f\"Generating responses for {len(user_prompts)} prompts.\")\n",
    "    responses = client.get_model_response_batch(\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompts=user_prompts,\n",
    "        temperature=temperature\n",
    "\n",
    "    )\n",
    "\n",
    "    output_data = []\n",
    "    for row, response in zip(df.to_dict(orient=\"records\"), responses):\n",
    "        row[\"response\"] = response\n",
    "        output_data.append(row)\n",
    "\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in output_data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Saved output to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"./benchmark/benchmark_v4.csv\"\n",
    "    output_dir = \"./benchmark/response_outputs\"\n",
    "    api_key = os.getenv(\"RITS_API_KEY\")\n",
    "    model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    print(api_key)\n",
    "    base_url = \"https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com/llama-3-1-8b-instruct/v1\"\n",
    "    temperature = 0.1\n",
    "    \n",
    "    main(\n",
    "        input_csv=input_csv,\n",
    "        output_dir=output_dir,\n",
    "        api_key=api_key,\n",
    "        model_id=model_id,\n",
    "        base_url=base_url,\n",
    "        temperature=temperature\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM as Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Judge for constraint adherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_json_response(response):\n",
    "    if response.startswith(\"```json\") and response.endswith(\"```\"):\n",
    "        response = response[7:-3].strip()\n",
    "    try:\n",
    "        response_dict = json.loads(response)\n",
    "        return response_dict\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        json_pattern = re.compile(r\"\\{.*?\\}\", re.DOTALL)\n",
    "        match = json_pattern.search(response)\n",
    "        if match:\n",
    "            json_block = match.group(0)\n",
    "            json_block = json_block.replace(\"\\\\'\", \"'\")\n",
    "            response_dict = json.loads(json_block)\n",
    "            return response_dict\n",
    "        else:\n",
    "            print(\"Not able to extract json data\")\n",
    "            return None\n",
    "    except:\n",
    "        print(\"Not able to extract json data\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response_constraint_decision(output):\n",
    "    if output is None:\n",
    "        return None\n",
    "    output_dict = load_model_json_response(output)\n",
    "    if output_dict:\n",
    "        return output_dict\n",
    "    return (None, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_batch_handler(function_to_batch: callable):\n",
    "    return lambda *args_lists: [function_to_batch(*args) for args in zip(*args_lists)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_constraint_validator(model_responses: str, constraints_list: list[str], instructions: str,\n",
    "                                  client: LLMClient):\n",
    "    \"\"\"Return a boolean which indicates whether response satisfies all constraints or not.\"\"\"\n",
    "\n",
    "    def build_user_prompt(instruction, constraints, response):\n",
    "        return f\"\"\"[Instruction]:\n",
    "    {instruction}\n",
    "\n",
    "    [Constraints]:\n",
    "    {constraints}\n",
    "\n",
    "    [Response]:\n",
    "    ```python\n",
    "    {response}\n",
    "    ```\"\"\"\n",
    "\n",
    "    system_prompt1 = \"\"\"You are a verifier. Your task is to evaluate whether a given response satisfies a set of constraints for a specific instruction.\n",
    "\n",
    "    You will be provided:\n",
    "    - An instruction\n",
    "    - A list of constraints\n",
    "    - A response to the instruction\n",
    "\n",
    "    Your task:\n",
    "    - Analyze the response against each constraint independently.\n",
    "    - For each constraint, determine whether it is satisfied, and provide explanation.\n",
    "    - Do not make assumptions beyond the constraints. Only base your judgment on what is explicitly written.\n",
    "\n",
    "    Output your evaluation in a valid JSON format like below:\n",
    "    {\"Evaluation\": [\n",
    "        {\n",
    "        \"Constraint\": \"<constraint text>\",\n",
    "        \"Reason\": \"explanation\",\n",
    "        \"Aligns\": [true|false]\n",
    "        },\n",
    "        ...\n",
    "    ],\n",
    "    \"FinalDecision\": {\n",
    "        \"Score\": \"<number of constraints met>/<total number of constraints>\",\n",
    "        \"Reason\": \"<Yes if all constraints were satisfied; No if any were violated, with explanation>\",\n",
    "        \"Aligns\": [true|false]\n",
    "    }\n",
    "    }\n",
    "\n",
    "    Do not include any text outside this JSON object.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompts = [\n",
    "        build_user_prompt(instruction, constraints, model_response) if model_response is not None else None\n",
    "        for instruction, constraints, model_response in zip(instructions, constraints_list, model_responses)\n",
    "    ]\n",
    "\n",
    "    outputs = client.get_model_response_batch(user_prompts=user_prompts,\n",
    "                                                      system_prompt=system_prompt1,\n",
    "                                                      temperature=0.3,\n",
    "                                                      max_new_tokens=1000)\n",
    "    extract_response_constraint_decision_batch = function_batch_handler(extract_response_constraint_decision)\n",
    "    is_correct_list = extract_response_constraint_decision_batch(outputs)\n",
    "    return is_correct_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "def create_clients(mode=\"rits\"):\n",
    "    if mode == \"rits\":\n",
    "        RITS_KEY = os.getenv(\"RITS_API_KEY\")\n",
    "        print(RITS_KEY)\n",
    "        base_url = \"https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com/microsoft-phi-4/v1\"\n",
    "        model_id = \"microsoft/phi-4\"\n",
    "        client = LLMClient(api_key=RITS_KEY,\n",
    "                                    model_id=model_id,\n",
    "                                    client_type=\"rits\",\n",
    "                                    base_url=base_url,\n",
    "                                    )\n",
    "    elif mode == \"GPT-azure\":\n",
    "        openai_key = os.getenv(\"IBM_OPENAI_API_KEY\")\n",
    "        base_url=\"https://ete-litellm.bx.cloud9.ibm.com\"\n",
    "        client = LLMClient(api_key=openai_key,\n",
    "                                  model_id=\"Azure/gpt-4.1-mini\",\n",
    "                                  base_url=base_url,\n",
    "                                  client_type=\"GPT\")\n",
    "        \n",
    "    elif mode == \"GPT\":\n",
    "        openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        client = LLMClient(api_key=openai_key,\n",
    "                                  model_id=\"gpt-4o-mini\",\n",
    "                                  client_type=\"GPT\")\n",
    "\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 17 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               10 non-null     int64  \n",
      " 1   dataset                          10 non-null     object \n",
      " 2   instruction                      10 non-null     object \n",
      " 3   code                             10 non-null     object \n",
      " 4   test                             5 non-null      object \n",
      " 5   relevant_categories              10 non-null     object \n",
      " 6   simplified_instruction           10 non-null     object \n",
      " 7   extracted_constraints            10 non-null     object \n",
      " 8   final_comprehensive_constraints  10 non-null     object \n",
      " 9   filtered_relevant_constraints    10 non-null     object \n",
      " 10  quality_scores                   10 non-null     object \n",
      " 11  relevance_score                  10 non-null     float64\n",
      " 12  objectivity_score                10 non-null     float64\n",
      " 13  atomicity_score                  10 non-null     float64\n",
      " 14  unified_quality_score            10 non-null     float64\n",
      " 15  combined_instruction             10 non-null     object \n",
      " 16  response                         10 non-null     object \n",
      "dtypes: float64(4), int64(1), object(12)\n",
      "memory usage: 1.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "res_df = pd.read_json(\"./benchmark/response_outputs/Llama-3.1-8B-Instruct_results.jsonl\",lines=True)\n",
    "res_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "client = create_clients(mode=\"GPT\")\n",
    "print(client.get_model_response(messages=[\n",
    "            {\"role\": \"user\", \"content\": \"What is the capital of France?\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48032bfa1e1d782cece12fedb6b3fb40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "client = create_clients()\n",
    "print(client.get_model_response(user_prompt=\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|                                                                                                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:25<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "client = create_clients(mode=\"GPT\")\n",
    "instructions = res_df[\"instruction\"].to_list()\n",
    "model_responses = res_df[\"response\"].to_list()\n",
    "constraints_list = res_df[\"filtered_relevant_constraints\"].to_list()\n",
    "\n",
    "result_list = response_constraint_validator(model_responses,constraints_list,instructions,client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Evaluation': [{'Constraint': 'The function should output a numpy.ndarray representing the generated complex wave.', 'Reason': \"The response includes a return statement that outputs 'wave_windowed', which is a numpy array of complex numbers.\", 'Aligns': True}, {'Constraint': 'The function should output a matplotlib.figure.Figure representing the figure object of the plot.', 'Reason': \"The response includes a return statement that outputs 'fig', which is the figure object of the plot.\", 'Aligns': True}, {'Constraint': 'The function should output a matplotlib.axes.Axes representing the axes object of the plot.', 'Reason': \"The response includes a return statement that outputs 'ax', which is the axes object of the plot.\", 'Aligns': True}, {'Constraint': 'The plot title must be \"Complex Wave with Hann Window\".', 'Reason': \"The response sets the title of the plot to 'Complex Wave with Hann Window' as required.\", 'Aligns': True}, {'Constraint': 'The x-label of the plot must be \"Time\".', 'Reason': \"The response sets the x-label of the plot to 'Time' as required.\", 'Aligns': True}, {'Constraint': 'The y-label of the plot must be \"Amplitude\".', 'Reason': \"The response sets the y-label of the plot to 'Amplitude' as required.\", 'Aligns': True}, {'Constraint': 'The wave must be generated using the formula wave = amplitude * np.exp(1j * 2 * math.pi * frequency * time).', 'Reason': 'The response uses the specified formula to generate the wave, thus satisfying this constraint.', 'Aligns': True}, {'Constraint': 'A Hann window must be applied to the wave to reduce edge effects.', 'Reason': \"The response applies a Hann window to the wave using 'get_window' and multiplies it with the wave, which satisfies this constraint.\", 'Aligns': True}], 'FinalDecision': {'Score': '8/8', 'Reason': 'Yes, all constraints were satisfied.', 'Aligns': True}}\n",
      "{'Evaluation': [{'Constraint': \"Remove all jQuery files (JavaScript files containing 'jquery' in their name) from a specified directory.\", 'Reason': \"The code iterates through the files in the specified directory and checks if 'jquery' is in the filename, removing those that match.\", 'Aligns': True}, {'Constraint': \"Removed files are logged in 'jquery_removal.log' file.\", 'Reason': \"The code sets up logging to a file named 'jquery_removal.log' and logs each removed file.\", 'Aligns': True}, {'Constraint': 'Raise FileNotFoundError if the specified directory does not exist.', 'Reason': 'The code checks if the directory exists and raises a FileNotFoundError if it does not.', 'Aligns': True}, {'Constraint': 'Output a tuple containing two elements: int: The number of files removed and list: The names of the removed files.', 'Reason': 'The function returns a tuple with the count of removed files and a list of their names.', 'Aligns': True}, {'Constraint': 'Ensure that the function handles directories with no jQuery files gracefully without raising unnecessary errors.', 'Reason': 'The code iterates through the files and simply does not log or count files if none are found, thus handling the case gracefully.', 'Aligns': True}], 'FinalDecision': {'Score': '5/5', 'Reason': 'Yes, all constraints were satisfied.', 'Aligns': True}}\n",
      "{'Evaluation': [{'Constraint': 'Define the URL patterns for the Django application.', 'Reason': 'The response includes a script that defines URL patterns for the Django application in both the main `urls.py` and the app-specific `urls.py`.', 'Aligns': True}, {'Constraint': 'Include URL patterns for the views: `index`, `movie_detail`, `movie_create`, `studio_create`, `hall_create`, and `country_create`.', 'Reason': 'The response lists all required URL patterns for the specified views in `movie_app/urls.py`.', 'Aligns': True}, {'Constraint': 'Ensure that the application serves media files in development.', 'Reason': 'The response includes code to serve media files in development by appending the necessary static URL patterns when `DEBUG` is true.', 'Aligns': True}, {'Constraint': 'Implement proper file handling for media uploads, ensuring files are stored securely and accessible only through the application.', 'Reason': 'The response does not explicitly mention any implementation details regarding secure file handling for media uploads, which is necessary to satisfy this constraint.', 'Aligns': False}], 'FinalDecision': {'Score': '3/4', 'Reason': 'No, one constraint was violated because the response did not address proper file handling for media uploads.', 'Aligns': False}}\n",
      "{'Evaluation': [{'Constraint': 'The system should handle different types of tenders, such as below-threshold, open UA, and open UA defense tenders.', 'Reason': 'The response defines an enum for TenderType which includes the specified tender types, indicating that the system is designed to handle them.', 'Aligns': True}, {'Constraint': 'Each tender type has its own set of rules for transitioning between statuses based on the number of bids, complaints, and auction results.', 'Reason': 'The response includes a dictionary that maps each tender type to its corresponding rules, fulfilling this requirement.', 'Aligns': True}, {'Constraint': \"Transition a tender to 'unsuccessful' if there are no bids.\", 'Reason': \"The response does not explicitly implement the transition to 'unsuccessful' based on the number of bids, thus not satisfying this constraint.\", 'Aligns': False}, {'Constraint': 'Set the auction period for tenders without bids.', 'Reason': 'The response does not address setting the auction period for tenders without bids, failing to meet this constraint.', 'Aligns': False}, {'Constraint': \"Transition a tender to 'qualification' if there is at least one bid.\", 'Reason': \"The response does not provide logic for transitioning to 'qualification' based on the presence of bids, hence this constraint is not satisfied.\", 'Aligns': False}, {'Constraint': 'Handle complaints and transition tenders accordingly.', 'Reason': 'The response does not mention how complaints affect the transition of tender statuses, failing to satisfy this constraint.', 'Aligns': False}, {'Constraint': \"Transition a tender to 'auction' when it's ready for auction.\", 'Reason': \"The response does not include any logic for transitioning a tender to 'auction', thus not satisfying this constraint.\", 'Aligns': False}, {'Constraint': 'Handle tenders with lots, including setting auction periods and transitioning lot statuses.', 'Reason': 'The response defines a Lot class but does not include any logic for handling lots or their auction periods, failing this constraint.', 'Aligns': False}, {'Constraint': 'Write a set of unit tests to verify that the system correctly transitions tenders and lots between statuses under various conditions.', 'Reason': 'The response does not include any unit tests, thus failing to meet this constraint.', 'Aligns': False}, {'Constraint': 'Use the provided code snippet as a starting point for creating the test cases.', 'Reason': 'The response includes a code snippet but does not show any test cases derived from it, failing this constraint.', 'Aligns': False}, {'Constraint': 'Ensure the system can handle unexpected input gracefully, such as invalid tender data or missing required fields.', 'Reason': 'The response includes error handling for invalid tenders and types, thus satisfying this constraint.', 'Aligns': True}, {'Constraint': 'The system should provide clear feedback to users when a tender status transition fails due to validation errors.', 'Reason': 'The response includes logging for errors when transitions fail, which provides feedback to users, thus satisfying this constraint.', 'Aligns': True}, {'Constraint': 'Include edge cases in the unit tests, such as tenders with maximum allowed bids and tenders with multiple complaints.', 'Reason': 'The response does not include any unit tests or mention of edge cases, thus failing this constraint.', 'Aligns': False}, {'Constraint': 'Implement logging for all status transitions to facilitate debugging and auditing.', 'Reason': 'The response includes a logger for error handling, which aligns with the requirement for logging status transitions.', 'Aligns': True}], 'FinalDecision': {'Score': '6/14', 'Reason': 'No, several constraints were violated, particularly those related to the implementation of status transitions and the absence of unit tests.', 'Aligns': False}}\n",
      "{'Evaluation': [{'Constraint': 'The function will raise a ValueError if input df is not a DataFrame.', 'Reason': 'The response includes a check using isinstance(df, pd.DataFrame) that raises a ValueError if df is not a DataFrame.', 'Aligns': True}, {'Constraint': 'The function should output with: LinearRegression: The trained linear regression model.', 'Reason': 'The response specifies that the function returns a LinearRegression model, but the actual return statement is missing in the provided code.', 'Aligns': False}], 'FinalDecision': {'Score': '1/2', 'Reason': 'No, not all constraints were satisfied; the return statement for the trained model is missing.', 'Aligns': False}}\n",
      "{'Evaluation': [{'Constraint': \"The pivot table must aggregate column 'D' using the sum function and column 'E' using the mean function.\", 'Reason': \"The response correctly specifies the aggregation functions for columns 'D' and 'E' as sum and mean respectively.\", 'Aligns': True}, {'Constraint': \"Utilize the pandas library's pivot_table function to create the pivot table.\", 'Reason': \"The response uses the pandas library's pivot_table function to create the pivot table as required.\", 'Aligns': True}, {'Constraint': 'Ensure the function returns the pivot table as a DataFrame object.', 'Reason': \"The response assigns the result of the pivot_table to the variable 'result', which is a DataFrame object.\", 'Aligns': True}, {'Constraint': \"Verify that the aggregation functions used (sum for 'D' and mean for 'E') are correctly applied to the respective columns in the pivot table.\", 'Reason': 'The response specifies the correct aggregation functions in the dictionary passed to aggfunc, ensuring they are applied correctly.', 'Aligns': True}, {'Constraint': 'Ensure that the numpy library is imported and used for the aggregation functions.', 'Reason': 'The response imports the numpy library and uses np.sum and np.mean as aggregation functions.', 'Aligns': True}], 'FinalDecision': {'Score': '5/5', 'Reason': 'Yes, all constraints were satisfied.', 'Aligns': True}}\n",
      "{'Evaluation': [{'Constraint': 'Without looping through every row of df2, join the two dataframes based on the timestamp.', 'Reason': 'The response uses the `merge_asof` function, which allows for joining the dataframes based on timestamps without explicitly looping through each row.', 'Aligns': True}, {'Constraint': 'For every row in df2, add data from df1 that was at that particular time.', 'Reason': 'The `merge_asof` function effectively adds the corresponding data from df1 to df2 based on the timestamp.', 'Aligns': True}, {'Constraint': 'Avoid looping through each row of df2 then comparing to each df1.', 'Reason': 'The use of `merge_asof` avoids the need for manual looping, thus satisfying this constraint.', 'Aligns': True}, {'Constraint': \"Utilize the pandas 'merge_asof' function to efficiently combine the dataframes based on timestamps.\", 'Reason': 'The response explicitly uses the `merge_asof` function as required.', 'Aligns': True}, {'Constraint': 'Ensure that the resulting dataframe maintains the original order of df2 after merging.', 'Reason': 'The response reorders the columns of the resulting dataframe to match the original order of df2, thus satisfying this constraint.', 'Aligns': True}, {'Constraint': 'Handle cases where there is no matching timestamp in df1 for a timestamp in df2 by filling with None.', 'Reason': \"The response includes a step to fill NaN values in the 'data' column with None, addressing this constraint.\", 'Aligns': True}], 'FinalDecision': {'Score': '6/6', 'Reason': 'Yes, all constraints were satisfied.', 'Aligns': True}}\n",
      "{'Evaluation': [{'Constraint': 'The function should be recursive.', 'Reason': \"The function 'remove_vowels_and_reverse' calls itself to process the string, which satisfies the requirement for recursion.\", 'Aligns': True}, {'Constraint': 'The function takes a string as an argument.', 'Reason': \"The function definition includes 's' as a parameter, which is specified as a string.\", 'Aligns': True}, {'Constraint': 'Remove all lowercase and uppercase vowels (a, e, i, o, u, A, E, I, O, U) from the string.', 'Reason': 'The function checks if the first character is a vowel and skips it if it is, effectively removing vowels from the string.', 'Aligns': True}, {'Constraint': 'The function should handle special characters and symbols in the input string.', 'Reason': 'The function processes all characters in the string, including special characters, without any specific handling, which meets the requirement.', 'Aligns': True}, {'Constraint': 'Return the new string in reverse order.', 'Reason': 'The function constructs the new string by prepending non-vowel characters, which results in the final string being in reverse order.', 'Aligns': True}, {'Constraint': 'The function should return an empty string if the input is an empty string.', 'Reason': 'The base case of the recursion explicitly returns an empty string when the input string is empty.', 'Aligns': True}, {'Constraint': 'The function should efficiently handle strings of up to 10,000 characters without significant performance degradation.', 'Reason': 'While the function is recursive, it does not explicitly demonstrate performance efficiency for large strings, which could lead to stack overflow for very long inputs.', 'Aligns': False}, {'Constraint': 'Include unit tests that cover various cases, including strings with no vowels, strings with only vowels, and strings with special characters.', 'Reason': 'The unit tests provided cover the specified cases, including no vowels, only vowels, and special characters.', 'Aligns': True}, {'Constraint': 'The function should handle both single-line and multi-line strings correctly.', 'Reason': 'The unit tests include a case for multi-line strings, indicating that the function handles them correctly.', 'Aligns': True}], 'FinalDecision': {'Score': '8/9', 'Reason': 'No, the function does not explicitly demonstrate efficiency for handling very long strings, which is a requirement.', 'Aligns': False}}\n",
      "{'Evaluation': [{'Constraint': \"Raise InvalidTriangleError with the message 'Invalid triangle: the given sides cannot form a valid triangle.' if the sides cannot form a valid triangle.\", 'Reason': 'The response correctly raises the InvalidTriangleError with the specified message when the triangle inequality conditions are not met.', 'Aligns': True}, {'Constraint': 'Check if the sum of the lengths of any two sides of a triangle must be greater than the length of the third side.', 'Reason': 'The response includes a check that ensures the sum of any two sides is greater than the third side, satisfying this constraint.', 'Aligns': True}, {'Constraint': 'Check if the difference between the length of any two sides of a triangle must be smaller than the length of the third side.', 'Reason': 'The response implicitly checks this condition through the triangle inequality checks, thus satisfying this constraint.', 'Aligns': True}, {'Constraint': 'The sides of the triangle must be integers.', 'Reason': 'The response checks that all sides are integers using isinstance before proceeding with calculations, satisfying this constraint.', 'Aligns': True}, {'Constraint': 'Write a function called compute_triangle_area that takes in three parameters: a, b, and c.', 'Reason': 'The response defines a function named compute_triangle_area that takes three parameters a, b, and c, thus meeting this requirement.', 'Aligns': True}, {'Constraint': 'The function must validate that all input parameters are positive integers before proceeding with calculations.', 'Reason': 'The response includes checks to ensure that all sides are positive integers before performing any calculations, satisfying this constraint.', 'Aligns': True}, {'Constraint': \"Implement Heron's formula correctly to compute the area of the triangle once the validity of the sides is confirmed.\", 'Reason': \"The response correctly implements Heron's formula to compute the area of the triangle after validating the sides, thus fulfilling this requirement.\", 'Aligns': True}, {'Constraint': 'Ensure that the function returns a float value representing the area of the triangle.', 'Reason': \"The function returns the area calculated using Heron's formula, which is a float, thus satisfying this constraint.\", 'Aligns': True}], 'FinalDecision': {'Score': '8/8', 'Reason': 'Yes, all constraints were satisfied.', 'Aligns': True}}\n",
      "{'Evaluation': [{'Constraint': 'Use a single loop iteration to return the third character.', 'Reason': 'The code uses a single loop to iterate through the characters of the string and returns the third non-whitespace character.', 'Aligns': True}, {'Constraint': 'Do not use any built-in functions or libraries.', 'Reason': 'The code does not utilize any built-in functions or libraries; it manually iterates through the string.', 'Aligns': True}, {'Constraint': 'Handle cases where the input string has leading or trailing whitespace characters.', 'Reason': 'The code explicitly checks for whitespace characters and ignores them during iteration.', 'Aligns': True}, {'Constraint': 'Return the third character after removing any leading or trailing whitespace characters.', 'Reason': 'The code correctly returns the third non-whitespace character, which is effectively the third character after removing leading and trailing whitespace.', 'Aligns': True}, {'Constraint': 'The input string will always have at least three characters.', 'Reason': 'The instruction states that the input string will always have at least three characters, and the code does not violate this assumption.', 'Aligns': True}], 'FinalDecision': {'Score': '5/5', 'Reason': 'Yes, all constraints were satisfied.', 'Aligns': True}}\n"
     ]
    }
   ],
   "source": [
    "for result in result_list:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_alignment_scores(evaluation_list):\n",
    "    results = []\n",
    "    for entry in evaluation_list:\n",
    "        eval_items = entry.get(\"Evaluation\", [])\n",
    "        aligns_row = [1 if item.get(\"Aligns\") else 0 for item in eval_items]\n",
    "        results.append(aligns_row)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], [1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "result = extract_alignment_scores(result_list)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48032bfa1e1d782cece12fedb6b3fb40\n",
      "Sending prompts to model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|                                                                                                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:20<00:00,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting alignment scores...\n",
      "Results saved to: LLMjudge_outputs/Llama-3.1-8B-Instruct_results_llmjudge.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import dotenv\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "def main(mode, input_path,response_column, output_dir, temperature=0.7):\n",
    "    # 1. Load input JSONL\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df = pd.read_json(input_path, lines=True)\n",
    "\n",
    "    # 2. Create model client\n",
    "    client = create_clients(mode=mode)\n",
    "\n",
    "    # 3. Prepare data\n",
    "    instructions = df[\"instruction\"].tolist()\n",
    "    model_responses = df[response_column].tolist()\n",
    "    constraints_list = df[\"filtered_relevant_constraints\"].tolist()\n",
    "\n",
    "    print(\"Sending prompts to model...\")\n",
    "    result_list = response_constraint_validator(\n",
    "        model_responses=model_responses,\n",
    "        constraints_list=constraints_list,\n",
    "        instructions=instructions,\n",
    "        client=client\n",
    "    )\n",
    "\n",
    "    print(\"Extracting alignment scores...\")\n",
    "    result_scores = extract_alignment_scores(result_list)\n",
    "    df[\"Constraint_adherence\"] = result_scores\n",
    "\n",
    "    # 4. Save output\n",
    "    input_filename = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    output_filename = f\"{input_filename}_llmjudge.jsonl\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    df.to_json(output_path, orient=\"records\", lines=True,force_ascii=False)\n",
    "\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "if __name__ == \"__main__\":\n",
    "    mode = \"rits\"\n",
    "    input_path = \"./benchmark/response_outputs/Llama-3.1-8B-Instruct_results.jsonl\"\n",
    "    response_column = \"response\"\n",
    "    output_dir = \"LLMjudge_outputs\"\n",
    "    temperature = 0.1\n",
    "\n",
    "    # Call main\n",
    "    main(\n",
    "        mode=mode,\n",
    "        input_path=input_path,\n",
    "        response_column=response_column,\n",
    "        output_dir=output_dir,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def compute_metrics_from_column(df, column_name):\n",
    "    # Ensure the column is parsed into lists\n",
    "    df[column_name] = df[column_name].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    # Store per-row CSR and SSR\n",
    "    csr_values = []\n",
    "    ssr_values = []\n",
    "\n",
    "    for r_list in df[column_name]:\n",
    "        r_list = list(map(int, r_list))\n",
    "        # CSR: all constraints must be 1\n",
    "        csr = int(all(r_list))\n",
    "        # SSR: mean of the binary values\n",
    "        ssr = sum(r_list) / len(r_list) if r_list else 0\n",
    "        csr_values.append(csr)\n",
    "        ssr_values.append(ssr)\n",
    "\n",
    "    df[\"CSR_per_row\"] = csr_values\n",
    "    df[\"SSR_per_row\"] = ssr_values\n",
    "\n",
    "    # Compute overall metrics\n",
    "    m = len(df)\n",
    "    overall_csr = sum(csr_values) / m if m > 0 else 0\n",
    "    overall_ssr = sum(ssr_values) / m if m > 0 else 0\n",
    "\n",
    "    return df, overall_csr, overall_ssr\n",
    "\n",
    "def main(input_path, column_name, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # Compute metrics\n",
    "    df_with_metrics, csr, ssr = compute_metrics_from_column(df, column_name)\n",
    "\n",
    "    # Save detailed output with CSR/SSR per row\n",
    "    input_filename = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    detailed_output_path = os.path.join(output_dir, f\"{input_filename}_metrics.csv\")\n",
    "    df_with_metrics.to_csv(detailed_output_path, index=False)\n",
    "\n",
    "    # Save summary file with filename, CSR, SSR\n",
    "    summary_df = pd.DataFrame([{\n",
    "        \"filename\": os.path.basename(input_path),\n",
    "        \"CSR\": csr,\n",
    "        \"SSR\": ssr\n",
    "    }])\n",
    "    summary_output_path = os.path.join(output_dir, \"metrics_summary.csv\")\n",
    "    summary_df.to_csv(summary_output_path, index=False)\n",
    "\n",
    "    print(f\"Saved row-level metrics to {detailed_output_path}\")\n",
    "    print(f\"Saved overall CSR and SSR to {summary_output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Compute CSR and SSR from constraint adherence column\")\n",
    "    parser.add_argument(\"--input_path\", type=str, required=True, help=\"Path to input CSV\")\n",
    "    parser.add_argument(\"--column_name\", type=str, required=True, help=\"Name of the column containing list of 1/0 values\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, required=True, help=\"Folder to save outputs\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(\n",
    "        input_path=args.input_path,\n",
    "        column_name=args.column_name,\n",
    "        output_dir=args.output_dir\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1493 entries, 0 to 1492\n",
      "Data columns (total 16 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1493 non-null   int64  \n",
      " 1   dataset                          1493 non-null   object \n",
      " 2   instruction                      1493 non-null   object \n",
      " 3   code                             1493 non-null   object \n",
      " 4   test                             594 non-null    object \n",
      " 5   relevant_categories              1493 non-null   object \n",
      " 6   simplified_instruction           1493 non-null   object \n",
      " 7   extracted_constraints            1493 non-null   object \n",
      " 8   final_comprehensive_constraints  1493 non-null   object \n",
      " 9   filtered_relevant_constraints    1493 non-null   object \n",
      " 10  quality_scores                   1493 non-null   object \n",
      " 11  relevance_score                  1493 non-null   float64\n",
      " 12  objectivity_score                1493 non-null   float64\n",
      " 13  atomicity_score                  1493 non-null   float64\n",
      " 14  unified_quality_score            1493 non-null   float64\n",
      " 15  combined_instruction             1493 non-null   object \n",
      "dtypes: float64(4), int64(1), object(11)\n",
      "memory usage: 186.8+ KB\n"
     ]
    }
   ],
   "source": [
    "benchmark_v4 = pd.read_csv(\"./benchmark/benchmark_v4.csv\")\n",
    "benchmark_v4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1493 entries, 0 to 1492\n",
      "Data columns (total 16 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               1493 non-null   int64  \n",
      " 1   dataset                          1493 non-null   object \n",
      " 2   instruction                      1493 non-null   object \n",
      " 3   code                             1493 non-null   object \n",
      " 4   test                             594 non-null    object \n",
      " 5   relevant_categories              1493 non-null   object \n",
      " 6   simplified_instruction           1493 non-null   object \n",
      " 7   extracted_constraints            1493 non-null   object \n",
      " 8   final_comprehensive_constraints  1493 non-null   object \n",
      " 9   filtered_relevant_constraints    1493 non-null   object \n",
      " 10  quality_scores                   1493 non-null   object \n",
      " 11  relevance_score                  1493 non-null   float64\n",
      " 12  objectivity_score                1493 non-null   float64\n",
      " 13  atomicity_score                  1493 non-null   float64\n",
      " 14  unified_quality_score            1493 non-null   float64\n",
      " 15  combined_instruction             1493 non-null   object \n",
      "dtypes: float64(4), int64(1), object(11)\n",
      "memory usage: 186.8+ KB\n"
     ]
    }
   ],
   "source": [
    "benchmark_v4 = benchmark_v4.drop(columns = [\"id.1\"])\n",
    "benchmark_v4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_v4.to_csv(\"../../benchmark/benchmark_v4.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Judge for Functional Correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With the given simplified instruction and model response the model should evaluate whether the generated code is correct for the instruction\n",
    "    - if the code is completely satisfies the instruction and there are no syntax or semantic errors\n",
    "    - if you have any doubts and think almost it is correct but it might not satisy some edge cases or so then mark it partially correct\n",
    "    - otherwise if it has any syntax or semantic errors then mark it as wrong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 18 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               10 non-null     int64  \n",
      " 1   dataset                          10 non-null     object \n",
      " 2   instruction                      10 non-null     object \n",
      " 3   code                             10 non-null     object \n",
      " 4   test                             5 non-null      object \n",
      " 5   relevant_categories              10 non-null     object \n",
      " 6   simplified_instruction           10 non-null     object \n",
      " 7   extracted_constraints            10 non-null     object \n",
      " 8   final_comprehensive_constraints  10 non-null     object \n",
      " 9   filtered_relevant_constraints    10 non-null     object \n",
      " 10  quality_scores                   10 non-null     object \n",
      " 11  relevance_score                  10 non-null     float64\n",
      " 12  objectivity_score                10 non-null     float64\n",
      " 13  atomicity_score                  10 non-null     float64\n",
      " 14  unified_quality_score            10 non-null     float64\n",
      " 15  combined_instruction             10 non-null     object \n",
      " 16  response                         10 non-null     object \n",
      " 17  Constraint_adherence             10 non-null     object \n",
      "dtypes: float64(4), int64(1), object(13)\n",
      "memory usage: 1.5+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_path = \"./benchmark/LLMjudge_outputs/constraint_adherence/Llama-3.1-8B-Instruct_results_llmjudge.jsonl\"\n",
    "df = pd.read_json(input_path,lines=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the response might contain  other parts that are not part of the instruction provided, don't worry about that and evaluate only on the simplified instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert Python developer and code reviewer.  \n",
      "    Your task is to evaluate whether a given Python code correctly follows the instruction.\n",
      "    \n",
      "    Evaluation Criteria:\n",
      "    - Completely Correct: If the code fully satisfies the instruction, and there are no syntax or semantic errors.\n",
      "    - Partially Correct: If the code is mostly satisfies the instruction and completely correct with no syntax or semantic errors, but may miss some edge cases or implementation details.\n",
      "    - Wrong: If the code has syntax errors, semantic errors, or clearly does not follow the instruction.\n",
      "    \n",
      "    Output Format:\n",
      "    Return your final evaluation as a dictionary without any explanation:\n",
      "    {\"reason\": \"<Your reason for the evaluation>\",\n",
      "      \"correctness\" :\"Completely Correct/Partially Correct/Wrong\",\n",
      "      }\n",
      "    \n",
      "    Input:\n",
      "    Instruction:  \n",
      "    Generates and plots a complex wave with a specified amplitude and frequency over given time points, applying a Hann window to reduce edge effects. The wave is represented as a complex number where the real part is the cosine component, and the imaginary part is the sine component. It returns both the wave and the plot object.\n",
      "Note that: Notes: The plot title is \"Complex Wave with Hann Window\". The x-label of the plot is \"Time\". The y-label of the plot is \"Amplitude\". The plot displays both the real and imaginary parts of the complex wave.\n",
      "The function should output with:\n",
      "    numpy.ndarray: The generated complex wave as a numpy array of complex numbers.\n",
      "    matplotlib.figure.Figure: The figure object of the plot.\n",
      "    matplotlib.axes.Axes: The axes object of the plot.\n",
      "You should write self-contained code starting with:\n",
      "```\n",
      "import numpy as np\n",
      "import math\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import get_window\n",
      "def task_func(amplitude, frequency, time):\n",
      "```\n",
      "    \n",
      "    Generated Code:  \n",
      "    ```python\n",
      "    ```python\n",
      "import numpy as np\n",
      "import math\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import get_window\n",
      "\n",
      "def task_func(amplitude, frequency, time):\n",
      "    \"\"\"\n",
      "    Generates and plots a complex wave with a specified amplitude and frequency over given time points,\n",
      "    applying a Hann window to reduce edge effects.\n",
      "\n",
      "    Args:\n",
      "        amplitude (float): The amplitude of the wave.\n",
      "        frequency (float): The frequency of the wave.\n",
      "        time (numpy.ndarray): The time points at which the wave is generated.\n",
      "\n",
      "    Returns:\n",
      "        numpy.ndarray: The generated complex wave as a numpy array of complex numbers.\n",
      "        matplotlib.figure.Figure: The figure object of the plot.\n",
      "        matplotlib.axes.Axes: The axes object of the plot.\n",
      "    \"\"\"\n",
      "\n",
      "    # Generate the complex wave using the formula wave = amplitude * np.exp(1j * 2 * math.pi * frequency * time)\n",
      "    wave = amplitude * np.exp(1j * 2 * math.pi * frequency * time)\n",
      "\n",
      "    # Apply a Hann window to the wave to reduce edge effects\n",
      "    hann_window = get_window('hann', len(time))\n",
      "    wave_windowed = wave * hann_window[:, None]\n",
      "\n",
      "    # Create a new figure and axis object\n",
      "    fig, ax = plt.subplots()\n",
      "\n",
      "    # Plot the real and imaginary parts of the complex wave\n",
      "    ax.plot(time, np.real(wave_windowed), label='Real part')\n",
      "    ax.plot(time, np.imag(wave_windowed), label='Imaginary part')\n",
      "\n",
      "    # Set the plot title, x-label, and y-label\n",
      "    ax.set_title('Complex Wave with Hann Window')\n",
      "    ax.set_xlabel('Time')\n",
      "    ax.set_ylabel('Amplitude')\n",
      "\n",
      "    # Add a legend to the plot\n",
      "    ax.legend()\n",
      "\n",
      "    # Return the generated complex wave and the plot object\n",
      "    return wave_windowed, fig, ax\n",
      "\n",
      "# Example usage:\n",
      "time = np.linspace(0, 1, 1000 \n"
     ]
    }
   ],
   "source": [
    "def code_correctness_prompt_v1(instruction,generated_code):\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert Python developer and code reviewer.  \n",
    "    Your task is to evaluate whether a given Python code correctly follows the instruction.\n",
    "    \n",
    "    Evaluation Criteria:\n",
    "    - Completely Correct: If the code fully satisfies the instruction, and there are no syntax or semantic errors.\n",
    "    - Partially Correct: If the code is mostly satisfies the instruction and completely correct with no syntax or semantic errors, but may miss some edge cases or implementation details.\n",
    "    - Wrong: If the code has syntax errors, semantic errors, or clearly does not follow the instruction.\n",
    "    \n",
    "    Output Format:\n",
    "    Return your final evaluation as a dictionary without any explanation:\n",
    "    {{\"reason\": \"<Your reason for the evaluation>\",\n",
    "      \"correctness\" :\"Completely Correct/Partially Correct/Wrong\",\n",
    "      }}\n",
    "    \n",
    "    Input:\n",
    "    Instruction:  \n",
    "    {instruction}\n",
    "    \n",
    "    Generated Code:  \n",
    "    ```python\n",
    "    {generated_code} \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "prompt = code_correctness_prompt_v1(df .iloc[0][\"instruction\"], df.iloc[0][\"response\"])\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|                                                                                                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.61it/s]\n"
     ]
    }
   ],
   "source": [
    "client = create_clients(mode=\"GPT\")\n",
    "instructions = df[\"simplified_instruction\"].to_list()\n",
    "model_responses = df[\"response\"].to_list()\n",
    "prompts = [code_correctness_prompt_v1(instruction, response) for instruction, response in zip(instructions, model_responses)]\n",
    "correctness_evaluation = client.get_model_response_batch(\n",
    "    user_prompts=prompts,\n",
    "    system_prompt=None,\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"reason\": \"The code has a syntax error due to an incomplete line at the end, which prevents it from running correctly.\", \"correctness\": \"Wrong\"}',\n",
       " '{\"reason\": \"The code correctly implements the functionality to remove jQuery files from a specified directory, handles exceptions, and logs the removed files. It also adheres to the specified output format.\", \"correctness\": \"Completely Correct\"}',\n",
       " '{\"reason\": \"The code correctly defines the URL patterns for the Django application as per the instructions, including all required views and handling of media files in development. There are no syntax or semantic errors.\", \"correctness\": \"Completely Correct\"}',\n",
       " '```json\\n{\"reason\": \"The code is incomplete and does not implement the full functionality required by the instruction. It lacks the logic for transitioning statuses based on bids, complaints, and auction results, as well as unit tests to verify the transitions.\", \"correctness\": \"Wrong\"}\\n```',\n",
       " '{\"reason\": \"The code is incomplete and does not return the trained LinearRegression model as required by the instruction. It also has an unfinished pipeline definition.\", \"correctness\": \"Wrong\"}',\n",
       " '{\"reason\": \"The code correctly implements the solution by using a dictionary to specify different aggregation functions for each column in the pivot table, and it does not contain any syntax or semantic errors.\", \"correctness\" :\"Completely Correct\"}',\n",
       " '{\"reason\": \"The code correctly uses the `merge_asof` function to join the two dataframes based on the timestamp, fulfilling the requirement of not looping through every row. It also handles NaN values appropriately and reorders the columns as specified.\", \"correctness\": \"Completely Correct\"}',\n",
       " '{\"reason\": \"The code correctly implements the recursive function to remove vowels and reverse the string. However, there is a mistake in the test case for multi-line strings where the expected output is incorrect due to a missing character in the reversed string.\", \"correctness\": \"Partially Correct\"}',\n",
       " '{\"reason\": \"The code correctly implements the function to compute the area of a triangle using Heron\\'s formula and includes proper validation for the triangle\\'s sides. It raises an appropriate exception for invalid triangles and handles input types correctly.\", \"correctness\": \"Completely Correct\"}',\n",
       " '{\"reason\": \"The code correctly implements the requirement to return the third non-whitespace character from the input string using a single loop iteration. It handles leading and trailing whitespace as specified and does not use any built-in functions or libraries.\", \"correctness\": \"Completely Correct\"}']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 19 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               10 non-null     int64  \n",
      " 1   dataset                          10 non-null     object \n",
      " 2   instruction                      10 non-null     object \n",
      " 3   code                             10 non-null     object \n",
      " 4   test                             5 non-null      object \n",
      " 5   relevant_categories              10 non-null     object \n",
      " 6   simplified_instruction           10 non-null     object \n",
      " 7   extracted_constraints            10 non-null     object \n",
      " 8   final_comprehensive_constraints  10 non-null     object \n",
      " 9   filtered_relevant_constraints    10 non-null     object \n",
      " 10  quality_scores                   10 non-null     object \n",
      " 11  relevance_score                  10 non-null     float64\n",
      " 12  objectivity_score                10 non-null     float64\n",
      " 13  atomicity_score                  10 non-null     float64\n",
      " 14  unified_quality_score            10 non-null     float64\n",
      " 15  combined_instruction             10 non-null     object \n",
      " 16  response                         10 non-null     object \n",
      " 17  Constraint_adherence             10 non-null     object \n",
      " 18  correctness_evaluation           10 non-null     object \n",
      "dtypes: float64(4), int64(1), object(14)\n",
      "memory usage: 1.6+ KB\n"
     ]
    }
   ],
   "source": [
    "correctness_evaluation = [extract_json(response, \"correctness\") for response in correctness_evaluation]\n",
    "df[\"correctness_evaluation\"] = correctness_evaluation\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>instruction</th>\n",
       "      <th>code</th>\n",
       "      <th>test</th>\n",
       "      <th>relevant_categories</th>\n",
       "      <th>simplified_instruction</th>\n",
       "      <th>extracted_constraints</th>\n",
       "      <th>final_comprehensive_constraints</th>\n",
       "      <th>filtered_relevant_constraints</th>\n",
       "      <th>quality_scores</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>objectivity_score</th>\n",
       "      <th>atomicity_score</th>\n",
       "      <th>unified_quality_score</th>\n",
       "      <th>combined_instruction</th>\n",
       "      <th>response</th>\n",
       "      <th>Constraint_adherence</th>\n",
       "      <th>correctness_evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>955</td>\n",
       "      <td>bigcode/bigcodebench</td>\n",
       "      <td>Generates and plots a complex wave with a spec...</td>\n",
       "      <td>wave = amplitude * np.exp(1j * 2 * math.pi * f...</td>\n",
       "      <td>import unittest\\nimport numpy as np\\nimport ma...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Generates and plots a complex wave with a spec...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>{'constraint_evaluations': [{'constraint_text'...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Generates and plots a complex wave with a spec...</td>\n",
       "      <td>```python\\nimport numpy as np\\nimport math\\nim...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>Wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>906</td>\n",
       "      <td>bigcode/bigcodebench</td>\n",
       "      <td>Removes all jQuery files (JavaScript files con...</td>\n",
       "      <td># Configure logging\\n    logging.basicConfig(f...</td>\n",
       "      <td>import unittest\\nfrom unittest.mock import Mag...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Removes all jQuery files (JavaScript files con...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>{'constraint_evaluations': [{'constraint_text'...</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.73</td>\n",
       "      <td>Removes all jQuery files (JavaScript files con...</td>\n",
       "      <td>```python\\nimport os\\nimport logging\\n\\ndef ta...</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>Completely Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1346</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>You are tasked with creating a Django applicat...</td>\n",
       "      <td>```python\\n# Import necessary Django modules\\n...</td>\n",
       "      <td>None</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>You are tasked with creating a Django applicat...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>{'constraint_evaluations': [{'constraint_text'...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.75</td>\n",
       "      <td>You are tasked with creating a Django applicat...</td>\n",
       "      <td>Here's a Python script that defines the URL pa...</td>\n",
       "      <td>[1, 1, 1, 0]</td>\n",
       "      <td>Completely Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>989</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>Design a system to automate the transition of ...</td>\n",
       "      <td>```python\\n# -*- coding: utf-8 -*-\\nimport uni...</td>\n",
       "      <td>None</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Design a system to automate the transition of ...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>{'constraint_evaluations': [{'constraint_text'...</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4.23</td>\n",
       "      <td>4.23</td>\n",
       "      <td>4.44</td>\n",
       "      <td>Design a system to automate the transition of ...</td>\n",
       "      <td>**System Design**\\n\\nWe will design a system u...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]</td>\n",
       "      <td>Wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>964</td>\n",
       "      <td>bigcode/bigcodebench</td>\n",
       "      <td>Train a linear regression model on a given Dat...</td>\n",
       "      <td>if not isinstance(df, pd.DataFrame):\\n        ...</td>\n",
       "      <td>import unittest\\nimport pandas as pd\\nfrom io ...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Train a linear regression model on a given Dat...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>{'constraint_evaluations': [{'constraint_text'...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Train a linear regression model on a given Dat...</td>\n",
       "      <td>```python\\nimport pandas as pd\\nfrom sklearn.m...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>Partially Correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                      dataset  \\\n",
       "0   955                         bigcode/bigcodebench   \n",
       "1   906                         bigcode/bigcodebench   \n",
       "2  1346  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "3   989  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "4   964                         bigcode/bigcodebench   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  Generates and plots a complex wave with a spec...   \n",
       "1  Removes all jQuery files (JavaScript files con...   \n",
       "2  You are tasked with creating a Django applicat...   \n",
       "3  Design a system to automate the transition of ...   \n",
       "4  Train a linear regression model on a given Dat...   \n",
       "\n",
       "                                                code  \\\n",
       "0  wave = amplitude * np.exp(1j * 2 * math.pi * f...   \n",
       "1  # Configure logging\\n    logging.basicConfig(f...   \n",
       "2  ```python\\n# Import necessary Django modules\\n...   \n",
       "3  ```python\\n# -*- coding: utf-8 -*-\\nimport uni...   \n",
       "4  if not isinstance(df, pd.DataFrame):\\n        ...   \n",
       "\n",
       "                                                test  \\\n",
       "0  import unittest\\nimport numpy as np\\nimport ma...   \n",
       "1  import unittest\\nfrom unittest.mock import Mag...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4  import unittest\\nimport pandas as pd\\nfrom io ...   \n",
       "\n",
       "                                 relevant_categories  \\\n",
       "0  ['Code Structure and Modularity', 'Input and O...   \n",
       "1  ['Code Structure and Modularity', 'Input and O...   \n",
       "2  ['Code Structure and Modularity', 'Input and O...   \n",
       "3  ['Code Structure and Modularity', 'Input and O...   \n",
       "4  ['Code Structure and Modularity', 'Input and O...   \n",
       "\n",
       "                              simplified_instruction  \\\n",
       "0  Generates and plots a complex wave with a spec...   \n",
       "1  Removes all jQuery files (JavaScript files con...   \n",
       "2  You are tasked with creating a Django applicat...   \n",
       "3  Design a system to automate the transition of ...   \n",
       "4  Train a linear regression model on a given Dat...   \n",
       "\n",
       "                               extracted_constraints  \\\n",
       "0  [{'type': 'Input and Output Handling', 'constr...   \n",
       "1  [{'type': 'File and Data Management', 'constra...   \n",
       "2  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "3  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "4  [{'type': 'Error Handling and Robustness', 'co...   \n",
       "\n",
       "                     final_comprehensive_constraints  \\\n",
       "0  [{'type': 'Input and Output Handling', 'constr...   \n",
       "1  [{'type': 'File and Data Management', 'constra...   \n",
       "2  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "3  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "4  [{'type': 'Error Handling and Robustness', 'co...   \n",
       "\n",
       "                       filtered_relevant_constraints  \\\n",
       "0  [{'type': 'Input and Output Handling', 'constr...   \n",
       "1  [{'type': 'File and Data Management', 'constra...   \n",
       "2  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "3  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "4  [{'type': 'Error Handling and Robustness', 'co...   \n",
       "\n",
       "                                      quality_scores  relevance_score  \\\n",
       "0  {'constraint_evaluations': [{'constraint_text'...             5.00   \n",
       "1  {'constraint_evaluations': [{'constraint_text'...             4.80   \n",
       "2  {'constraint_evaluations': [{'constraint_text'...             5.00   \n",
       "3  {'constraint_evaluations': [{'constraint_text'...             4.85   \n",
       "4  {'constraint_evaluations': [{'constraint_text'...             5.00   \n",
       "\n",
       "   objectivity_score  atomicity_score  unified_quality_score  \\\n",
       "0               5.00             5.00                   5.00   \n",
       "1               4.60             4.80                   4.73   \n",
       "2               4.50             4.75                   4.75   \n",
       "3               4.23             4.23                   4.44   \n",
       "4               5.00             5.00                   5.00   \n",
       "\n",
       "                                combined_instruction  \\\n",
       "0  Generates and plots a complex wave with a spec...   \n",
       "1  Removes all jQuery files (JavaScript files con...   \n",
       "2  You are tasked with creating a Django applicat...   \n",
       "3  Design a system to automate the transition of ...   \n",
       "4  Train a linear regression model on a given Dat...   \n",
       "\n",
       "                                            response  \\\n",
       "0  ```python\\nimport numpy as np\\nimport math\\nim...   \n",
       "1  ```python\\nimport os\\nimport logging\\n\\ndef ta...   \n",
       "2  Here's a Python script that defines the URL pa...   \n",
       "3  **System Design**\\n\\nWe will design a system u...   \n",
       "4  ```python\\nimport pandas as pd\\nfrom sklearn.m...   \n",
       "\n",
       "                         Constraint_adherence correctness_evaluation  \n",
       "0                    [1, 1, 1, 1, 1, 1, 1, 1]                  Wrong  \n",
       "1                             [1, 1, 1, 1, 1]     Completely Correct  \n",
       "2                                [1, 1, 1, 0]     Completely Correct  \n",
       "3  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]                  Wrong  \n",
       "4                                      [1, 0]      Partially Correct  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 3 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   instruction             10 non-null     object\n",
      " 1   response                10 non-null     object\n",
      " 2   correctness_evaluation  10 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 368.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_new =df[[\"simplified_instruction\",\"response\",\"correctness_evaluation\"]].copy()\n",
    "df_new.columns = [\"instruction\", \"response\", \"correctness_evaluation\"]\n",
    "df_new.info()\n",
    "df_new.head()\n",
    "df_new.to_json(\"./benchmark/LLMjudge_outputs/code_correctness/Llama-3.1-8B-Instruct_results_llmjudge_with_correctness.jsonl\", orient=\"records\", lines=True, force_ascii=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Generates and plots a complex wave with a specified amplitude and frequency over given time points, applying a Hann window to reduce edge effects. The wave is represented as a complex number where the real part is the cosine component, and the imaginary part is the sine component. It returns both the wave and the plot object. Note that: The plot title is \"Complex Wave with Hann Window\". The x-label of the plot is \"Time\". The y-label of the plot is \"Amplitude\". The plot displays both the real and imaginary parts of the complex wave. You should write self-contained code starting with:\n",
      "```\n",
      "import numpy as np\n",
      "import math\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import get_window\n",
      "def task_func(amplitude, frequency, time):\n",
      "```\n",
      "Response: ```python\n",
      "import numpy as np\n",
      "import math\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import get_window\n",
      "\n",
      "def task_func(amplitude, frequency, time):\n",
      "    \"\"\"\n",
      "    Generates and plots a complex wave with a specified amplitude and frequency over given time points,\n",
      "    applying a Hann window to reduce edge effects.\n",
      "\n",
      "    Args:\n",
      "        amplitude (float): The amplitude of the wave.\n",
      "        frequency (float): The frequency of the wave.\n",
      "        time (numpy.ndarray): The time points at which the wave is generated.\n",
      "\n",
      "    Returns:\n",
      "        numpy.ndarray: The generated complex wave as a numpy array of complex numbers.\n",
      "        matplotlib.figure.Figure: The figure object of the plot.\n",
      "        matplotlib.axes.Axes: The axes object of the plot.\n",
      "    \"\"\"\n",
      "\n",
      "    # Generate the complex wave using the formula wave = amplitude * np.exp(1j * 2 * math.pi * frequency * time)\n",
      "    wave = amplitude * np.exp(1j * 2 * math.pi * frequency * time)\n",
      "\n",
      "    # Apply a Hann window to the wave to reduce edge effects\n",
      "    hann_window = get_window('hann', len(time))\n",
      "    wave_windowed = wave * hann_window[:, None]\n",
      "\n",
      "    # Create a new figure and axis object\n",
      "    fig, ax = plt.subplots()\n",
      "\n",
      "    # Plot the real and imaginary parts of the complex wave\n",
      "    ax.plot(time, np.real(wave_windowed), label='Real part')\n",
      "    ax.plot(time, np.imag(wave_windowed), label='Imaginary part')\n",
      "\n",
      "    # Set the plot title, x-label, and y-label\n",
      "    ax.set_title('Complex Wave with Hann Window')\n",
      "    ax.set_xlabel('Time')\n",
      "    ax.set_ylabel('Amplitude')\n",
      "\n",
      "    # Add a legend to the plot\n",
      "    ax.legend()\n",
      "\n",
      "    # Return the generated complex wave and the plot object\n",
      "    return wave_windowed, fig, ax\n",
      "\n",
      "# Example usage:\n",
      "time = np.linspace(0, 1, 1000\n",
      "Correctness Evaluation: Wrong\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Removes all jQuery files (JavaScript files containing 'jquery' in their name) from a specified directory. The function should output with: tuple: A tuple containing two elements: int: The number of files removed. list: The names of the removed files. You should write self-contained code starting with:\n",
      "```\n",
      "import os\n",
      "import logging\n",
      "def task_func(directory):\n",
      "```\n",
      "Response: ```python\n",
      "import os\n",
      "import logging\n",
      "\n",
      "def task_func(directory):\n",
      "    \"\"\"\n",
      "    Removes all jQuery files from a specified directory and logs the removed files.\n",
      "\n",
      "    Args:\n",
      "        directory (str): The path to the directory containing jQuery files.\n",
      "\n",
      "    Returns:\n",
      "        tuple: A tuple containing the number of files removed and a list of the names of the removed files.\n",
      "\n",
      "    Raises:\n",
      "        FileNotFoundError: If the specified directory does not exist.\n",
      "    \"\"\"\n",
      "\n",
      "    # Set up logging to log the removed files\n",
      "    logging.basicConfig(filename='jquery_removal.log', level=logging.INFO, format='%(message)s')\n",
      "\n",
      "    # Check if the directory exists\n",
      "    if not os.path.exists(directory):\n",
      "        raise FileNotFoundError(f\"The directory '{directory}' does not exist.\")\n",
      "\n",
      "    # Initialize variables to keep track of the number of removed files and their names\n",
      "    removed_count = 0\n",
      "    removed_files = []\n",
      "\n",
      "    # Iterate over all files in the directory\n",
      "    for filename in os.listdir(directory):\n",
      "        # Construct the full path to the file\n",
      "        filepath = os.path.join(directory, filename)\n",
      "\n",
      "        # Check if the file is a jQuery file (contains 'jquery' in its name)\n",
      "        if 'jquery' in filename.lower():\n",
      "            try:\n",
      "                # Attempt to remove the file\n",
      "                os.remove(filepath)\n",
      "                # Log the removed file\n",
      "                logging.info(f\"Removed file: {filename}\")\n",
      "                # Increment the removed count and add the file name to the list\n",
      "                removed_count += 1\n",
      "                removed_files.append(filename)\n",
      "            except Exception as e:\n",
      "                # Log any errors that occur during file removal\n",
      "                logging.error(f\"Error removing file '{filename}': {str(e)}\")\n",
      "\n",
      "    # Return the number of removed files and their names\n",
      "    return removed_count, removed_files\n",
      "```\n",
      "\n",
      "Example usage:\n",
      "\n",
      "```python\n",
      "try:\n",
      "    removed_count, removed_files = task_func('/path/to/directory')\n",
      "    print(f\"Removed {removed_count} files: {removed_files\n",
      "Correctness Evaluation: Completely Correct\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: You are tasked with creating a Django application for managing a movie database. The application should allow users to view a list of movies, access detailed information about a specific movie, and create new entries for movies, studios, halls, and countries. The application should also handle media files correctly.\n",
      "\n",
      "Write a Python script that defines the URL patterns for the Django application. The script should include URL patterns for the following views:\n",
      "- `index`: A view that displays the list of movies.\n",
      "- `movie_detail`: A view that displays detailed information about a specific movie, accessed via a unique slug.\n",
      "- `movie_create`: A view that provides a form for creating a new movie entry.\n",
      "- `studio_create`: A view that provides a form for creating a new studio entry.\n",
      "- `hall_create`: A view that provides a form for creating a new hall entry.\n",
      "- `country_create`: A view that provides a form for creating a new country entry.\n",
      "\n",
      "Additionally, ensure that the application serves media files in development by appending the necessary static URL patterns.\n",
      "Response: Here's a Python script that defines the URL patterns for the Django application:\n",
      "\n",
      "```python\n",
      "# myproject/urls.py\n",
      "\n",
      "from django.contrib import admin\n",
      "from django.urls import path, include\n",
      "from django.conf import settings\n",
      "from django.conf.urls.static import static\n",
      "\n",
      "urlpatterns = [\n",
      "    path('admin/', admin.site.urls),\n",
      "    path('', include('movie_app.urls')),\n",
      "]\n",
      "\n",
      "if settings.DEBUG:\n",
      "    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\n",
      "```\n",
      "\n",
      "```python\n",
      "# movie_app/urls.py\n",
      "\n",
      "from django.urls import path\n",
      "from . import views\n",
      "\n",
      "urlpatterns = [\n",
      "    path('', views.index, name='index'),\n",
      "    path('movie/<slug:slug>/', views.movie_detail, name='movie_detail'),\n",
      "    path('movie/create/', views.movie_create, name='movie_create'),\n",
      "    path('studio/create/', views.studio_create, name='studio_create'),\n",
      "    path('hall/create/', views.hall_create, name='hall_create'),\n",
      "    path('country/create/', views.country_create, name='country_create'),\n",
      "]\n",
      "```\n",
      "\n",
      "In the above code:\n",
      "\n",
      "- `myproject/urls.py` includes the main URL configuration for the project, which includes the `movie_app` URL configuration.\n",
      "- `movie_app/urls.py` defines the URL patterns for the `movie_app` application. It includes URL patterns for the views: `index`, `movie_detail`, `movie_create`, `studio_create`, `hall_create`, and `country_create`.\n",
      "\n",
      "To serve media files in development, we append the necessary static URL patterns to the `urlpatterns` list in `myproject/urls.py`. This is done using the `static` function from `django.conf.urls.static`, which takes the `MEDIA_URL` and `MEDIA_ROOT` settings as arguments.\n",
      "\n",
      "**Settings**\n",
      "\n",
      "In your `settings.py` file, you should have the following settings:\n",
      "\n",
      "```python\n",
      "# settings.py\n",
      "\n",
      "MEDIA_ROOT = os.path.join(BASE_DIR, 'media')\n",
      "MEDIA_URL = '/media/'\n",
      "``\n",
      "Correctness Evaluation: Completely Correct\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Design a system to automate the transition of tender statuses in an e-procurement platform. The system should handle different types of tenders, such as below-threshold, open UA, and open UA defense tenders. Each tender type has its own set of rules for transitioning between statuses based on the number of bids, complaints, and auction results.\n",
      "\n",
      "The system should be able to:\n",
      "- Transition a tender to 'unsuccessful' if there are no bids.\n",
      "- Set the auction period for tenders without bids.\n",
      "- Transition a tender to 'qualification' if there is at least one bid.\n",
      "- Handle complaints and transition tenders accordingly.\n",
      "- Transition a tender to 'auction' when it's ready for auction.\n",
      "- Handle tenders with lots, including setting auction periods and transitioning lot statuses.\n",
      "\n",
      "Write a set of unit tests to verify that the system correctly transitions tenders and lots between statuses under various conditions. Use the provided code snippet as a starting point for creating the test cases.\n",
      "Response: **System Design**\n",
      "\n",
      "We will design a system using Python classes to handle the transition of tender statuses. We will use a dictionary to store the rules for each tender type.\n",
      "\n",
      "```python\n",
      "import logging\n",
      "from dataclasses import dataclass\n",
      "from enum import Enum\n",
      "from typing import Dict, List\n",
      "\n",
      "# Define the tender status enum\n",
      "class TenderStatus(Enum):\n",
      "    PENDING = 1\n",
      "    QUALIFICATION = 2\n",
      "    AUCTION = 3\n",
      "    UNSUCCESSFUL = 4\n",
      "\n",
      "# Define the tender type enum\n",
      "class TenderType(Enum):\n",
      "    BELOW_THRESHOLD = 1\n",
      "    OPEN_UA = 2\n",
      "    OPEN_UA_DEFENSE = 3\n",
      "\n",
      "# Define the tender data class\n",
      "@dataclass\n",
      "class Tender:\n",
      "    id: int\n",
      "    type: TenderType\n",
      "    bids: int\n",
      "    complaints: int\n",
      "    auction_period: int\n",
      "    lots: List['Lot']\n",
      "\n",
      "# Define the lot data class\n",
      "@dataclass\n",
      "class Lot:\n",
      "    id: int\n",
      "    auction_period: int\n",
      "\n",
      "# Define the tender manager class\n",
      "class TenderManager:\n",
      "    def __init__(self):\n",
      "        self.rules = {\n",
      "            TenderType.BELOW_THRESHOLD: self._below_threshold_rules,\n",
      "            TenderType.OPEN_UA: self._open_ua_rules,\n",
      "            TenderType.OPEN_UA_DEFENSE: self._open_ua_defense_rules,\n",
      "        }\n",
      "        self.logger = logging.getLogger(__name__)\n",
      "\n",
      "    def transition_status(self, tender: Tender) -> bool:\n",
      "        if not tender:\n",
      "            self.logger.error(\"Invalid tender\")\n",
      "            return False\n",
      "\n",
      "        if tender.type not in self.rules:\n",
      "            self.logger.error(\"Invalid tender type\")\n",
      "            return False\n",
      "\n",
      "        rules = self.rules[tender.type]\n",
      "        if not rules(tender):\n",
      "            self.logger.error(\"Failed to transition tender status\")\n",
      "            return False\n",
      "\n",
      "        return True\n",
      "\n",
      "    def _below_threshold_rules(self, tender: Tender) -> bool:\n",
      "        if tender.bids == 0\n",
      "Correctness Evaluation: Wrong\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Train a linear regression model on a given DataFrame. The function should output with: LinearRegression: The trained linear regression model. You should write self-contained code starting with:\n",
      "```\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# Constants\n",
      "FEATURES = ['feature '+str(i) for i in range(1, 11)]\n",
      "TARGET = 'target'\n",
      "def task_func(df):\n",
      "```\n",
      "Response: ```python\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "import numpy as np\n",
      "\n",
      "# Constants\n",
      "FEATURES = ['feature '+str(i) for i in range(1, 11)]\n",
      "TARGET = 'target'\n",
      "\n",
      "def task_func(df):\n",
      "    \"\"\"\n",
      "    Train a linear regression model on a given DataFrame.\n",
      "\n",
      "    Args:\n",
      "        df (pd.DataFrame): Input DataFrame.\n",
      "\n",
      "    Returns:\n",
      "        LinearRegression: The trained linear regression model.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the input df is not a DataFrame.\n",
      "    \"\"\"\n",
      "    # Check if input is a DataFrame\n",
      "    if not isinstance(df, pd.DataFrame):\n",
      "        raise ValueError(\"Input must be a pandas DataFrame\")\n",
      "\n",
      "    # Check if DataFrame is empty\n",
      "    if df.empty:\n",
      "        raise ValueError(\"Input DataFrame is empty\")\n",
      "\n",
      "    # Check if required columns exist in the DataFrame\n",
      "    if TARGET not in df.columns or not all(feature in df.columns for feature in FEATURES):\n",
      "        raise ValueError(\"Required columns are missing from the DataFrame\")\n",
      "\n",
      "    # Split data into features and target\n",
      "    X = df[FEATURES]\n",
      "    y = df[TARGET]\n",
      "\n",
      "    # Split data into training and testing sets\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "    # Define preprocessing pipeline\n",
      "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
      "    categorical_features = X.select_dtypes(include=['object']).columns\n",
      "\n",
      "    numeric_transformer = Pipeline(steps=[\n",
      "        ('imputer', SimpleImputer(strategy='median')),\n",
      "        ('scaler', StandardScaler())])\n",
      "\n",
      "    categorical_transformer = Pipeline(steps=[\n",
      "        ('imputer', SimpleImputer(strategy='constant\n",
      "Correctness Evaluation: Partially Correct\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Problem: Was trying to generate a pivot table with multiple \"values\" columns. I know I can use aggfunc to aggregate values the way I want to, but what if I don't want to sum or avg both columns but instead I want sum of one column while mean of the other one. So is it possible to do so using pandas?\n",
      "\n",
      "Now this will get a pivot table with sum:\n",
      "\n",
      "pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\n",
      "And this for mean:\n",
      "pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\n",
      "How can I get sum for D and mean for E?\n",
      "\n",
      "Hope my question is clear enough.\n",
      "\n",
      "A:\n",
      "<code>\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "np.random.seed(1)\n",
      "df = pd.DataFrame({\n",
      "'A' : ['abc', 'def', 'xyz', 'abc'] * 3,\n",
      "'B' : ['A', 'B', 'C'] * 4,\n",
      "'D' : np.random.randn(12),\n",
      "'E' : np.random.randn(12)\n",
      "})\n",
      "</code>\n",
      "result = ... # put solution in this variable\n",
      "Response: You can achieve this by passing a dictionary to the `aggfunc` parameter of the `pivot_table` function. This dictionary will map the column names to their respective aggregation functions.\n",
      "\n",
      "Here's the solution:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "np.random.seed(1)\n",
      "df = pd.DataFrame({\n",
      "    'A' : ['abc', 'def', 'xyz', 'abc'] * 3,\n",
      "    'B' : ['A', 'B', 'C'] * 4,\n",
      "    'D' : np.random.randn(12),\n",
      "    'E' : np.random.randn(12)\n",
      "})\n",
      "\n",
      "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
      "\n",
      "print(result)\n",
      "```\n",
      "\n",
      "In this code:\n",
      "\n",
      "- We create a dictionary `{'D': np.sum, 'E': np.mean}` that maps the column names 'D' and 'E' to their respective aggregation functions `np.sum` and `np.mean`.\n",
      "- We pass this dictionary to the `aggfunc` parameter of the `pivot_table` function.\n",
      "- The `pivot_table` function will apply the specified aggregation functions to the respective columns in the pivot table.\n",
      "\n",
      "This will produce a pivot table where the values in column 'D' are summed and the values in column 'E' are averaged.\n",
      "Correctness Evaluation: Completely Correct\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Problem:\n",
      "Say I have two dataframes:\n",
      "df1:                          df2:\n",
      "+-------------------+----+    +-------------------+-----+\n",
      "|  Timestamp        |data|    |  Timestamp        |stuff|\n",
      "+-------------------+----+    +-------------------+-----+\n",
      "|2019/04/02 11:00:01| 111|    |2019/04/02 11:00:14|  101|\n",
      "|2019/04/02 11:00:15| 222|    |2019/04/02 11:00:15|  202|\n",
      "|2019/04/02 11:00:29| 333|    |2019/04/02 11:00:16|  303|\n",
      "|2019/04/02 11:00:30| 444|    |2019/04/02 11:00:30|  404|\n",
      "+-------------------+----+    |2019/04/02 11:00:31|  505|\n",
      "                              +-------------------+-----+\n",
      "\n",
      "Without looping through every row of df2, I am trying to join the two dataframes based on the timestamp. So for every row in df2, it will \"add\" data from df1 that was at that particular time. In this example, the resulting dataframe would be:\n",
      "Adding df1 data to df2:\n",
      "+-------------------+-----+----+\n",
      "|  Timestamp        |stuff|data|\n",
      "+-------------------+-----+----+\n",
      "|2019/04/02 11:00:14|  101| 222|\n",
      "|2019/04/02 11:00:15|  202| 222|\n",
      "|2019/04/02 11:00:16|  303| 333|\n",
      "|2019/04/02 11:00:30|  404| 444|\n",
      "|2019/04/02 11:00:31|  505|None|\n",
      "+-------------------+-----+----+\n",
      "\n",
      "Looping through each row of df2 then comparing to each df1 is very inefficient. Is there another way?\n",
      "\n",
      "\n",
      "A:\n",
      "<code>\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "df1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n",
      "                    'data': [111, 222, 333, 444]})\n",
      "df2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n",
      "                    'stuff': [101, 202, 303, 404, 505]})\n",
      "df1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\n",
      "df2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\n",
      "</code>\n",
      "result = ... # put solution in this variable\n",
      "Response: To efficiently combine the dataframes based on timestamps without looping through every row of df2, you can utilize the pandas `merge_asof` function. Here's the solution:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Create dataframes\n",
      "df1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n",
      "                    'data': [111, 222, 333, 444]})\n",
      "df2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n",
      "                    'stuff': [101, 202, 303, 404, 505]})\n",
      "\n",
      "# Convert Timestamp to datetime\n",
      "df1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\n",
      "df2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\n",
      "\n",
      "# Merge df1 and df2 using merge_asof\n",
      "result = pd.merge_asof(df2, df1, on='Timestamp', direction='forward')\n",
      "\n",
      "# Fill NaN values in 'data' column with None\n",
      "result['data'] = result['data'].fillna(None)\n",
      "\n",
      "# Reorder columns to match the original order of df2\n",
      "result = result[['Timestamp', 'stuff', 'data']]\n",
      "\n",
      "print(result)\n",
      "```\n",
      "\n",
      "This solution will output:\n",
      "\n",
      "```\n",
      "                  Timestamp  stuff    data\n",
      "0 2019-04-02 11:00:14      101   222.0\n",
      "1 2019-04-02 11:00:15\n",
      "Correctness Evaluation: Completely Correct\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Write a recursive function that takes a string as an argument and removes all lowercase and uppercase vowels (a, e, i, o, u, A, E, I, O, U) from it. The function should handle special characters and symbols in the input string and return the new string in reverse order.\n",
      "Response: **Recursive Function to Remove Vowels and Reverse String**\n",
      "\n",
      "```python\n",
      "def remove_vowels_and_reverse(s):\n",
      "    \"\"\"\n",
      "    Removes all lowercase and uppercase vowels from the input string and returns the new string in reverse order.\n",
      "\n",
      "    Args:\n",
      "        s (str): The input string.\n",
      "\n",
      "    Returns:\n",
      "        str: The new string with vowels removed and in reverse order.\n",
      "    \"\"\"\n",
      "    # Base case: If the string is empty, return an empty string.\n",
      "    if not s:\n",
      "        return \"\"\n",
      "\n",
      "    # Check if the first character is a vowel.\n",
      "    if s[0].lower() in \"aeiou\":\n",
      "        # If it's a vowel, recursively call the function on the rest of the string.\n",
      "        return remove_vowels_and_reverse(s[1:])\n",
      "    else:\n",
      "        # If it's not a vowel, recursively call the function on the rest of the string and prepend the first character.\n",
      "        return s[0] + remove_vowels_and_reverse(s[1:])\n",
      "```\n",
      "\n",
      "**Unit Tests**\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "\n",
      "class TestRemoveVowelsAndReverse(unittest.TestCase):\n",
      "    def test_no_vowels(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"hello\"), \"olleh\")\n",
      "\n",
      "    def test_only_vowels(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"aeiou\"), \"\")\n",
      "\n",
      "    def test_special_characters(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"hello!@#$\"), \"!@#$olleh\")\n",
      "\n",
      "    def test_multi_line_string(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"hello\\nworld\"), \"dlrow\\olleh\")\n",
      "\n",
      "    def test_empty_string(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"\"), \"\")\n",
      "\n",
      "    def test_single_character(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"a\"), \"\")\n",
      "\n",
      "    def test_long_string(self):\n",
      "        long_string = \"a\" * 10000\n",
      "        self.assertEqual(remove_vowels_and_reverse(long_string), \"\")\n",
      "\n",
      "if\n",
      "Correctness Evaluation: Partially Correct\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Create a function to compute the area of a triangle when the length of its three sides are known. The sides of the triangle are represented by the variables a, b, and c.\n",
      "\n",
      "To check if the sides can form a valid triangle, you should use the following conditions:\n",
      "- The sum of the lengths of any two sides of a triangle must be greater than the length of the third side.\n",
      "- The difference between the length of any two sides of a triangle must be smaller than the length of the third side.\n",
      "\n",
      "If the given sides can form a valid triangle, your function should compute and return the area of the triangle using the same Heron's formula provided in the previous test.\n",
      "\n",
      "Example Usage:\n",
      "```python\n",
      "try:\n",
      "    area = compute_triangle_area(6, 8, 10)\n",
      "    print(area)  # Output: 24\n",
      "except InvalidTriangleError as e:\n",
      "    print(e)  # Output: Invalid triangle: the given sides cannot form a triangle.\n",
      "```\n",
      "\n",
      "Hints:\n",
      "- You can use the math.sqrt() function from the math module to compute the square root.\n",
      "Response: Here's a Python solution that meets the requirements:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "class InvalidTriangleError(Exception):\n",
      "    \"\"\"Custom exception for invalid triangle.\"\"\"\n",
      "    pass\n",
      "\n",
      "def compute_triangle_area(a, b, c):\n",
      "    \"\"\"\n",
      "    Compute the area of a triangle given its three sides.\n",
      "\n",
      "    Args:\n",
      "    a (int): The length of the first side.\n",
      "    b (int): The length of the second side.\n",
      "    c (int): The length of the third side.\n",
      "\n",
      "    Returns:\n",
      "    float: The area of the triangle.\n",
      "\n",
      "    Raises:\n",
      "    InvalidTriangleError: If the given sides cannot form a valid triangle.\n",
      "    \"\"\"\n",
      "    # Check if all input parameters are positive integers\n",
      "    if not (isinstance(a, int) and isinstance(b, int) and isinstance(c, int)):\n",
      "        raise TypeError(\"All sides must be integers.\")\n",
      "    if a <= 0 or b <= 0 or c <= 0:\n",
      "        raise ValueError(\"All sides must be positive.\")\n",
      "\n",
      "    # Check if the sides can form a valid triangle\n",
      "    if a + b <= c or a + c <= b or b + c <= a:\n",
      "        raise InvalidTriangleError(\"Invalid triangle: the given sides cannot form a triangle.\")\n",
      "\n",
      "    # Compute the semi-perimeter\n",
      "    s = (a + b + c) / 2\n",
      "\n",
      "    # Compute the area using Heron's formula\n",
      "    area = math.sqrt(s * (s - a) * (s - b) * (s - c))\n",
      "\n",
      "    return area\n",
      "\n",
      "# Example usage:\n",
      "try:\n",
      "    area = compute_triangle_area(6, 8, 10)\n",
      "    print(area)  # Output: 24.0\n",
      "except InvalidTriangleError as e:\n",
      "    print(e)  # Output: Invalid triangle: the given sides cannot form a triangle.\n",
      "```\n",
      "\n",
      "This solution defines a custom exception `InvalidTriangleError` to handle cases where the given sides cannot form a valid triangle. It then checks if the input\n",
      "Correctness Evaluation: Completely Correct\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Given a string ``word``, write code to return the third character in the string using a single loop iteration and without using any built-in functions or libraries. Additionally, the code should handle cases where the input string has leading or trailing whitespace characters and should return the third character after removing any leading or trailing whitespace characters.\n",
      "\n",
      "Example:\n",
      "Input: word = \"   hello   \"\n",
      "Output: \"l\"\n",
      "\n",
      "Note:\n",
      "- The input string will always have at least three characters.\n",
      "- The characters in the string can be uppercase or lowercase alphabets.\n",
      "- The input string may contain special characters or whitespace characters as well.\n",
      "Response: Here's a Python function that uses a single loop iteration to return the third character in the string, ignoring leading and trailing whitespace characters:\n",
      "\n",
      "```python\n",
      "def get_third_char(word):\n",
      "    \"\"\"\n",
      "    Returns the third character in the string, ignoring leading and trailing whitespace characters.\n",
      "\n",
      "    Args:\n",
      "        word (str): The input string.\n",
      "\n",
      "    Returns:\n",
      "        str: The third character in the string.\n",
      "    \"\"\"\n",
      "    # Initialize an index to keep track of the current character position\n",
      "    index = 0\n",
      "    \n",
      "    # Iterate over each character in the string\n",
      "    for char in word:\n",
      "        # If the current character is not a whitespace, increment the index\n",
      "        if char != ' ':\n",
      "            index += 1\n",
      "            # If the index is 3, return the current character\n",
      "            if index == 3:\n",
      "                return char\n",
      "\n",
      "# Example usage:\n",
      "word = \"   hello   \"\n",
      "print(get_third_char(word))  # Output: \"l\"\n",
      "```\n",
      "\n",
      "This function works by iterating over each character in the string. It keeps track of the current character position using an index variable. When it encounters a non-whitespace character, it increments the index. If the index reaches 3, it returns the current character. This approach ensures that the function uses a single loop iteration and handles cases where the input string has leading or trailing whitespace characters.\n",
      "Correctness Evaluation: Completely Correct\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_row(row):\n",
    "    print(f\"Instruction: {row['instruction']}\")\n",
    "    print(f\"Response: {row['response']}\")\n",
    "    print(f\"Correctness Evaluation: {row['correctness_evaluation']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "for _, row in df_new.iterrows():\n",
    "    print_row(row)\n",
    "    print(\"\\n\")  # Add a newline for better readability between rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total = 0\n",
      "count = 0\n",
      "num = 1\n",
      "\n",
      "while count < 20:\n",
      "    total += num\n",
      "    num += 2\n",
      "    count += 1\n",
      "\n",
      "print(total)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_correctness_prompt_v2(simplified_instruction,generated_code):\n",
    "    prompt = f\"\"\"You are an expert Python developer and code reviewer.  \n",
    "    Your task is to evaluate whether a given Python code correctly follows the simplified instruction, based only on three dimensions:\n",
    "\n",
    "    1. Syntax Correctness: Is the code free of syntax errors?\n",
    "    2. Semantic Correctness: Does the code make logical sense and run as intended?\n",
    "    3. Constraint Correctness: Does the code satisfy the constraints explicitly in the **simplified instruction**?\n",
    "\n",
    "    Important Notes:\n",
    "    - Only evaluate based on the **simplified instruction**. \n",
    "    - Ignore any extra functionality in the response that is not asked for in the instruction.\n",
    "    - Do not penalize for unrelated content, as long as it doesn't interfere with satisfying the instruction.\n",
    "\n",
    "    Evaluation Criteria:\n",
    "    - Completely Correct: Code has no syntax or semantic errors and satisfies all constraints in the simplified instruction.\n",
    "    - Partially Correct: Code has no syntax or semantic errors, but may miss one or more constraints in the simplified instruction.\n",
    "    - Wrong: if the code is not partially also correct then it is wrong \n",
    "    Output Format:\n",
    "    Return your final evaluation as a dictionary without any explanation:\n",
    "    {{\"reason\": \"<Your reason for the evaluation>\",\n",
    "      \"correctness\": \"Completely Correct/Partially Correct/Wrong\"\n",
    "    }}\n",
    "\n",
    "    Input:\n",
    "    Simplified Instruction:  \n",
    "    {simplified_instruction}\n",
    "\n",
    "    Generated Code:  \n",
    "    ```python\n",
    "    {generated_code}\n",
    "    ```\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_correctness_prompt_v3(instruction,generated_code):\n",
    "    prompt = f\"\"\"You are an expert Python developer and code reviewer.  \n",
    "    Your task is to evaluate whether a given Python code correctly follows the instruction, based only on three dimensions:\n",
    "\n",
    "    1. Syntax Correctness: Is the code free of syntax errors?\n",
    "    2. Semantic Correctness: Does the code make logical sense and run as intended?\n",
    "    3. Constraint Correctness: Does the code satisfy the constraints explicitly in the instruction?\n",
    "\n",
    "    Important Notes:\n",
    "    - Only evaluate based on the instruction. \n",
    "    - Ignore any extra functionality in the response that is not asked for in the instruction.\n",
    "    - Do not penalize for unrelated content, as long as it doesn't interfere with satisfying the instruction.\n",
    "\n",
    "    Evaluation Criteria:\n",
    "    - Completely Correct: Code has no syntax or semantic errors and satisfies all constraints in the instruction.\n",
    "    - Partially Correct: Code has no syntax or semantic errors, but may miss one or more constraints in the instruction.\n",
    "    - Wrong: If the code is not even partially correct, then it is considered wrong.\n",
    "\n",
    "    Output Format:\n",
    "    Return your final evaluation as a dictionary without any explanation:\n",
    "    {{\"reason\": \"<Your reason for the evaluation>\",\n",
    "      \"correctness\": \"Completely Correct/Partially Correct/Wrong\"\n",
    "    }}\n",
    "\n",
    "    Input:\n",
    "    Instruction:  \n",
    "    {instruction}\n",
    "\n",
    "    Generated Code:  \n",
    "    ```python\n",
    "    {generated_code}\n",
    "    ```\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|                                                                                                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.54it/s]\n"
     ]
    }
   ],
   "source": [
    "client = create_clients(mode=\"GPT-azure\")\n",
    "instructions = df[\"simplified_instruction\"].to_list()\n",
    "model_responses = df[\"response\"].to_list()\n",
    "prompts = [code_correctness_prompt_v3(instruction, response) for instruction, response in zip(instructions, model_responses)]\n",
    "correctness_evaluation = client.get_model_response_batch(\n",
    "    user_prompts=prompts,\n",
    "    system_prompt=None,\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['```json\\n{\\n  \"reason\": \"The code is syntactically correct except for an incomplete example usage line at the end, which is outside the function and can be ignored as it is not part of the required function. The function itself is free of syntax errors. Semantically, the code correctly generates a complex wave with the real part as cosine and imaginary part as sine using np.exp(1j * ...), applies a Hann window, and plots both parts with the correct title and axis labels. The constraint about returning both the wave and the plot object is satisfied. The only minor issue is that hann_window is 1D and wave is 1D complex, so multiplying wave * hann_window[:, None] introduces an unnecessary dimension; it should be wave * hann_window. However, this does not cause an error and the code runs as intended. Therefore, the code is Completely Correct.\",\\n  \"correctness\": \"Completely Correct\"\\n}\\n```',\n",
       " '{\"reason\": \"The code is syntactically correct and logically removes all files containing \\'jquery\\' in their names from the specified directory. It returns a tuple with the count and list of removed files as required. It also handles the case where the directory does not exist by raising FileNotFoundError. The logging setup and error handling do not interfere with the required functionality. The example usage snippet is incomplete but does not affect the correctness of the main function. Therefore, the code fully satisfies the instruction.\", \"correctness\": \"Completely Correct\"}',\n",
       " '{\"reason\": \"The code is free of syntax errors and logically defines the required URL patterns for the specified views. It correctly includes the main project urls.py that includes the app urls.py, which defines all required paths: index, movie_detail with slug, and create views for movie, studio, hall, and country. It also correctly appends static URL patterns for serving media files in development using settings.MEDIA_URL and settings.MEDIA_ROOT. All constraints in the instruction are satisfied.\", \"correctness\": \"Completely Correct\"}',\n",
       " '{\"reason\": \"The provided code snippet is incomplete and contains a syntax error (missing colon after \\'if tender.bids == 0\\'). It does not include any unit tests as requested in the instruction. Therefore, it neither fully implements the system nor provides the required tests to verify tender status transitions.\", \"correctness\": \"Wrong\"}',\n",
       " '{\"reason\": \"The code contains a syntax error due to an incomplete string in the categorical_transformer pipeline (\\'constant is not closed). Also, the code is incomplete and does not return the trained LinearRegression model as required. Therefore, it is not even partially correct.\", \"correctness\": \"Wrong\"}',\n",
       " '{\"reason\": \"The code is syntactically correct and logically applies different aggregation functions to different columns in the pivot table as requested. However, the parameter \\'rows\\' used in pd.pivot_table is deprecated and should be replaced with \\'index\\' to satisfy current pandas API constraints.\", \"correctness\": \"Partially Correct\"}',\n",
       " '{\"reason\": \"The code is syntactically correct and uses pandas merge_asof to join df2 with df1 on Timestamp without looping, which is the intended efficient approach. However, the merge_asof call uses direction=\\'forward\\', which matches each df2 timestamp with the next df1 timestamp, but the problem requires matching each df2 timestamp with the most recent df1 timestamp at or before that time (i.e., direction=\\'backward\\'). This causes incorrect matches (e.g., 2019/04/02 11:00:14 in df2 matched to 2019/04/02 11:00:15 in df1 instead of 2019/04/02 11:00:01). Also, the output example is incomplete and the code snippet is truncated. Therefore, semantic correctness is compromised and the constraint of correct timestamp-based joining is not met.\",\"correctness\":\"Partially Correct\"}',\n",
       " '{\"reason\": \"The code has no syntax errors and logically removes vowels from the string. However, it does not return the new string in reverse order as required by the instruction; it returns the string with vowels removed but in the original order. Therefore, it fails the constraint correctness.\", \"correctness\": \"Partially Correct\"}',\n",
       " '{\"reason\": \"The code is syntactically correct and logically computes the area of a triangle using Heron\\'s formula when given valid sides. It raises the specified InvalidTriangleError if the sides cannot form a triangle based on the sum condition. However, it does not check the difference condition explicitly as required by the instruction. Therefore, it partially satisfies the constraints.\", \"correctness\": \"Partially Correct\"}',\n",
       " '{\"reason\": \"The code has no syntax errors and logically attempts to return the third non-whitespace character. However, it does not correctly remove trailing whitespace before counting characters, as it only skips spaces during iteration but does not trim trailing spaces. This means the third character returned may not be the third character after trimming both leading and trailing whitespace. Also, it only skips spaces (\\' \\') and not other whitespace characters, which violates the requirement to remove all leading and trailing whitespace. Therefore, it does not satisfy the constraint of returning the third character after removing leading and trailing whitespace using a single loop iteration.\", \"correctness\": \"Partially Correct\"}']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|                                                                                                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 20 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               10 non-null     int64  \n",
      " 1   dataset                          10 non-null     object \n",
      " 2   instruction                      10 non-null     object \n",
      " 3   code                             10 non-null     object \n",
      " 4   test                             5 non-null      object \n",
      " 5   relevant_categories              10 non-null     object \n",
      " 6   simplified_instruction           10 non-null     object \n",
      " 7   extracted_constraints            10 non-null     object \n",
      " 8   final_comprehensive_constraints  10 non-null     object \n",
      " 9   filtered_relevant_constraints    10 non-null     object \n",
      " 10  quality_scores                   10 non-null     object \n",
      " 11  relevance_score                  10 non-null     float64\n",
      " 12  objectivity_score                10 non-null     float64\n",
      " 13  atomicity_score                  10 non-null     float64\n",
      " 14  unified_quality_score            10 non-null     float64\n",
      " 15  combined_instruction             10 non-null     object \n",
      " 16  response                         10 non-null     object \n",
      " 17  Constraint_adherence             10 non-null     object \n",
      " 18  correctness_reason               10 non-null     object \n",
      " 19  correctness_level                10 non-null     object \n",
      "dtypes: float64(4), int64(1), object(15)\n",
      "memory usage: 1.7+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "client = create_clients(mode=\"GPT-azure\")\n",
    "instructions = df[\"simplified_instruction\"].to_list()\n",
    "model_responses = df[\"response\"].to_list()\n",
    "prompts = [code_correctness_prompt_v2(instruction, response) for instruction, response in zip(instructions, model_responses)]\n",
    "correctness_evaluation_v2 = client.get_model_response_batch(\n",
    "    user_prompts=prompts,\n",
    "    system_prompt=None,\n",
    "    temperature=0.1\n",
    ")\n",
    "correctness_level_2 = [extract_json(response, \"correctness\") for response in correctness_evaluation_v2]\n",
    "correctness_reason_2= [extract_json(response, \"reason\") for response in correctness_evaluation_v2]\n",
    "df[\"correctness_reason\"] = correctness_reason_2\n",
    "df[\"correctness_level\"] = correctness_level_2\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The code has a syntax error due to an incomplete example usage line (missing closing parenthesis). Additionally, the code incorrectly applies the Hann window by using wave * hann_window[:, None], which adds an unnecessary dimension and will cause a broadcasting error since wave is 1D. The correct application should be wave * hann_window. Therefore, the code has both syntax and semantic errors and does not fully satisfy the instruction constraints.',\n",
       " \"The code is syntactically correct and logically removes all files containing 'jquery' in their name from the specified directory. It returns a tuple with the count and list of removed files as required. The additional logging does not interfere with the instruction and is acceptable. The code also properly handles the case where the directory does not exist by raising an exception, which is reasonable and does not violate constraints.\",\n",
       " 'The code is free of syntax errors and logically defines the required URL patterns for the specified views. It correctly includes the main project urls.py that includes the app urls.py, which defines all required paths: index, movie_detail with slug, and create views for movie, studio, hall, and country. It also correctly appends static URL patterns for serving media files in development. All constraints from the simplified instruction are satisfied.',\n",
       " \"The provided code snippet is incomplete and contains a syntax error (missing colon after 'if tender.bids == 0'). It does not include any unit tests as required by the instruction. Therefore, it fails syntax correctness and does not satisfy the instruction constraints.\",\n",
       " \"The code snippet contains a syntax error due to an incomplete string in the SimpleImputer strategy parameter ('constant is not closed). Also, the code is incomplete and does not return the trained LinearRegression model as required. Therefore, it fails syntax correctness and constraint correctness.\",\n",
       " \"The code is syntactically correct and logically applies different aggregation functions to different columns in the pivot table as requested. However, the parameter 'rows' used in pd.pivot_table is deprecated and should be replaced with 'index' to satisfy current pandas API constraints. Despite this, the code will run correctly in older pandas versions and meets the instruction's requirement to get sum for 'D' and mean for 'E'.\",\n",
       " \"The code is syntactically correct and uses pandas merge_asof to join df2 with df1 on Timestamp without looping, which satisfies the requirement. However, the direction='forward' parameter causes the merge to look for the next timestamp in df1 that is greater than or equal to df2's timestamp, which matches the example output. The code also fills NaN with None and reorders columns as requested. The code snippet is incomplete in the output but the logic is correct and matches the simplified instruction and example output.\",\n",
       " \"The code has syntax errors (improper markdown usage and incomplete code block), semantic errors (the function returns the string with vowels removed but in original order, not reversed as required), and the unit tests contain errors (e.g., incorrect expected output with backslash in 'dlrow\\\\olleh'). Therefore, it fails to satisfy the instruction.\",\n",
       " \"The code is syntactically correct and logically computes the area of a triangle using Heron's formula. It correctly raises an InvalidTriangleError if the triangle inequality (sum of any two sides greater than the third) is not met. However, it does not check the second constraint from the instruction: that the difference between the length of any two sides must be smaller than the length of the third side. Therefore, it misses one explicit constraint.\",\n",
       " \"The code has no syntax errors and logically attempts to find the third non-whitespace character by iterating once over the string. However, it does not correctly handle trailing whitespace because it counts all non-space characters including those in the middle and end, rather than trimming leading and trailing whitespace first. The instruction requires removing leading and trailing whitespace before returning the third character, but the code only skips spaces during iteration and does not remove trailing whitespace properly. Also, it only considers spaces (' ') as whitespace, ignoring other whitespace characters. Therefore, it misses the constraint of removing leading and trailing whitespace before indexing.\"]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_reason_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wrong',\n",
       " 'Completely Correct',\n",
       " 'Completely Correct',\n",
       " 'Wrong',\n",
       " 'Wrong',\n",
       " 'Partially Correct',\n",
       " 'Completely Correct',\n",
       " 'Wrong',\n",
       " 'Partially Correct',\n",
       " 'Partially Correct']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_level_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|                                                                                                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 20 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   id                               10 non-null     int64  \n",
      " 1   dataset                          10 non-null     object \n",
      " 2   instruction                      10 non-null     object \n",
      " 3   code                             10 non-null     object \n",
      " 4   test                             5 non-null      object \n",
      " 5   relevant_categories              10 non-null     object \n",
      " 6   simplified_instruction           10 non-null     object \n",
      " 7   extracted_constraints            10 non-null     object \n",
      " 8   final_comprehensive_constraints  10 non-null     object \n",
      " 9   filtered_relevant_constraints    10 non-null     object \n",
      " 10  quality_scores                   10 non-null     object \n",
      " 11  relevance_score                  10 non-null     float64\n",
      " 12  objectivity_score                10 non-null     float64\n",
      " 13  atomicity_score                  10 non-null     float64\n",
      " 14  unified_quality_score            10 non-null     float64\n",
      " 15  combined_instruction             10 non-null     object \n",
      " 16  response                         10 non-null     object \n",
      " 17  Constraint_adherence             10 non-null     object \n",
      " 18  correctness_reason               10 non-null     object \n",
      " 19  correctness_level                10 non-null     object \n",
      "dtypes: float64(4), int64(1), object(15)\n",
      "memory usage: 1.7+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "client = create_clients(mode=\"GPT-azure\")\n",
    "instructions = df[\"simplified_instruction\"].to_list()\n",
    "model_responses = df[\"response\"].to_list()\n",
    "prompts = [code_correctness_prompt_v3(instruction, response) for instruction, response in zip(instructions, model_responses)]\n",
    "correctness_evaluation_v3 = client.get_model_response_batch(\n",
    "    user_prompts=prompts,\n",
    "    system_prompt=None,\n",
    "    temperature=0.1\n",
    ")\n",
    "correctness_level_3 = [extract_json(response, \"correctness\") for response in correctness_evaluation_v3]\n",
    "correctness_reason_3= [extract_json(response, \"reason\") for response in correctness_evaluation_v3]\n",
    "df[\"correctness_reason\"] = correctness_reason_3\n",
    "df[\"correctness_level\"] = correctness_level_3\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Partially Correct',\n",
       " 'Completely Correct',\n",
       " 'Completely Correct',\n",
       " 'Wrong',\n",
       " 'Wrong',\n",
       " 'Partially Correct',\n",
       " 'Partially Correct',\n",
       " 'Partially Correct',\n",
       " 'Partially Correct',\n",
       " 'Partially Correct']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_level_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The code has a syntax error due to an incomplete example usage line (missing closing parenthesis). Additionally, the code incorrectly applies the Hann window by using wave * hann_window[:, None], which adds an unnecessary new axis and will cause a shape mismatch error. The Hann window should be applied as wave * hann_window. The code otherwise logically generates the complex wave, applies the Hann window, plots the real and imaginary parts with correct labels and title, and returns the wave and plot objects as required. Therefore, it is not completely correct due to syntax and semantic errors.',\n",
       " \"The code is syntactically correct and logically removes all files containing 'jquery' in their names from the specified directory, returning the correct tuple as required. It also satisfies the constraint of being self-contained and starting with the specified imports and function definition. The extra logging functionality does not interfere with the requirements.\",\n",
       " 'The code is free of syntax errors and logically defines the required URL patterns for the specified views. It correctly includes the main project URLs and the app URLs, and appends static URL patterns for serving media files in development as required. All constraints from the instruction are satisfied.',\n",
       " \"The provided code snippet is incomplete and contains a syntax error (missing colon after 'if tender.bids == 0'). It does not include any unit tests as requested by the instruction. Therefore, it fails syntax correctness, semantic correctness, and does not satisfy the constraint to write unit tests verifying tender status transitions.\",\n",
       " 'The code snippet is incomplete and contains a syntax error due to an unfinished string in the categorical_transformer pipeline. Therefore, it is not syntax correct and cannot run as intended. It also does not fully satisfy the instruction since the function is incomplete and does not return the trained LinearRegression model as required.',\n",
       " \"The code is syntactically correct and logically applies different aggregation functions (sum for 'D' and mean for 'E') using a dictionary in the aggfunc parameter, which satisfies the instruction. However, the code uses the deprecated 'rows' parameter instead of 'index' in pd.pivot_table, which may cause a warning or error in recent pandas versions. Despite this, the core requirement is met.\",\n",
       " \"The code is syntactically correct and uses pandas merge_asof to join df2 with df1 on Timestamp without looping, which is efficient. However, the direction='forward' parameter causes the merge to match df2 timestamps to the next df1 timestamp, whereas the example output requires matching to the most recent past or equal timestamp (i.e., direction='backward'). Also, the example output shows integer data values or None, but the code fills NaN with None and leaves data as float due to merge_asof behavior. Despite these minor differences, the approach is correct and no syntax or semantic errors exist, but the direction parameter does not satisfy the exact constraint of matching the closest past timestamp as shown in the example output, so it misses a key constraint.\",\n",
       " 'The code has no syntax errors and logically removes vowels from the string. However, it does not reverse the resulting string as required by the instruction. The function returns the string with vowels removed but in the original order, not reversed. Therefore, it fails the constraint correctness dimension.',\n",
       " \"The code is syntactically correct and logically computes the area of a triangle using Heron's formula. It correctly raises an InvalidTriangleError if the sides do not satisfy the triangle inequality conditions (sum of any two sides greater than the third). However, it does not check the second constraint explicitly: that the difference between the length of any two sides must be smaller than the length of the third side. This constraint is necessary to fully validate the triangle. Therefore, the code misses one of the explicit constraints in the instruction.\",\n",
       " \"The code has no syntax errors and logically returns the third non-whitespace character in the string using a single loop iteration. However, it does not correctly handle trailing whitespace characters because it only skips spaces during counting but does not remove trailing whitespace before counting. For example, if the string has trailing spaces, the code may incorrectly count characters beyond the trimmed string. Also, it only skips spaces (' ') and not other whitespace characters (like tabs or newlines), which violates the requirement to remove leading and trailing whitespace characters. Therefore, it does not fully satisfy the constraint of removing leading and trailing whitespace before returning the third character.\"]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_reason_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>instruction</th>\n",
       "      <th>code</th>\n",
       "      <th>test</th>\n",
       "      <th>relevant_categories</th>\n",
       "      <th>simplified_instruction</th>\n",
       "      <th>extracted_constraints</th>\n",
       "      <th>final_comprehensive_constraints</th>\n",
       "      <th>filtered_relevant_constraints</th>\n",
       "      <th>quality_scores</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>objectivity_score</th>\n",
       "      <th>atomicity_score</th>\n",
       "      <th>unified_quality_score</th>\n",
       "      <th>combined_instruction</th>\n",
       "      <th>response</th>\n",
       "      <th>Constraint_adherence</th>\n",
       "      <th>correctness_reason</th>\n",
       "      <th>correctness_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>955</td>\n",
       "      <td>bigcode/bigcodebench</td>\n",
       "      <td>Generates and plots a complex wave with a spec...</td>\n",
       "      <td>wave = amplitude * np.exp(1j * 2 * math.pi * f...</td>\n",
       "      <td>import unittest\\nimport numpy as np\\nimport ma...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Generates and plots a complex wave with a spec...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>[{'type': 'Input and Output Handling', 'constr...</td>\n",
       "      <td>{'constraint_evaluations': [{'constraint_text'...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Generates and plots a complex wave with a spec...</td>\n",
       "      <td>```python\\nimport numpy as np\\nimport math\\nim...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>The code has a syntax error due to a missing c...</td>\n",
       "      <td>Wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>906</td>\n",
       "      <td>bigcode/bigcodebench</td>\n",
       "      <td>Removes all jQuery files (JavaScript files con...</td>\n",
       "      <td># Configure logging\\n    logging.basicConfig(f...</td>\n",
       "      <td>import unittest\\nfrom unittest.mock import Mag...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Removes all jQuery files (JavaScript files con...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>[{'type': 'File and Data Management', 'constra...</td>\n",
       "      <td>{'constraint_evaluations': [{'constraint_text'...</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.73</td>\n",
       "      <td>Removes all jQuery files (JavaScript files con...</td>\n",
       "      <td>```python\\nimport os\\nimport logging\\n\\ndef ta...</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>The code has no syntax or semantic errors, but...</td>\n",
       "      <td>Partially Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1346</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>You are tasked with creating a Django applicat...</td>\n",
       "      <td>```python\\n# Import necessary Django modules\\n...</td>\n",
       "      <td>None</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>You are tasked with creating a Django applicat...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>{'constraint_evaluations': [{'constraint_text'...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.75</td>\n",
       "      <td>You are tasked with creating a Django applicat...</td>\n",
       "      <td>Here's a Python script that defines the URL pa...</td>\n",
       "      <td>[1, 1, 1, 0]</td>\n",
       "      <td>The code is free of syntax errors and logicall...</td>\n",
       "      <td>Partially Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>989</td>\n",
       "      <td>Multilingual-Multimodal-NLP/McEval-Instruct</td>\n",
       "      <td>Design a system to automate the transition of ...</td>\n",
       "      <td>```python\\n# -*- coding: utf-8 -*-\\nimport uni...</td>\n",
       "      <td>None</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Design a system to automate the transition of ...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>[{'type': 'Code Structure and Modularity', 'co...</td>\n",
       "      <td>{'constraint_evaluations': [{'constraint_text'...</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4.23</td>\n",
       "      <td>4.23</td>\n",
       "      <td>4.44</td>\n",
       "      <td>Design a system to automate the transition of ...</td>\n",
       "      <td>**System Design**\\n\\nWe will design a system u...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]</td>\n",
       "      <td>The code has syntax errors (incomplete functio...</td>\n",
       "      <td>Wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>964</td>\n",
       "      <td>bigcode/bigcodebench</td>\n",
       "      <td>Train a linear regression model on a given Dat...</td>\n",
       "      <td>if not isinstance(df, pd.DataFrame):\\n        ...</td>\n",
       "      <td>import unittest\\nimport pandas as pd\\nfrom io ...</td>\n",
       "      <td>['Code Structure and Modularity', 'Input and O...</td>\n",
       "      <td>Train a linear regression model on a given Dat...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>[{'type': 'Error Handling and Robustness', 'co...</td>\n",
       "      <td>{'constraint_evaluations': [{'constraint_text'...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Train a linear regression model on a given Dat...</td>\n",
       "      <td>```python\\nimport pandas as pd\\nfrom sklearn.m...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>The code has a syntax error due to an incomple...</td>\n",
       "      <td>Wrong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                      dataset  \\\n",
       "0   955                         bigcode/bigcodebench   \n",
       "1   906                         bigcode/bigcodebench   \n",
       "2  1346  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "3   989  Multilingual-Multimodal-NLP/McEval-Instruct   \n",
       "4   964                         bigcode/bigcodebench   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  Generates and plots a complex wave with a spec...   \n",
       "1  Removes all jQuery files (JavaScript files con...   \n",
       "2  You are tasked with creating a Django applicat...   \n",
       "3  Design a system to automate the transition of ...   \n",
       "4  Train a linear regression model on a given Dat...   \n",
       "\n",
       "                                                code  \\\n",
       "0  wave = amplitude * np.exp(1j * 2 * math.pi * f...   \n",
       "1  # Configure logging\\n    logging.basicConfig(f...   \n",
       "2  ```python\\n# Import necessary Django modules\\n...   \n",
       "3  ```python\\n# -*- coding: utf-8 -*-\\nimport uni...   \n",
       "4  if not isinstance(df, pd.DataFrame):\\n        ...   \n",
       "\n",
       "                                                test  \\\n",
       "0  import unittest\\nimport numpy as np\\nimport ma...   \n",
       "1  import unittest\\nfrom unittest.mock import Mag...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4  import unittest\\nimport pandas as pd\\nfrom io ...   \n",
       "\n",
       "                                 relevant_categories  \\\n",
       "0  ['Code Structure and Modularity', 'Input and O...   \n",
       "1  ['Code Structure and Modularity', 'Input and O...   \n",
       "2  ['Code Structure and Modularity', 'Input and O...   \n",
       "3  ['Code Structure and Modularity', 'Input and O...   \n",
       "4  ['Code Structure and Modularity', 'Input and O...   \n",
       "\n",
       "                              simplified_instruction  \\\n",
       "0  Generates and plots a complex wave with a spec...   \n",
       "1  Removes all jQuery files (JavaScript files con...   \n",
       "2  You are tasked with creating a Django applicat...   \n",
       "3  Design a system to automate the transition of ...   \n",
       "4  Train a linear regression model on a given Dat...   \n",
       "\n",
       "                               extracted_constraints  \\\n",
       "0  [{'type': 'Input and Output Handling', 'constr...   \n",
       "1  [{'type': 'File and Data Management', 'constra...   \n",
       "2  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "3  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "4  [{'type': 'Error Handling and Robustness', 'co...   \n",
       "\n",
       "                     final_comprehensive_constraints  \\\n",
       "0  [{'type': 'Input and Output Handling', 'constr...   \n",
       "1  [{'type': 'File and Data Management', 'constra...   \n",
       "2  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "3  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "4  [{'type': 'Error Handling and Robustness', 'co...   \n",
       "\n",
       "                       filtered_relevant_constraints  \\\n",
       "0  [{'type': 'Input and Output Handling', 'constr...   \n",
       "1  [{'type': 'File and Data Management', 'constra...   \n",
       "2  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "3  [{'type': 'Code Structure and Modularity', 'co...   \n",
       "4  [{'type': 'Error Handling and Robustness', 'co...   \n",
       "\n",
       "                                      quality_scores  relevance_score  \\\n",
       "0  {'constraint_evaluations': [{'constraint_text'...             5.00   \n",
       "1  {'constraint_evaluations': [{'constraint_text'...             4.80   \n",
       "2  {'constraint_evaluations': [{'constraint_text'...             5.00   \n",
       "3  {'constraint_evaluations': [{'constraint_text'...             4.85   \n",
       "4  {'constraint_evaluations': [{'constraint_text'...             5.00   \n",
       "\n",
       "   objectivity_score  atomicity_score  unified_quality_score  \\\n",
       "0               5.00             5.00                   5.00   \n",
       "1               4.60             4.80                   4.73   \n",
       "2               4.50             4.75                   4.75   \n",
       "3               4.23             4.23                   4.44   \n",
       "4               5.00             5.00                   5.00   \n",
       "\n",
       "                                combined_instruction  \\\n",
       "0  Generates and plots a complex wave with a spec...   \n",
       "1  Removes all jQuery files (JavaScript files con...   \n",
       "2  You are tasked with creating a Django applicat...   \n",
       "3  Design a system to automate the transition of ...   \n",
       "4  Train a linear regression model on a given Dat...   \n",
       "\n",
       "                                            response  \\\n",
       "0  ```python\\nimport numpy as np\\nimport math\\nim...   \n",
       "1  ```python\\nimport os\\nimport logging\\n\\ndef ta...   \n",
       "2  Here's a Python script that defines the URL pa...   \n",
       "3  **System Design**\\n\\nWe will design a system u...   \n",
       "4  ```python\\nimport pandas as pd\\nfrom sklearn.m...   \n",
       "\n",
       "                         Constraint_adherence  \\\n",
       "0                    [1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1                             [1, 1, 1, 1, 1]   \n",
       "2                                [1, 1, 1, 0]   \n",
       "3  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]   \n",
       "4                                      [1, 0]   \n",
       "\n",
       "                                  correctness_reason  correctness_level  \n",
       "0  The code has a syntax error due to a missing c...              Wrong  \n",
       "1  The code has no syntax or semantic errors, but...  Partially Correct  \n",
       "2  The code is free of syntax errors and logicall...  Partially Correct  \n",
       "3  The code has syntax errors (incomplete functio...              Wrong  \n",
       "4  The code has a syntax error due to an incomple...              Wrong  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Generates and plots a complex wave with a specified amplitude and frequency over given time points, applying a Hann window to reduce edge effects. The wave is represented as a complex number where the real part is the cosine component, and the imaginary part is the sine component. It returns both the wave and the plot object.\n",
      "Note that: Notes: The plot title is \"Complex Wave with Hann Window\". The x-label of the plot is \"Time\". The y-label of the plot is \"Amplitude\". The plot displays both the real and imaginary parts of the complex wave.\n",
      "The function should output with:\n",
      "    numpy.ndarray: The generated complex wave as a numpy array of complex numbers.\n",
      "    matplotlib.figure.Figure: The figure object of the plot.\n",
      "    matplotlib.axes.Axes: The axes object of the plot.\n",
      "You should write self-contained code starting with:\n",
      "```\n",
      "import numpy as np\n",
      "import math\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import get_window\n",
      "def task_func(amplitude, frequency, time):\n",
      "```\n",
      "Response: ```python\n",
      "import numpy as np\n",
      "import math\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import get_window\n",
      "\n",
      "def task_func(amplitude, frequency, time):\n",
      "    \"\"\"\n",
      "    Generates and plots a complex wave with a specified amplitude and frequency over given time points,\n",
      "    applying a Hann window to reduce edge effects.\n",
      "\n",
      "    Args:\n",
      "        amplitude (float): The amplitude of the wave.\n",
      "        frequency (float): The frequency of the wave.\n",
      "        time (numpy.ndarray): The time points at which the wave is generated.\n",
      "\n",
      "    Returns:\n",
      "        numpy.ndarray: The generated complex wave as a numpy array of complex numbers.\n",
      "        matplotlib.figure.Figure: The figure object of the plot.\n",
      "        matplotlib.axes.Axes: The axes object of the plot.\n",
      "    \"\"\"\n",
      "\n",
      "    # Generate the complex wave using the formula wave = amplitude * np.exp(1j * 2 * math.pi * frequency * time)\n",
      "    wave = amplitude * np.exp(1j * 2 * math.pi * frequency * time)\n",
      "\n",
      "    # Apply a Hann window to the wave to reduce edge effects\n",
      "    hann_window = get_window('hann', len(time))\n",
      "    wave_windowed = wave * hann_window[:, None]\n",
      "\n",
      "    # Create a new figure and axis object\n",
      "    fig, ax = plt.subplots()\n",
      "\n",
      "    # Plot the real and imaginary parts of the complex wave\n",
      "    ax.plot(time, np.real(wave_windowed), label='Real part')\n",
      "    ax.plot(time, np.imag(wave_windowed), label='Imaginary part')\n",
      "\n",
      "    # Set the plot title, x-label, and y-label\n",
      "    ax.set_title('Complex Wave with Hann Window')\n",
      "    ax.set_xlabel('Time')\n",
      "    ax.set_ylabel('Amplitude')\n",
      "\n",
      "    # Add a legend to the plot\n",
      "    ax.legend()\n",
      "\n",
      "    # Return the generated complex wave and the plot object\n",
      "    return wave_windowed, fig, ax\n",
      "\n",
      "# Example usage:\n",
      "time = np.linspace(0, 1, 1000\n",
      "--------------------------------------------------------------------------------\n",
      "Correctness Level: Completely Correct\n",
      "Correctness Reason: The code is syntactically correct except for an incomplete example usage line at the end, which is outside the function and does not affect the function itself. The function correctly generates a complex wave with the specified amplitude and frequency, applies a Hann window, plots both real and imaginary parts with the correct title and labels, and returns the wave and plot objects as required. The only minor issue is the unnecessary use of [:, None] when multiplying the wave by the 1D Hann window, which is not needed since both are 1D arrays and this does not cause an error. Overall, the function satisfies all constraints in the simplified instruction.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Removes all jQuery files (JavaScript files containing 'jquery' in their name) from a specified directory.\n",
      "Note that: Removed files are logged in 'jquery_removal.log' file.\n",
      "The function should raise the exception for: If the specified directory does not exist the code would raise FileNotFoundError.\n",
      "The function should output with:\n",
      "    tuple: A tuple containing two elements:\n",
      "    int: The number of files removed.\n",
      "    list: The names of the removed files.\n",
      "You should write self-contained code starting with:\n",
      "```\n",
      "import os\n",
      "import logging\n",
      "def task_func(directory):\n",
      "```\n",
      "Response: ```python\n",
      "import os\n",
      "import logging\n",
      "\n",
      "def task_func(directory):\n",
      "    \"\"\"\n",
      "    Removes all jQuery files from a specified directory and logs the removed files.\n",
      "\n",
      "    Args:\n",
      "        directory (str): The path to the directory containing jQuery files.\n",
      "\n",
      "    Returns:\n",
      "        tuple: A tuple containing the number of files removed and a list of the names of the removed files.\n",
      "\n",
      "    Raises:\n",
      "        FileNotFoundError: If the specified directory does not exist.\n",
      "    \"\"\"\n",
      "\n",
      "    # Set up logging to log the removed files\n",
      "    logging.basicConfig(filename='jquery_removal.log', level=logging.INFO, format='%(message)s')\n",
      "\n",
      "    # Check if the directory exists\n",
      "    if not os.path.exists(directory):\n",
      "        raise FileNotFoundError(f\"The directory '{directory}' does not exist.\")\n",
      "\n",
      "    # Initialize variables to keep track of the number of removed files and their names\n",
      "    removed_count = 0\n",
      "    removed_files = []\n",
      "\n",
      "    # Iterate over all files in the directory\n",
      "    for filename in os.listdir(directory):\n",
      "        # Construct the full path to the file\n",
      "        filepath = os.path.join(directory, filename)\n",
      "\n",
      "        # Check if the file is a jQuery file (contains 'jquery' in its name)\n",
      "        if 'jquery' in filename.lower():\n",
      "            try:\n",
      "                # Attempt to remove the file\n",
      "                os.remove(filepath)\n",
      "                # Log the removed file\n",
      "                logging.info(f\"Removed file: {filename}\")\n",
      "                # Increment the removed count and add the file name to the list\n",
      "                removed_count += 1\n",
      "                removed_files.append(filename)\n",
      "            except Exception as e:\n",
      "                # Log any errors that occur during file removal\n",
      "                logging.error(f\"Error removing file '{filename}': {str(e)}\")\n",
      "\n",
      "    # Return the number of removed files and their names\n",
      "    return removed_count, removed_files\n",
      "```\n",
      "\n",
      "Example usage:\n",
      "\n",
      "```python\n",
      "try:\n",
      "    removed_count, removed_files = task_func('/path/to/directory')\n",
      "    print(f\"Removed {removed_count} files: {removed_files\n",
      "--------------------------------------------------------------------------------\n",
      "Correctness Level: Completely Correct\n",
      "Correctness Reason: The code is syntactically correct and logically removes all files containing 'jquery' in their name from the specified directory. It returns a tuple with the count and list of removed files as required. The additional logging does not interfere with the instruction and is allowed. Therefore, it satisfies all constraints in the simplified instruction.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: You are tasked with creating a Django application for managing a movie database. The application should allow users to view a list of movies, access detailed information about a specific movie, and create new entries for movies, studios, halls, and countries. The application should also handle media files correctly.\n",
      "\n",
      "Write a Python script that defines the URL patterns for the Django application. The script should include URL patterns for the following views:\n",
      "- `index`: A view that displays the list of movies.\n",
      "- `movie_detail`: A view that displays detailed information about a specific movie, accessed via a unique slug.\n",
      "- `movie_create`: A view that provides a form for creating a new movie entry.\n",
      "- `studio_create`: A view that provides a form for creating a new studio entry.\n",
      "- `hall_create`: A view that provides a form for creating a new hall entry.\n",
      "- `country_create`: A view that provides a form for creating a new country entry.\n",
      "\n",
      "Additionally, ensure that the application serves media files in development by appending the necessary static URL patterns.\n",
      "Response: Here's a Python script that defines the URL patterns for the Django application:\n",
      "\n",
      "```python\n",
      "# myproject/urls.py\n",
      "\n",
      "from django.contrib import admin\n",
      "from django.urls import path, include\n",
      "from django.conf import settings\n",
      "from django.conf.urls.static import static\n",
      "\n",
      "urlpatterns = [\n",
      "    path('admin/', admin.site.urls),\n",
      "    path('', include('movie_app.urls')),\n",
      "]\n",
      "\n",
      "if settings.DEBUG:\n",
      "    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\n",
      "```\n",
      "\n",
      "```python\n",
      "# movie_app/urls.py\n",
      "\n",
      "from django.urls import path\n",
      "from . import views\n",
      "\n",
      "urlpatterns = [\n",
      "    path('', views.index, name='index'),\n",
      "    path('movie/<slug:slug>/', views.movie_detail, name='movie_detail'),\n",
      "    path('movie/create/', views.movie_create, name='movie_create'),\n",
      "    path('studio/create/', views.studio_create, name='studio_create'),\n",
      "    path('hall/create/', views.hall_create, name='hall_create'),\n",
      "    path('country/create/', views.country_create, name='country_create'),\n",
      "]\n",
      "```\n",
      "\n",
      "In the above code:\n",
      "\n",
      "- `myproject/urls.py` includes the main URL configuration for the project, which includes the `movie_app` URL configuration.\n",
      "- `movie_app/urls.py` defines the URL patterns for the `movie_app` application. It includes URL patterns for the views: `index`, `movie_detail`, `movie_create`, `studio_create`, `hall_create`, and `country_create`.\n",
      "\n",
      "To serve media files in development, we append the necessary static URL patterns to the `urlpatterns` list in `myproject/urls.py`. This is done using the `static` function from `django.conf.urls.static`, which takes the `MEDIA_URL` and `MEDIA_ROOT` settings as arguments.\n",
      "\n",
      "**Settings**\n",
      "\n",
      "In your `settings.py` file, you should have the following settings:\n",
      "\n",
      "```python\n",
      "# settings.py\n",
      "\n",
      "MEDIA_ROOT = os.path.join(BASE_DIR, 'media')\n",
      "MEDIA_URL = '/media/'\n",
      "``\n",
      "--------------------------------------------------------------------------------\n",
      "Correctness Level: Completely Correct\n",
      "Correctness Reason: The code is free of syntax errors and logically defines the URL patterns as required. It includes URL patterns for all specified views (index, movie_detail with slug, movie_create, studio_create, hall_create, country_create) in the app's urls.py. The project's urls.py includes the app's urls and correctly appends static URL patterns for serving media files in development. All constraints from the simplified instruction are met.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Design a system to automate the transition of tender statuses in an e-procurement platform. The system should handle different types of tenders, such as below-threshold, open UA, and open UA defense tenders. Each tender type has its own set of rules for transitioning between statuses based on the number of bids, complaints, and auction results.\n",
      "\n",
      "The system should be able to:\n",
      "- Transition a tender to 'unsuccessful' if there are no bids.\n",
      "- Set the auction period for tenders without bids.\n",
      "- Transition a tender to 'qualification' if there is at least one bid.\n",
      "- Handle complaints and transition tenders accordingly.\n",
      "- Transition a tender to 'auction' when it's ready for auction.\n",
      "- Handle tenders with lots, including setting auction periods and transitioning lot statuses.\n",
      "\n",
      "Write a set of unit tests to verify that the system correctly transitions tenders and lots between statuses under various conditions. Use the provided code snippet as a starting point for creating the test cases.\n",
      "Response: **System Design**\n",
      "\n",
      "We will design a system using Python classes to handle the transition of tender statuses. We will use a dictionary to store the rules for each tender type.\n",
      "\n",
      "```python\n",
      "import logging\n",
      "from dataclasses import dataclass\n",
      "from enum import Enum\n",
      "from typing import Dict, List\n",
      "\n",
      "# Define the tender status enum\n",
      "class TenderStatus(Enum):\n",
      "    PENDING = 1\n",
      "    QUALIFICATION = 2\n",
      "    AUCTION = 3\n",
      "    UNSUCCESSFUL = 4\n",
      "\n",
      "# Define the tender type enum\n",
      "class TenderType(Enum):\n",
      "    BELOW_THRESHOLD = 1\n",
      "    OPEN_UA = 2\n",
      "    OPEN_UA_DEFENSE = 3\n",
      "\n",
      "# Define the tender data class\n",
      "@dataclass\n",
      "class Tender:\n",
      "    id: int\n",
      "    type: TenderType\n",
      "    bids: int\n",
      "    complaints: int\n",
      "    auction_period: int\n",
      "    lots: List['Lot']\n",
      "\n",
      "# Define the lot data class\n",
      "@dataclass\n",
      "class Lot:\n",
      "    id: int\n",
      "    auction_period: int\n",
      "\n",
      "# Define the tender manager class\n",
      "class TenderManager:\n",
      "    def __init__(self):\n",
      "        self.rules = {\n",
      "            TenderType.BELOW_THRESHOLD: self._below_threshold_rules,\n",
      "            TenderType.OPEN_UA: self._open_ua_rules,\n",
      "            TenderType.OPEN_UA_DEFENSE: self._open_ua_defense_rules,\n",
      "        }\n",
      "        self.logger = logging.getLogger(__name__)\n",
      "\n",
      "    def transition_status(self, tender: Tender) -> bool:\n",
      "        if not tender:\n",
      "            self.logger.error(\"Invalid tender\")\n",
      "            return False\n",
      "\n",
      "        if tender.type not in self.rules:\n",
      "            self.logger.error(\"Invalid tender type\")\n",
      "            return False\n",
      "\n",
      "        rules = self.rules[tender.type]\n",
      "        if not rules(tender):\n",
      "            self.logger.error(\"Failed to transition tender status\")\n",
      "            return False\n",
      "\n",
      "        return True\n",
      "\n",
      "    def _below_threshold_rules(self, tender: Tender) -> bool:\n",
      "        if tender.bids == 0\n",
      "--------------------------------------------------------------------------------\n",
      "Correctness Level: Wrong\n",
      "Correctness Reason: The provided code snippet is incomplete and contains a syntax error (missing colon after 'if tender.bids == 0'). It does not include any unit tests as required by the instruction. Therefore, it fails syntax correctness, semantic correctness, and does not satisfy the constraint to write unit tests verifying tender status transitions.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Train a linear regression model on a given DataFrame.\n",
      "The function should raise the exception for: The function will raise a ValueError is input df is not a DataFrame.\n",
      "The function should output with:\n",
      "    LinearRegression: The trained linear regression model.\n",
      "You should write self-contained code starting with:\n",
      "```\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# Constants\n",
      "FEATURES = ['feature '+str(i) for i in range(1, 11)]\n",
      "TARGET = 'target'\n",
      "def task_func(df):\n",
      "```\n",
      "Response: ```python\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "import numpy as np\n",
      "\n",
      "# Constants\n",
      "FEATURES = ['feature '+str(i) for i in range(1, 11)]\n",
      "TARGET = 'target'\n",
      "\n",
      "def task_func(df):\n",
      "    \"\"\"\n",
      "    Train a linear regression model on a given DataFrame.\n",
      "\n",
      "    Args:\n",
      "        df (pd.DataFrame): Input DataFrame.\n",
      "\n",
      "    Returns:\n",
      "        LinearRegression: The trained linear regression model.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the input df is not a DataFrame.\n",
      "    \"\"\"\n",
      "    # Check if input is a DataFrame\n",
      "    if not isinstance(df, pd.DataFrame):\n",
      "        raise ValueError(\"Input must be a pandas DataFrame\")\n",
      "\n",
      "    # Check if DataFrame is empty\n",
      "    if df.empty:\n",
      "        raise ValueError(\"Input DataFrame is empty\")\n",
      "\n",
      "    # Check if required columns exist in the DataFrame\n",
      "    if TARGET not in df.columns or not all(feature in df.columns for feature in FEATURES):\n",
      "        raise ValueError(\"Required columns are missing from the DataFrame\")\n",
      "\n",
      "    # Split data into features and target\n",
      "    X = df[FEATURES]\n",
      "    y = df[TARGET]\n",
      "\n",
      "    # Split data into training and testing sets\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "    # Define preprocessing pipeline\n",
      "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
      "    categorical_features = X.select_dtypes(include=['object']).columns\n",
      "\n",
      "    numeric_transformer = Pipeline(steps=[\n",
      "        ('imputer', SimpleImputer(strategy='median')),\n",
      "        ('scaler', StandardScaler())])\n",
      "\n",
      "    categorical_transformer = Pipeline(steps=[\n",
      "        ('imputer', SimpleImputer(strategy='constant\n",
      "--------------------------------------------------------------------------------\n",
      "Correctness Level: Wrong\n",
      "Correctness Reason: The code snippet contains a syntax error due to an incomplete string in the categorical_transformer pipeline definition, causing a syntax error. Therefore, it is Wrong.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Problem:\n",
      "Was trying to generate a pivot table with multiple \"values\" columns. I know I can use aggfunc to aggregate values the way I want to, but what if I don't want to sum or avg both columns but instead I want sum of one column while mean of the other one. So is it possible to do so using pandas?\n",
      "\n",
      "\n",
      "df = pd.DataFrame({\n",
      "'A' : ['abc', 'def', 'xyz', 'abc'] * 3,\n",
      "'B' : ['A', 'B', 'C'] * 4,\n",
      "'D' : np.random.arange(12),\n",
      "'E' : np.random.arange(12)\n",
      "})\n",
      "Now this will get a pivot table with sum:\n",
      "\n",
      "\n",
      "pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\n",
      "And this for mean:\n",
      "\n",
      "\n",
      "pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\n",
      "How can I get sum for D and mean for E?\n",
      "\n",
      "\n",
      "Hope my question is clear enough.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A:\n",
      "<code>\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "np.random.seed(1)\n",
      "df = pd.DataFrame({\n",
      "'A' : ['abc', 'def', 'xyz', 'abc'] * 3,\n",
      "'B' : ['A', 'B', 'C'] * 4,\n",
      "'D' : np.random.randn(12),\n",
      "'E' : np.random.randn(12)\n",
      "})\n",
      "</code>\n",
      "result = ... # put solution in this variable\n",
      "BEGIN SOLUTION\n",
      "<code>\n",
      "Response: You can achieve this by passing a dictionary to the `aggfunc` parameter of the `pivot_table` function. This dictionary will map the column names to their respective aggregation functions.\n",
      "\n",
      "Here's the solution:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "np.random.seed(1)\n",
      "df = pd.DataFrame({\n",
      "    'A' : ['abc', 'def', 'xyz', 'abc'] * 3,\n",
      "    'B' : ['A', 'B', 'C'] * 4,\n",
      "    'D' : np.random.randn(12),\n",
      "    'E' : np.random.randn(12)\n",
      "})\n",
      "\n",
      "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n",
      "\n",
      "print(result)\n",
      "```\n",
      "\n",
      "In this code:\n",
      "\n",
      "- We create a dictionary `{'D': np.sum, 'E': np.mean}` that maps the column names 'D' and 'E' to their respective aggregation functions `np.sum` and `np.mean`.\n",
      "- We pass this dictionary to the `aggfunc` parameter of the `pivot_table` function.\n",
      "- The `pivot_table` function will apply the specified aggregation functions to the respective columns in the pivot table.\n",
      "\n",
      "This will produce a pivot table where the values in column 'D' are summed and the values in column 'E' are averaged.\n",
      "--------------------------------------------------------------------------------\n",
      "Correctness Level: Completely Correct\n",
      "Correctness Reason: The code is syntactically correct and logically applies different aggregation functions to different columns using a dictionary for aggfunc, which satisfies the instruction. However, the parameter 'rows' is deprecated in pandas.pivot_table and should be replaced with 'index'. Despite this, the code will still run with a warning, so it is semantically correct. The constraint to get sum for 'D' and mean for 'E' is met.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Problem:\n",
      "Say I have two dataframes:\n",
      "df1:                          df2:\n",
      "+-------------------+----+    +-------------------+-----+\n",
      "|  Timestamp        |data|    |  Timestamp        |stuff|\n",
      "+-------------------+----+    +-------------------+-----+\n",
      "|2019/04/02 11:00:01| 111|    |2019/04/02 11:00:14|  101|\n",
      "|2019/04/02 11:00:15| 222|    |2019/04/02 11:00:15|  202|\n",
      "|2019/04/02 11:00:29| 333|    |2019/04/02 11:00:16|  303|\n",
      "|2019/04/02 11:00:30| 444|    |2019/04/02 11:00:30|  404|\n",
      "+-------------------+----+    |2019/04/02 11:00:31|  505|\n",
      "                              +-------------------+-----+\n",
      "\n",
      "\n",
      "Without looping through every row of df2, I am trying to join the two dataframes based on the timestamp. So for every row in df2, it will \"add\" data from df1 that was at that particular time. In this example, the resulting dataframe would be:\n",
      "Adding df1 data to df2:\n",
      "+-------------------+-----+----+\n",
      "|  Timestamp        |stuff|data|\n",
      "+-------------------+-----+----+\n",
      "|2019/04/02 11:00:14|  101| 222|\n",
      "|2019/04/02 11:00:15|  202| 222|\n",
      "|2019/04/02 11:00:16|  303| 333|\n",
      "|2019/04/02 11:00:30|  404| 444|\n",
      "|2019/04/02 11:00:31|  505|None|\n",
      "+-------------------+-----+----+\n",
      "\n",
      "\n",
      "Looping through each row of df2 then comparing to each df1 is very inefficient. Is there another way?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A:\n",
      "<code>\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "df1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n",
      "                    'data': [111, 222, 333, 444]})\n",
      "df2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n",
      "                    'stuff': [101, 202, 303, 404, 505]})\n",
      "df1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\n",
      "df2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\n",
      "</code>\n",
      "result = ... # put solution in this variable\n",
      "BEGIN SOLUTION\n",
      "<code>\n",
      "Response: To efficiently combine the dataframes based on timestamps without looping through every row of df2, you can utilize the pandas `merge_asof` function. Here's the solution:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Create dataframes\n",
      "df1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n",
      "                    'data': [111, 222, 333, 444]})\n",
      "df2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n",
      "                    'stuff': [101, 202, 303, 404, 505]})\n",
      "\n",
      "# Convert Timestamp to datetime\n",
      "df1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\n",
      "df2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\n",
      "\n",
      "# Merge df1 and df2 using merge_asof\n",
      "result = pd.merge_asof(df2, df1, on='Timestamp', direction='forward')\n",
      "\n",
      "# Fill NaN values in 'data' column with None\n",
      "result['data'] = result['data'].fillna(None)\n",
      "\n",
      "# Reorder columns to match the original order of df2\n",
      "result = result[['Timestamp', 'stuff', 'data']]\n",
      "\n",
      "print(result)\n",
      "```\n",
      "\n",
      "This solution will output:\n",
      "\n",
      "```\n",
      "                  Timestamp  stuff    data\n",
      "0 2019-04-02 11:00:14      101   222.0\n",
      "1 2019-04-02 11:00:15\n",
      "--------------------------------------------------------------------------------\n",
      "Correctness Level: Completely Correct\n",
      "Correctness Reason: The code is syntactically correct and uses pandas merge_asof with direction='forward' to join df2 with the next matching timestamp in df1, which matches the example output. It fills NaNs with None and reorders columns as requested. Thus, it satisfies all constraints without looping through rows.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Write a recursive function that takes a string as an argument and removes all lowercase and uppercase vowels (a, e, i, o, u, A, E, I, O, U) from it. The function should handle special characters and symbols in the input string and return the new string in reverse order.\n",
      "Response: **Recursive Function to Remove Vowels and Reverse String**\n",
      "\n",
      "```python\n",
      "def remove_vowels_and_reverse(s):\n",
      "    \"\"\"\n",
      "    Removes all lowercase and uppercase vowels from the input string and returns the new string in reverse order.\n",
      "\n",
      "    Args:\n",
      "        s (str): The input string.\n",
      "\n",
      "    Returns:\n",
      "        str: The new string with vowels removed and in reverse order.\n",
      "    \"\"\"\n",
      "    # Base case: If the string is empty, return an empty string.\n",
      "    if not s:\n",
      "        return \"\"\n",
      "\n",
      "    # Check if the first character is a vowel.\n",
      "    if s[0].lower() in \"aeiou\":\n",
      "        # If it's a vowel, recursively call the function on the rest of the string.\n",
      "        return remove_vowels_and_reverse(s[1:])\n",
      "    else:\n",
      "        # If it's not a vowel, recursively call the function on the rest of the string and prepend the first character.\n",
      "        return s[0] + remove_vowels_and_reverse(s[1:])\n",
      "```\n",
      "\n",
      "**Unit Tests**\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "\n",
      "class TestRemoveVowelsAndReverse(unittest.TestCase):\n",
      "    def test_no_vowels(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"hello\"), \"olleh\")\n",
      "\n",
      "    def test_only_vowels(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"aeiou\"), \"\")\n",
      "\n",
      "    def test_special_characters(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"hello!@#$\"), \"!@#$olleh\")\n",
      "\n",
      "    def test_multi_line_string(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"hello\\nworld\"), \"dlrow\\olleh\")\n",
      "\n",
      "    def test_empty_string(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"\"), \"\")\n",
      "\n",
      "    def test_single_character(self):\n",
      "        self.assertEqual(remove_vowels_and_reverse(\"a\"), \"\")\n",
      "\n",
      "    def test_long_string(self):\n",
      "        long_string = \"a\" * 10000\n",
      "        self.assertEqual(remove_vowels_and_reverse(long_string), \"\")\n",
      "\n",
      "if\n",
      "--------------------------------------------------------------------------------\n",
      "Correctness Level: Wrong\n",
      "Correctness Reason: The code has syntax errors (unclosed code blocks, incomplete 'if' statement at the end) and semantic errors (the function returns the string with vowels removed but in the original order, not reversed as required). Therefore, it does not satisfy the instruction to return the new string in reverse order.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Create a function to compute the area of a triangle when the length of its three sides are known. The sides of the triangle are represented by the variables a, b, and c.\n",
      "\n",
      "The function should first check if the given sides can form a valid triangle. If the sides cannot form a valid triangle, the function should raise a custom exception called `InvalidTriangleError` with the message \"Invalid triangle: the given sides cannot form a triangle.\"\n",
      "\n",
      "To check if the sides can form a valid triangle, you should use the following conditions:\n",
      "- The sum of the lengths of any two sides of a triangle must be greater than the length of the third side.\n",
      "- The difference between the length of any two sides of a triangle must be smaller than the length of the third side.\n",
      "- Additionally, the sides of the triangle must be integers.\n",
      "\n",
      "If the given sides can form a valid triangle, your function should compute and return the area of the triangle using the same Heron's formula provided in the previous test.\n",
      "\n",
      "To summarize:\n",
      "- Write a function called `compute_triangle_area` that takes in three parameters: a, b, and c.\n",
      "- Check if the given sides can form a valid triangle using the conditions stated above. If not, raise an `InvalidTriangleError` with the message \"Invalid triangle: the given sides cannot form a triangle.\"\n",
      "- If the sides can form a valid triangle, compute and return the area of the triangle using Heron's formula.\n",
      "\n",
      "Example Usage:\n",
      "```python\n",
      "try:\n",
      "    area = compute_triangle_area(6, 8, 10)\n",
      "    print(area)  # Output: 24\n",
      "except InvalidTriangleError as e:\n",
      "    print(e)  # Output: Invalid triangle: the given sides cannot form a triangle.\n",
      "```\n",
      "\n",
      "Hints:\n",
      "- You can use the math.sqrt() function from the math module to compute the square root.\n",
      "- Make sure to handle the case where the sides cannot form a valid triangle by raising the custom exception.\n",
      "- You may assume that the given side lengths are positive integers.\n",
      "- Remember to check if the sides are integers in addition to the existing conditions.\n",
      "Response: Here's a Python solution that meets the requirements:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "class InvalidTriangleError(Exception):\n",
      "    \"\"\"Custom exception for invalid triangle.\"\"\"\n",
      "    pass\n",
      "\n",
      "def compute_triangle_area(a, b, c):\n",
      "    \"\"\"\n",
      "    Compute the area of a triangle given its three sides.\n",
      "\n",
      "    Args:\n",
      "    a (int): The length of the first side.\n",
      "    b (int): The length of the second side.\n",
      "    c (int): The length of the third side.\n",
      "\n",
      "    Returns:\n",
      "    float: The area of the triangle.\n",
      "\n",
      "    Raises:\n",
      "    InvalidTriangleError: If the given sides cannot form a valid triangle.\n",
      "    \"\"\"\n",
      "    # Check if all input parameters are positive integers\n",
      "    if not (isinstance(a, int) and isinstance(b, int) and isinstance(c, int)):\n",
      "        raise TypeError(\"All sides must be integers.\")\n",
      "    if a <= 0 or b <= 0 or c <= 0:\n",
      "        raise ValueError(\"All sides must be positive.\")\n",
      "\n",
      "    # Check if the sides can form a valid triangle\n",
      "    if a + b <= c or a + c <= b or b + c <= a:\n",
      "        raise InvalidTriangleError(\"Invalid triangle: the given sides cannot form a triangle.\")\n",
      "\n",
      "    # Compute the semi-perimeter\n",
      "    s = (a + b + c) / 2\n",
      "\n",
      "    # Compute the area using Heron's formula\n",
      "    area = math.sqrt(s * (s - a) * (s - b) * (s - c))\n",
      "\n",
      "    return area\n",
      "\n",
      "# Example usage:\n",
      "try:\n",
      "    area = compute_triangle_area(6, 8, 10)\n",
      "    print(area)  # Output: 24.0\n",
      "except InvalidTriangleError as e:\n",
      "    print(e)  # Output: Invalid triangle: the given sides cannot form a triangle.\n",
      "```\n",
      "\n",
      "This solution defines a custom exception `InvalidTriangleError` to handle cases where the given sides cannot form a valid triangle. It then checks if the input\n",
      "--------------------------------------------------------------------------------\n",
      "Correctness Level: Partially Correct\n",
      "Correctness Reason: The code is syntactically correct and logically computes the area of a triangle using Heron's formula. It correctly raises an InvalidTriangleError if the sides do not satisfy the triangle inequality conditions (sum of any two sides greater than the third). However, it misses the explicit check for the difference between any two sides being smaller than the third side, which is part of the simplified instruction constraints. The additional type and positivity checks are not required but do not interfere with correctness.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Instruction: Given a string ``word``, write code to return the third character in the string using a single loop iteration and without using any built-in functions or libraries. Additionally, the code should handle cases where the input string has leading or trailing whitespace characters and should return the third character after removing any leading or trailing whitespace characters.\n",
      "\n",
      "Example:\n",
      "Input: word = \"   hello   \"\n",
      "Output: \"l\"\n",
      "\n",
      "Note:\n",
      "- The input string will always have at least three characters.\n",
      "- The characters in the string can be uppercase or lowercase alphabets.\n",
      "- The input string may contain special characters or whitespace characters as well.\n",
      "- The code should handle cases where the input string has leading or trailing whitespace characters and should return the third character after removing any leading or trailing whitespace characters.\n",
      "Response: Here's a Python function that uses a single loop iteration to return the third character in the string, ignoring leading and trailing whitespace characters:\n",
      "\n",
      "```python\n",
      "def get_third_char(word):\n",
      "    \"\"\"\n",
      "    Returns the third character in the string, ignoring leading and trailing whitespace characters.\n",
      "\n",
      "    Args:\n",
      "        word (str): The input string.\n",
      "\n",
      "    Returns:\n",
      "        str: The third character in the string.\n",
      "    \"\"\"\n",
      "    # Initialize an index to keep track of the current character position\n",
      "    index = 0\n",
      "    \n",
      "    # Iterate over each character in the string\n",
      "    for char in word:\n",
      "        # If the current character is not a whitespace, increment the index\n",
      "        if char != ' ':\n",
      "            index += 1\n",
      "            # If the index is 3, return the current character\n",
      "            if index == 3:\n",
      "                return char\n",
      "\n",
      "# Example usage:\n",
      "word = \"   hello   \"\n",
      "print(get_third_char(word))  # Output: \"l\"\n",
      "```\n",
      "\n",
      "This function works by iterating over each character in the string. It keeps track of the current character position using an index variable. When it encounters a non-whitespace character, it increments the index. If the index reaches 3, it returns the current character. This approach ensures that the function uses a single loop iteration and handles cases where the input string has leading or trailing whitespace characters.\n",
      "--------------------------------------------------------------------------------\n",
      "Correctness Level: Partially Correct\n",
      "Correctness Reason: The code has no syntax errors and logically attempts to find the third non-whitespace character by iterating once over the string. However, it does not correctly handle trailing whitespace because it only skips spaces and counts non-space characters, not trimming trailing whitespace before counting. The instruction requires removing both leading and trailing whitespace before returning the third character, but this code only ignores spaces during iteration and does not remove trailing whitespace properly. Also, it only skips spaces (' '), not all whitespace characters, which is a minor deviation. Therefore, it partially satisfies the constraints.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_row(row):\n",
    "    print(f\"Instruction: {row['instruction']}\")\n",
    "    print(f\"Response: {row['response']}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Correctness Level: {row['correctness_level']}\")\n",
    "    print(f\"Correctness Reason: {row['correctness_reason']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    print_row(row)\n",
    "    print(\"\\n\")  # Add a newline for better readability between rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import ast\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_metrics_from_column(df, column_name):\n",
    "    # Parse constraint adherence column\n",
    "    df[column_name] = df[column_name].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    csr_values = []\n",
    "    ssr_values = []\n",
    "\n",
    "    for r_list in df[column_name]:\n",
    "        r_list = list(map(int, r_list))\n",
    "        csr = int(all(r_list))\n",
    "        ssr = sum(r_list) / len(r_list) if r_list else 0\n",
    "        csr_values.append(csr)\n",
    "        ssr_values.append(ssr)\n",
    "\n",
    "    df[\"CSR_per_row\"] = csr_values\n",
    "    df[\"SSR_per_row\"] = ssr_values\n",
    "\n",
    "    # Compute overall metrics\n",
    "    m = len(df)\n",
    "    overall_csr = sum(csr_values) / m if m > 0 else 0\n",
    "    overall_ssr = sum(ssr_values) / m if m > 0 else 0\n",
    "\n",
    "    return df, overall_csr, overall_ssr\n",
    "\n",
    "def compute_combination_metrics(df):\n",
    "    metrics = {}\n",
    "\n",
    "    # Only keep CSR=1 combinations\n",
    "    for correctness in [\"Completely Correct\", \"Partially Correct\", \"Wrong\"]:\n",
    "        key = f\"{correctness.replace(' ', '_')}_CSR1\"\n",
    "        metrics[key] = len(df[(df[\"correctness_level\"] == correctness) & (df[\"CSR_per_row\"] == 1)])\n",
    "\n",
    "    # Avg SSR for each correctness level\n",
    "    avg_ssr = (\n",
    "        df.groupby(\"correctness_level\")[\"SSR_per_row\"]\n",
    "        .mean()\n",
    "        .rename(lambda x: f\"Avg_SSR_{x.replace(' ', '_')}\")\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    metrics.update(avg_ssr)\n",
    "    return metrics\n",
    "\n",
    "def main(input_path, column_name, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df = pd.read_json(input_path, lines=True) if input_path.endswith(\".jsonl\") else pd.read_csv(input_path)\n",
    "\n",
    "    # Compute row-level CSR and SSR\n",
    "    df_with_metrics, csr, ssr = compute_metrics_from_column(df, column_name)\n",
    "\n",
    "    # Compute filtered combination metrics\n",
    "    combo_metrics = compute_combination_metrics(df_with_metrics)\n",
    "\n",
    "    # Save row-level data\n",
    "    input_filename = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    detailed_output_path = os.path.join(output_dir, f\"{input_filename}_metrics.csv\")\n",
    "    df_with_metrics.to_csv(detailed_output_path, index=False)\n",
    "\n",
    "    # Save metrics summary\n",
    "    summary_data = {\n",
    "        \"filename\": os.path.basename(input_path),\n",
    "        \"Overall_CSR\": csr,\n",
    "        \"Overall_SSR\": ssr\n",
    "    }\n",
    "    summary_data.update(combo_metrics)\n",
    "    summary_df = pd.DataFrame([summary_data])\n",
    "    summary_output_path = os.path.join(output_dir, \"metrics_summary.csv\")\n",
    "    print(summary_df.info())\n",
    "    # summary_df.to_csv(summary_output_path, index=False)\n",
    "\n",
    "    print(f\"Saved row-level metrics to {detailed_output_path}\")\n",
    "    print(f\"Saved overall + combination metrics to {summary_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dccstor/shanmukh/sravani_internship/CIF_Benchmark/scripts/constraints\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 9 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   filename                    1 non-null      object \n",
      " 1   Overall_CSR                 1 non-null      float64\n",
      " 2   Overall_SSR                 1 non-null      float64\n",
      " 3   Completely_Correct_CSR1     1 non-null      int64  \n",
      " 4   Partially_Correct_CSR1      1 non-null      int64  \n",
      " 5   Wrong_CSR1                  1 non-null      int64  \n",
      " 6   Avg_SSR_Completely_Correct  1 non-null      float64\n",
      " 7   Avg_SSR_Partially_Correct   1 non-null      float64\n",
      " 8   Avg_SSR_Wrong               1 non-null      float64\n",
      "dtypes: float64(5), int64(3), object(1)\n",
      "memory usage: 200.0+ bytes\n",
      "None\n",
      "Saved row-level metrics to ../../benchmark/metrics_output/Llama-3.1-8B-Instruct_results_llmjudge_with_correctness_metrics.csv\n",
      "Saved overall + combination metrics to ../../benchmark/metrics_output/metrics_summary.csv\n"
     ]
    }
   ],
   "source": [
    "input_path = \"../../benchmark/LLMjudge_outputs/code_correctness/Llama-3.1-8B-Instruct_results_llmjudge_with_correctness.jsonl\"\n",
    "column_name = \"Constraint_adherence\"\n",
    "output_dir = \"../../benchmark/metrics_output\"\n",
    "main(\n",
    "input_path=input_path,\n",
    "column_name=column_name,\n",
    "output_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "with open(\"/dccstor/shanmukh/sravani_internship/benchmark_experiments/benchmark_dataset/benchmark_v5.jsonl\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1400,\n",
       " 'dataset': 'Multilingual-Multimodal-NLP/McEval-Instruct',\n",
       " 'instruction': '### 142. Linked List Cycle II\\n\\nGiven the `head` of a linked list, return the node where the cycle begins. If there is no cycle, return `null`.\\n\\nTo represent a cycle in the given linked list, we use an integer `pos` which represents the position (0-indexed) in the linked list where the tail connects to. If `pos` is `-1`, then there is no cycle in the linked list.\\n\\n**Note**: Do not modify the linked list.\\n\\n**Follow up**:\\nCan you solve it using `O(1)` (i.e., constant) memory?\\n\\n**Example**:\\n```\\nInput: head = [3,2,0,-4], pos = 1\\nOutput: The node with value 2\\nExplanation: There is a cycle in the linked list, where the tail connects to the second node.\\n```',\n",
       " 'code': '```python\\nfrom ListNode import *\\n\\n# Definition for singly-linked list.\\n# class ListNode(object):\\n#     def __init__(self, x):\\n#         self.val = x\\n#         self.next = None\\n\\nclass Solution(object):\\n    def detectCycle(self, head):\\n        \"\"\"\\n        :type head: ListNode\\n        :rtype: ListNode\\n        \"\"\"\\n        slow = fast = head\\n        while fast and fast.next:\\n            slow = slow.next\\n            fast = fast.next.next\\n            if slow == fast:\\n                break\\n        else:\\n            return None\\n        \\n        while head != slow:\\n            head = head.next\\n            slow = slow.next\\n        \\n        return head\\n\\n# Test cases to verify the solution\\nif __name__ == \"__main__\":\\n    # Helper function to create a linked list with a cycle\\n    def create_cycle_list(values, pos):\\n        head = ListNode(0)\\n        current = head\\n        cycle_entry = None\\n        for i, val in enumerate(values):\\n            current.next = ListNode(val)\\n            current = current.next\\n            if i == pos:\\n                cycle_entry = current\\n        if cycle_entry:\\n            current.next = cycle_entry\\n        return head.next\\n\\n    # Test case 1\\n    head = create_cycle_list([3, 2, 0, -4], 1)\\n    sl = Solution()\\n    assert sl.detectCycle(head).val == 2\\n\\n    # Test case 2\\n    head = create_cycle_list([1, 2], 0)\\n    assert sl.detectCycle(head).val == 1\\n\\n    # Test case 3\\n    head = create_cycle_list([1], -1)\\n    assert sl.detectCycle(head) == None\\n\\n    print(\"All test cases passed.\")\\n```\\n\\nIn the provided solution, the `detectCycle` function uses Floyd\\'s Tortoise and Hare algorithm to detect a cycle in the linked list. If a cycle is detected, it then finds the entry point of the cycle and returns the corresponding node. If no cycle is detected, it returns `None`. The test cases verify that the solution works correctly for various scenarios.',\n",
       " 'test': None,\n",
       " 'relevant_categories': \"['Code Structure and Modularity', 'Input and Output Handling', 'Error Handling and Robustness', 'Data Processing and Transformation', 'Performance and Optimization', 'Testing and Debugging', 'Documentation and Readability']\",\n",
       " 'simplified_instruction': 'Given the `head` of a linked list, return the node where the cycle begins. If there is no cycle, return `null`. To represent a cycle in the given linked list, we use an integer `pos` which represents the position (0-indexed) in the linked list where the tail connects to. If `pos` is `-1`, then there is no cycle in the linked list. **Note**: Do not modify the linked list. **Follow up**: Can you solve it using `O(1)` (i.e., constant) memory? **Example**: Input: head = [3,2,0,-4], pos = 1 Output: The node with value 2 Explanation: There is a cycle in the linked list, where the tail connects to the second node.',\n",
       " 'extracted_constraints': \"[{'type': 'Input and Output Handling', 'constraint': 'Return the node where the cycle begins.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Input and Output Handling', 'constraint': 'Return null if there is no cycle.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Input and Output Handling', 'constraint': 'Use an integer pos to represent the position (0-indexed) in the linked list where the tail connects to.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Input and Output Handling', 'constraint': 'If pos is -1, then there is no cycle in the linked list.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Data Processing and Transformation', 'constraint': 'Do not modify the linked list.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Performance and Optimization', 'constraint': 'Solve it using O(1) (i.e., constant) memory.', 'instruction_part': 'Extracted from instruction'}]\",\n",
       " 'final_comprehensive_constraints': \"[{'type': 'Input and Output Handling', 'constraint': 'Return the node where the cycle begins.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Input and Output Handling', 'constraint': 'Return null if there is no cycle.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Input and Output Handling', 'constraint': 'Use an integer pos to represent the position (0-indexed) in the linked list where the tail connects to.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Input and Output Handling', 'constraint': 'If pos is -1, then there is no cycle in the linked list.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Data Processing and Transformation', 'constraint': 'Do not modify the linked list.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Performance and Optimization', 'constraint': 'Solve it using O(1) (i.e., constant) memory.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Code Structure and Modularity', 'constraint': 'Implement the cycle detection algorithm using a two-pointer technique to ensure clarity and efficiency.', 'instruction_part': 'Newly Generated'}, {'type': 'Testing and Debugging', 'constraint': 'Include test cases that cover edge cases, such as an empty list and a single node without a cycle.', 'instruction_part': 'Newly Generated'}, {'type': 'Documentation and Readability', 'constraint': 'Provide clear comments in the code to explain the logic behind the cycle detection algorithm.', 'instruction_part': 'Newly Generated'}, {'type': 'Error Handling and Robustness', 'constraint': 'Ensure the function handles invalid inputs gracefully, such as passing a non-ListNode object as head.', 'instruction_part': 'Newly Generated'}]\",\n",
       " 'filtered_relevant_constraints': \"[{'type': 'Input and Output Handling', 'constraint': 'Return the node where the cycle begins.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Input and Output Handling', 'constraint': 'Return null if there is no cycle.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Input and Output Handling', 'constraint': 'Use an integer pos to represent the position (0-indexed) in the linked list where the tail connects to.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Input and Output Handling', 'constraint': 'If pos is -1, then there is no cycle in the linked list.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Data Processing and Transformation', 'constraint': 'Do not modify the linked list.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Performance and Optimization', 'constraint': 'Solve it using O(1) (i.e., constant) memory.', 'instruction_part': 'Extracted from instruction'}, {'type': 'Code Structure and Modularity', 'constraint': 'Implement the cycle detection algorithm using a two-pointer technique to ensure clarity and efficiency.', 'instruction_part': 'Newly Generated'}, {'type': 'Testing and Debugging', 'constraint': 'Include test cases that cover edge cases, such as an empty list and a single node without a cycle.', 'instruction_part': 'Newly Generated'}, {'type': 'Documentation and Readability', 'constraint': 'Provide clear comments in the code to explain the logic behind the cycle detection algorithm.', 'instruction_part': 'Newly Generated'}, {'type': 'Error Handling and Robustness', 'constraint': 'Ensure the function handles invalid inputs gracefully, such as passing a non-ListNode object as head.', 'instruction_part': 'Newly Generated'}]\",\n",
       " 'quality_scores': '{\\'constraint_evaluations\\': [{\\'constraint_text\\': \\'Return the node where the cycle begins.\\', \\'atomicity_score\\': 5, \\'relevance_score\\': 5, \\'objectivity_score\\': 5, \\'reasoning\\': \\'This constraint is atomic as it specifies a single requirement: to return the node where the cycle begins. It is highly relevant to the task of detecting cycles in a linked list and can be objectively evaluated by checking the return value of the function.\\'}, {\\'constraint_text\\': \\'Return null if there is no cycle.\\', \\'atomicity_score\\': 5, \\'relevance_score\\': 5, \\'objectivity_score\\': 5, \\'reasoning\\': \"This constraint is atomic, clearly stating a single requirement. It is relevant as it directly addresses the expected behavior of the function when no cycle is present. The evaluation is objective, as it can be tested by checking the return value against \\'null\\'.\"}, {\\'constraint_text\\': \\'Use an integer pos to represent the position (0-indexed) in the linked list where the tail connects to.\\', \\'atomicity_score\\': 5, \\'relevance_score\\': 5, \\'objectivity_score\\': 5, \\'reasoning\\': \\'This constraint is atomic and specifies a single requirement regarding the input format. It is relevant as it directly relates to how cycles are represented in the linked list. The evaluation is objective, as it can be verified by checking the input parameters.\\'}, {\\'constraint_text\\': \\'If pos is -1, then there is no cycle in the linked list.\\', \\'atomicity_score\\': 5, \\'relevance_score\\': 5, \\'objectivity_score\\': 5, \\'reasoning\\': \"This constraint is atomic and clearly defines a single condition. It is relevant as it directly pertains to the cycle detection logic. The evaluation is objective, as it can be tested by checking the value of \\'pos\\'.\"}, {\\'constraint_text\\': \\'Do not modify the linked list.\\', \\'atomicity_score\\': 5, \\'relevance_score\\': 5, \\'objectivity_score\\': 5, \\'reasoning\\': \\'This constraint is atomic, stating a single requirement regarding the immutability of the linked list. It is relevant to the task as it ensures the integrity of the input data. The evaluation is objective, as it can be verified by examining the state of the linked list after function execution.\\'}, {\\'constraint_text\\': \\'Solve it using O(1) (i.e., constant) memory.\\', \\'atomicity_score\\': 5, \\'relevance_score\\': 5, \\'objectivity_score\\': 5, \\'reasoning\\': \\'This constraint is atomic and specifies a single performance requirement. It is highly relevant as it addresses the efficiency of the solution. The evaluation is objective, as it can be measured by analyzing the memory usage of the algorithm.\\'}, {\\'constraint_text\\': \\'Implement the cycle detection algorithm using a two-pointer technique to ensure clarity and efficiency.\\', \\'atomicity_score\\': 4, \\'relevance_score\\': 5, \\'objectivity_score\\': 4, \\'reasoning\\': \"This constraint is mostly atomic but suggests a specific implementation method, which could be seen as slightly less focused. It is relevant as it directly relates to the algorithm\\'s implementation. The evaluation is somewhat objective, but the term \\'clarity and efficiency\\' could be interpreted differently.\"}, {\\'constraint_text\\': \\'Include test cases that cover edge cases, such as an empty list and a single node without a cycle.\\', \\'atomicity_score\\': 4, \\'relevance_score\\': 5, \\'objectivity_score\\': 4, \\'reasoning\\': \"This constraint is not fully atomic as it combines multiple requirements (covering edge cases). It is relevant as it pertains to testing the function\\'s robustness. The evaluation is somewhat objective, but the specifics of what constitutes \\'edge cases\\' could vary.\"}, {\\'constraint_text\\': \\'Provide clear comments in the code to explain the logic behind the cycle detection algorithm.\\', \\'atomicity_score\\': 4, \\'relevance_score\\': 4, \\'objectivity_score\\': 3, \\'reasoning\\': \"This constraint is not fully atomic as it implies multiple actions (providing comments). It is relevant to documentation but less so to the core functionality. The evaluation is subjective, as \\'clear comments\\' can vary in interpretation.\"}, {\\'constraint_text\\': \\'Ensure the function handles invalid inputs gracefully, such as passing a non-ListNode object as head.\\', \\'atomicity_score\\': 5, \\'relevance_score\\': 5, \\'objectivity_score\\': 5, \\'reasoning\\': \"This constraint is atomic, specifying a single requirement regarding error handling. It is highly relevant as it addresses the robustness of the function. The evaluation is objective, as it can be tested by providing invalid inputs and checking the function\\'s response.\"}], \\'avg_atomicity\\': 4.6, \\'avg_relevance\\': 4.8, \\'avg_objectivity\\': 4.4, \\'unified_quality_score\\': 4.6, \\'overall_analysis\\': \\'The set of constraints is generally of high quality, with most constraints being atomic, relevant, and objective. The strengths lie in the clear articulation of requirements directly related to the task of cycle detection in linked lists. However, some constraints could benefit from improved atomicity and objectivity, particularly those that suggest implementation methods or testing strategies. Overall, the constraints provide a solid foundation for guiding the implementation of the linked list cycle detection algorithm.\\'}',\n",
       " 'relevance_score': 4.8,\n",
       " 'objectivity_score': 4.4,\n",
       " 'atomicity_score': 4.6,\n",
       " 'unified_quality_score': 4.6,\n",
       " 'combined_instruction': '### 142. Linked List Cycle II\\n\\nGiven the `head` of a linked list, return the node where the cycle begins. **Return the node where the cycle begins.** If there is no cycle, return `null`. **Return null if there is no cycle.**\\n\\nTo represent a cycle in the given linked list, we use an integer `pos` which represents the position (0-indexed) in the linked list where the tail connects to. **Use an integer pos to represent the position (0-indexed) in the linked list where the tail connects to.** If `pos` is `-1`, then there is no cycle in the linked list. **If pos is -1, then there is no cycle in the linked list.**\\n\\n**Note**: **Do not modify the linked list.**\\n\\n**Follow up**:\\nCan you solve it using `O(1)` (i.e., constant) memory? **Solve it using O(1) (i.e., constant) memory.**\\n\\n**Example**:\\n```\\nInput: head = [3,2,0,-4], pos = 1\\nOutput: The node with value 2\\nExplanation: There is a cycle in the linked list, where the tail connects to the second node.\\n```\\n\\n**Implement the cycle detection algorithm using a two-pointer technique to ensure clarity and efficiency.**\\n\\n**Include test cases that cover edge cases, such as an empty list and a single node without a cycle.**\\n\\n**Provide clear comments in the code to explain the logic behind the cycle detection algorithm.**\\n\\n**Ensure the function handles invalid inputs gracefully, such as passing a non-ListNode object as head.**',\n",
       " 'constraint_wise_presence': [True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Input and Output Handling',\n",
       "  'constraint': 'Return the node where the cycle begins.',\n",
       "  'instruction_part': 'Extracted from instruction'},\n",
       " {'type': 'Input and Output Handling',\n",
       "  'constraint': 'Return null if there is no cycle.',\n",
       "  'instruction_part': 'Extracted from instruction'},\n",
       " {'type': 'Input and Output Handling',\n",
       "  'constraint': 'Use an integer pos to represent the position (0-indexed) in the linked list where the tail connects to.',\n",
       "  'instruction_part': 'Extracted from instruction'},\n",
       " {'type': 'Input and Output Handling',\n",
       "  'constraint': 'If pos is -1, then there is no cycle in the linked list.',\n",
       "  'instruction_part': 'Extracted from instruction'},\n",
       " {'type': 'Data Processing and Transformation',\n",
       "  'constraint': 'Do not modify the linked list.',\n",
       "  'instruction_part': 'Extracted from instruction'},\n",
       " {'type': 'Performance and Optimization',\n",
       "  'constraint': 'Solve it using O(1) (i.e., constant) memory.',\n",
       "  'instruction_part': 'Extracted from instruction'},\n",
       " {'type': 'Code Structure and Modularity',\n",
       "  'constraint': 'Implement the cycle detection algorithm using a two-pointer technique to ensure clarity and efficiency.',\n",
       "  'instruction_part': 'Newly Generated'},\n",
       " {'type': 'Testing and Debugging',\n",
       "  'constraint': 'Include test cases that cover edge cases, such as an empty list and a single node without a cycle.',\n",
       "  'instruction_part': 'Newly Generated'},\n",
       " {'type': 'Documentation and Readability',\n",
       "  'constraint': 'Provide clear comments in the code to explain the logic behind the cycle detection algorithm.',\n",
       "  'instruction_part': 'Newly Generated'},\n",
       " {'type': 'Error Handling and Robustness',\n",
       "  'constraint': 'Ensure the function handles invalid inputs gracefully, such as passing a non-ListNode object as head.',\n",
       "  'instruction_part': 'Newly Generated'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(data[0][\"filtered_relevant_constraints\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "fail = 0\n",
    "for idx, ele in enumerate(data):\n",
    "    if len(eval(ele['filtered_relevant_constraints'])) == len(ele['constraint_wise_presence']):\n",
    "        pass\n",
    "    else:\n",
    "        fail += 1\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "internship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
